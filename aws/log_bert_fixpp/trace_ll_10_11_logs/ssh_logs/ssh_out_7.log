Parent process ID: 4548 node: 172.31.22.229
2 per stage
16 servers!
Config:
ranks: range(7, 8)
train batch size: 1024
partitions: 8
chunk_size: 8
data depth: 2
stage to rank map: 0,8;1,9;2,10;3,11;4,12;5,13;6,14;7,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=7 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,8;1,9;2,10;3,11;4,12;5,13;6,14;7,15; --batch-size=512 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.10976719856262207
SHARED WEIGHTS ARE
[(0, 7)]
this rank  7 is part of pipeline replica  0
64 chunks
Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9399.416
Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5645.953
Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5712.112
Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5487.038
Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5599.642
Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4542.454
Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4325.432
Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4191.437
Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4238.451
Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4320.193
Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4466.524
Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4527.030
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4390.393
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4329.836
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4386.968
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4371.275
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4349.924
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4454.620
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4450.108
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4551.967
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 4278.723
2022-11-23 14:17:30.267374 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 7 signal handler called with signal 10
Opt ckpt time 8.386304140090942
Process done with return code 0
Parent process ID: 5111 node: 172.31.31.40
2 per stage
16 servers!
Config:
ranks: range(7, 8)
train batch size: 1024
partitions: 8
chunk_size: 8
data depth: 2
stage to rank map: 0,8;1,9;2,10;3,11;4,12;5,13;6,14;7,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=7 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,8;1,9;2,10;3,11;4,12;5,13;6,14;7,15; --batch-size=512 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 21
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.13141465187072754
SHARED WEIGHTS ARE
[(0, 7)]
this rank  7 is part of pipeline replica  0
64 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_21.pt
2022-11-23 14:17:58.892368 resume step from  21
2022-11-23 14:18:08.833686 - Finished loading checkpoint, takes 9929.334 secs
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 12684.019
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5301.050
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5354.405
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5331.532
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5369.845
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4187.912
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4146.505
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4279.859
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4187.477
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4301.390
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4234.211
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4227.186
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4172.288
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4230.022
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4179.804
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4187.297
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4278.183
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4217.616
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4309.199
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4223.683
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4678.057
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4303.699
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4141.670
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([5], dtype=torch.int32), speed: 4160.392
2022-11-23 14:20:03.558499 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 7 signal handler called with signal 10
Opt ckpt time 9.725517988204956
Process done with return code 0
Parent process ID: 5668 node: 172.31.31.40
1 per stage
8 servers!
Config:
ranks: range(7, 8)
train batch size: 1024
partitions: 8
chunk_size: 8
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;
World size is 8
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=7 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7; --batch-size=1024 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 45
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.11971592903137207
SHARED WEIGHTS ARE
[(0, 7)]
this rank  7 is part of pipeline replica  0
128 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_45.pt
2022-11-23 14:20:31.672360 resume step from  45
2022-11-23 14:20:41.490519 - Finished loading checkpoint, takes 9805.872 secs
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 19298.540
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7527.704
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7349.549
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7462.982
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7355.760
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7361.689
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7467.824
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7250.802
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7430.532
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7903.071
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7439.283
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7336.257
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 7358.735
2022-11-23 14:22:30.070652 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 7 signal handler called with signal 10
Opt ckpt time 9.27620530128479
Process done with return code 0
Parent process ID: 6034 node: 172.31.17.44
1 per stage
8 servers!
Config:
ranks: range(7, 8)
train batch size: 1024
partitions: 8
chunk_size: 8
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;
World size is 8
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=7 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7; --batch-size=1024 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 58
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.11108112335205078
SHARED WEIGHTS ARE
[(0, 7)]
this rank  7 is part of pipeline replica  0
128 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_58.pt
2022-11-23 14:22:58.780021 resume step from  58
2022-11-23 14:23:07.151366 - Finished loading checkpoint, takes 8359.071 secs
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 15963.381
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7498.249
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7444.568
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7447.834
7 Overflow !!
7 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7585.328
