Parent process ID: 48502 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 4 0 3598379.5166015625 0
End of simulation:  Mini-batch time (usec) = 4923429
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 142174, max long fwd 144458; min long bwd 186536, max long bwd 191040
Time taken by simulation: 45 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 8 0 954047.119140625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3096062
Min send: 10000000, max send 0
Min long send: 249051, max long send 268766
Min fwd: 79507, max fwd 86627; min bwd 88131, max bwd 94131
Min long fwd: 58556, max long fwd 63385; min long bwd 92867, max long bwd 97582
Time taken by simulation: 166 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 13 0 485476.6540527344 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3469734
Min send: 10000000, max send 0
Min long send: 248801, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 53890, max bwd 64826
Min long fwd: 37108, max long fwd 45662; min long bwd 64919, max long bwd 72155
Time taken by simulation: 422 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 364488.09814453125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3900556
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 20622, max fwd 60312; min bwd 40311, max bwd 50179
Min long fwd: 29744, max long fwd 36491; min long bwd 47928, max long bwd 56568
Time taken by simulation: 803 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 26 0 254962.82958984375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5944030
Min send: 10000000, max send 0
Min long send: 248735, max long send 278723
Min fwd: 10836, max fwd 44441; min bwd 24199, max bwd 35953
Min long fwd: 21303, max long fwd 29429; min long bwd 33298, max long bwd 40795
Time taken by simulation: 1937 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 32 0 244280.30395507812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7674225
Min send: 10000000, max send 0
Min long send: 248719, max long send 278723
Min fwd: 6779, max fwd 41103; min bwd 15994, max bwd 28589
Min long fwd: 18906, max long fwd 26091; min long bwd 25267, max long bwd 32480
Time taken by simulation: 3794 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10790 microseconds

{1: 4.923429, 2: 3.096062, 3: 3.469734, 4: 3.900556, 6: 5.94403, 8: 7.674225, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.096062
16 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 16
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31; --batch-size=64 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 03:57:13.814646 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=64, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=False, resume_step=-1, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;', chunk_size=8, batch_size=64, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.1540820598602295
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
8 chunks
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
DLL 2022-11-26 03:57:26.712289 - PARAMETER SEED : 12439 
DLL 2022-11-26 03:57:26.712429 - PARAMETER train_start : True 
DLL 2022-11-26 03:57:26.712480 - PARAMETER batch_size_per_gpu : 64 
DLL 2022-11-26 03:57:26.712526 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:57:36.548425] Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9761.408
DLL 2022-11-26 03:57:36.559193 - Training Epoch: 0 Training Iteration: 1  average_loss : 11.2509765625  step_loss : 11.2509765625  learning_rate : 2.9986455118223097e-06 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:57:39.749985] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3201.369
DLL 2022-11-26 03:57:39.755273 - Training Epoch: 0 Training Iteration: 2  average_loss : nan  step_loss : nan  learning_rate : 5.9972910236446195e-06 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:57:42.966393] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3216.382
DLL 2022-11-26 03:57:42.971406 - Training Epoch: 0 Training Iteration: 3  average_loss : nan  step_loss : nan  learning_rate : 8.995936535466931e-06 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:57:46.126814] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3160.368
DLL 2022-11-26 03:57:46.131735 - Training Epoch: 0 Training Iteration: 4  average_loss : nan  step_loss : nan  learning_rate : 1.1994582047289239e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:57:49.295055] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3168.218
DLL 2022-11-26 03:57:49.300239 - Training Epoch: 0 Training Iteration: 5  average_loss : nan  step_loss : nan  learning_rate : 1.499322755911155e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:57:51.463468] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2168.359
DLL 2022-11-26 03:57:51.468287 - Training Epoch: 0 Training Iteration: 6  average_loss : nan  step_loss : nan  learning_rate : 1.7991873070933862e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:57:53.644984] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2181.486
DLL 2022-11-26 03:57:53.650182 - Training Epoch: 0 Training Iteration: 7  average_loss : nan  step_loss : nan  learning_rate : 2.099051858275617e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:57:55.813051] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2168.051
DLL 2022-11-26 03:57:55.818674 - Training Epoch: 0 Training Iteration: 8  average_loss : nan  step_loss : nan  learning_rate : 2.3989164094578478e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:57:57.969490] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2156.410
DLL 2022-11-26 03:57:57.974486 - Training Epoch: 0 Training Iteration: 9  average_loss : nan  step_loss : nan  learning_rate : 2.698780960640079e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:58:00.146731] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2177.199
DLL 2022-11-26 03:58:00.152048 - Training Epoch: 0 Training Iteration: 10  average_loss : nan  step_loss : nan  learning_rate : 2.99864551182231e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:58:02.329662] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2182.903
DLL 2022-11-26 03:58:02.334952 - Training Epoch: 0 Training Iteration: 11  average_loss : nan  step_loss : nan  learning_rate : 3.298510063004541e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:58:04.463562] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2133.873
DLL 2022-11-26 03:58:04.468731 - Training Epoch: 0 Training Iteration: 12  average_loss : nan  step_loss : nan  learning_rate : 3.5983746141867724e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:58:06.671172] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2207.580
DLL 2022-11-26 03:58:06.676062 - Training Epoch: 0 Training Iteration: 13  average_loss : nan  step_loss : nan  learning_rate : 3.898239165369003e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:58:08.876838] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2205.639
DLL 2022-11-26 03:58:08.881745 - Training Epoch: 0 Training Iteration: 14  average_loss : nan  step_loss : nan  learning_rate : 4.198103716551234e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:58:11.018186] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2141.350
DLL 2022-11-26 03:58:11.023009 - Training Epoch: 0 Training Iteration: 15  average_loss : nan  step_loss : nan  learning_rate : 4.497968267733465e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:58:13.205949] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2187.701
DLL 2022-11-26 03:58:13.210988 - Training Epoch: 0 Training Iteration: 16  average_loss : nan  step_loss : nan  learning_rate : 4.7978328189156956e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:58:15.383129] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2177.144
DLL 2022-11-26 03:58:15.388571 - Training Epoch: 0 Training Iteration: 17  average_loss : nan  step_loss : nan  learning_rate : 5.097697370097927e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:58:17.563448] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2180.295
DLL 2022-11-26 03:58:17.568372 - Training Epoch: 0 Training Iteration: 18  average_loss : nan  step_loss : nan  learning_rate : 5.397561921280158e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:58:19.717366] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2153.889
DLL 2022-11-26 03:58:19.722202 - Training Epoch: 0 Training Iteration: 19  average_loss : nan  step_loss : nan  learning_rate : 5.697426472462389e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:21.872837] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2155.431
DLL 2022-11-26 03:58:21.877899 - Training Epoch: 0 Training Iteration: 20  average_loss : nan  step_loss : nan  learning_rate : 5.99729102364462e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:24.091485] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2218.637
DLL 2022-11-26 03:58:24.128310 - Training Epoch: 0 Training Iteration: 21  average_loss : nan  step_loss : nan  learning_rate : 6.297155574826851e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:26.327962] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2236.441
DLL 2022-11-26 03:58:26.332732 - Training Epoch: 0 Training Iteration: 22  average_loss : nan  step_loss : nan  learning_rate : 6.597020126009082e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:28.510432] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2182.446
DLL 2022-11-26 03:58:28.515549 - Training Epoch: 0 Training Iteration: 23  average_loss : nan  step_loss : nan  learning_rate : 6.896884677191313e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:30.672311] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2161.842
DLL 2022-11-26 03:58:30.677326 - Training Epoch: 0 Training Iteration: 24  average_loss : nan  step_loss : nan  learning_rate : 7.196749228373545e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:32.848452] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2176.091
DLL 2022-11-26 03:58:32.853098 - Training Epoch: 0 Training Iteration: 25  average_loss : nan  step_loss : nan  learning_rate : 7.496613779555775e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:35.064955] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2216.498
DLL 2022-11-26 03:58:35.070414 - Training Epoch: 0 Training Iteration: 26  average_loss : nan  step_loss : nan  learning_rate : 7.796478330738006e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:37.224289] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2159.299
DLL 2022-11-26 03:58:37.229002 - Training Epoch: 0 Training Iteration: 27  average_loss : nan  step_loss : nan  learning_rate : 8.096342881920237e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:39.394858] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2170.562
DLL 2022-11-26 03:58:39.399798 - Training Epoch: 0 Training Iteration: 28  average_loss : nan  step_loss : nan  learning_rate : 8.396207433102469e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:41.583420] Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2188.507
DLL 2022-11-26 03:58:41.588066 - Training Epoch: 0 Training Iteration: 29  average_loss : nan  step_loss : nan  learning_rate : 8.696071984284699e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:43.743370] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2159.971
DLL 2022-11-26 03:58:43.750777 - Training Epoch: 0 Training Iteration: 30  average_loss : nan  step_loss : nan  learning_rate : 8.99593653546693e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:45.912229] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2168.795
DLL 2022-11-26 03:58:45.917175 - Training Epoch: 0 Training Iteration: 31  average_loss : nan  step_loss : nan  learning_rate : 9.29580108664916e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:48.075569] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2163.313
DLL 2022-11-26 03:58:48.080611 - Training Epoch: 0 Training Iteration: 32  average_loss : nan  step_loss : nan  learning_rate : 9.595665637831391e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:50.297565] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2221.948
DLL 2022-11-26 03:58:50.302868 - Training Epoch: 0 Training Iteration: 33  average_loss : nan  step_loss : nan  learning_rate : 9.895530189013622e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:52.780752] Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2483.154
DLL 2022-11-26 03:58:52.786012 - Training Epoch: 0 Training Iteration: 34  average_loss : nan  step_loss : nan  learning_rate : 0.00010195394740195854 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:54.968484] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2187.692
DLL 2022-11-26 03:58:54.973233 - Training Epoch: 0 Training Iteration: 35  average_loss : nan  step_loss : nan  learning_rate : 0.00010495259291378085 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:57.183498] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2214.994
DLL 2022-11-26 03:58:57.188558 - Training Epoch: 0 Training Iteration: 36  average_loss : nan  step_loss : nan  learning_rate : 0.00010795123842560316 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:58:59.377192] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2193.672
DLL 2022-11-26 03:58:59.382782 - Training Epoch: 0 Training Iteration: 37  average_loss : nan  step_loss : nan  learning_rate : 0.00011094988393742548 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:01.529113] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2151.887
DLL 2022-11-26 03:59:01.534050 - Training Epoch: 0 Training Iteration: 38  average_loss : nan  step_loss : nan  learning_rate : 0.00011394852944924778 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:03.731227] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2202.112
DLL 2022-11-26 03:59:03.736171 - Training Epoch: 0 Training Iteration: 39  average_loss : nan  step_loss : nan  learning_rate : 0.00011694717496107008 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:05.935774] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2204.508
DLL 2022-11-26 03:59:05.940705 - Training Epoch: 0 Training Iteration: 40  average_loss : nan  step_loss : nan  learning_rate : 0.0001199458204728924 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:08.138448] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2202.638
DLL 2022-11-26 03:59:08.143839 - Training Epoch: 0 Training Iteration: 41  average_loss : nan  step_loss : nan  learning_rate : 0.0001229444659847147 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:10.309947] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2171.463
DLL 2022-11-26 03:59:10.314921 - Training Epoch: 0 Training Iteration: 42  average_loss : nan  step_loss : nan  learning_rate : 0.00012594311149653702 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:12.456302] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2146.327
DLL 2022-11-26 03:59:12.461190 - Training Epoch: 0 Training Iteration: 43  average_loss : nan  step_loss : nan  learning_rate : 0.00012894175700835933 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:14.691414] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2235.117
DLL 2022-11-26 03:59:14.696368 - Training Epoch: 0 Training Iteration: 44  average_loss : nan  step_loss : nan  learning_rate : 0.00013194040252018164 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:16.939098] Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2247.647
DLL 2022-11-26 03:59:16.944622 - Training Epoch: 0 Training Iteration: 45  average_loss : nan  step_loss : nan  learning_rate : 0.00013493904803200396 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:19.124229] Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2185.106
DLL 2022-11-26 03:59:19.129737 - Training Epoch: 0 Training Iteration: 46  average_loss : nan  step_loss : nan  learning_rate : 0.00013793769354382627 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:21.297482] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2173.209
DLL 2022-11-26 03:59:21.302426 - Training Epoch: 0 Training Iteration: 47  average_loss : nan  step_loss : nan  learning_rate : 0.00014093633905564855 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:23.519946] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2222.416
DLL 2022-11-26 03:59:23.524997 - Training Epoch: 0 Training Iteration: 48  average_loss : nan  step_loss : nan  learning_rate : 0.0001439349845674709 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:25.711938] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2191.967
DLL 2022-11-26 03:59:25.716769 - Training Epoch: 0 Training Iteration: 49  average_loss : nan  step_loss : nan  learning_rate : 0.00014693363007929318 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:59:27.933632] Finished iteration 49, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 2221.646
DLL 2022-11-26 03:59:27.938733 - Training Epoch: 0 Training Iteration: 50  average_loss : nan  step_loss : nan  learning_rate : 0.0001499322755911155 
2022-11-26 03:59:27.938850 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 03:59:27.938869 - PARAMETER checkpoint_step : 50 
Opt ckpt time 7.8823347091674805
Process done with return code 0
Parent process ID: 49789 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 66 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 10 0 892117.1264648438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3456563
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 200 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 15 0 476084.3811035156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3889823
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 54355, max bwd 65681
Min long fwd: 38932, max long fwd 44034; min long bwd 64405, max long bwd 71935
Time taken by simulation: 494 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 19 0 352272.7355957031 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4275253
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 22410, max fwd 61283; min bwd 39748, max bwd 51303
Min long fwd: 29385, max long fwd 37648; min long bwd 47928, max long bwd 54735
Time taken by simulation: 857 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2498 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4544 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11076 microseconds

{1: 4.144382, 2: 3.456563, 3: 3.889823, 4: 4.275253, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.456563
14 per stage
28 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 14
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26;1,3,5,7,9,11,13,15,17,19,21,23,25,27;
World size is 28
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26;1,3,5,7,9,11,13,15,17,19,21,23,25,27; --batch-size=73 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 50
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:00:28.259229 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=73, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=50, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24,26;1,3,5,7,9,11,13,15,17,19,21,23,25,27;', chunk_size=8, batch_size=73, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.19198894500732422
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
10 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_50.pt
2022-11-26 04:00:38.677242 resume step from  50
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:01:38.554351 - Finished loading checkpoint, takes 59.850 secs
DLL 2022-11-26 04:01:38.555313 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:01:38.555430 - PARAMETER train_start : True 
DLL 2022-11-26 04:01:38.555491 - PARAMETER batch_size_per_gpu : 73 
DLL 2022-11-26 04:01:38.555527 - PARAMETER learning_rate : 0.006 
Process done with return code 1
Parent process ID: 51521 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 85 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 10 0 660629.9438476562 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3225075
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 309 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 522 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 998 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2545 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4737 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10939 microseconds

{1: 4.144382, 2: 3.225075, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.225075
13 per stage
26 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 13
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24;1,3,5,7,9,11,13,15,17,19,21,23,25;
World size is 26
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24;1,3,5,7,9,11,13,15,17,19,21,23,25; --batch-size=78 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 50
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:02:41.109600 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=78, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=50, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24;1,3,5,7,9,11,13,15,17,19,21,23,25;', chunk_size=8, batch_size=78, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.1552598476409912
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
10 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_50.pt
2022-11-26 04:02:51.490588 resume step from  50
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:03:50.125236 - Finished loading checkpoint, takes 58.607 secs
DLL 2022-11-26 04:03:50.126130 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:03:50.126248 - PARAMETER train_start : True 
DLL 2022-11-26 04:03:50.126335 - PARAMETER batch_size_per_gpu : 78 
DLL 2022-11-26 04:03:50.126373 - PARAMETER learning_rate : 0.006 
Process done with return code 1
Parent process ID: 53275 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 48 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 210 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 512 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 1148 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2436 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4794 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11109 microseconds

{1: 4.144382, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 50
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:04:52.909606 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=85, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=50, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;', chunk_size=8, batch_size=85, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.16797304153442383
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_50.pt
2022-11-26 04:05:04.380754 resume step from  50
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:06:02.080837 - Finished loading checkpoint, takes 57.673 secs
DLL 2022-11-26 04:06:02.082098 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:06:02.082217 - PARAMETER train_start : True 
DLL 2022-11-26 04:06:02.082320 - PARAMETER batch_size_per_gpu : 85 
DLL 2022-11-26 04:06:02.082363 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:06:19.531417] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5628.175
DLL 2022-11-26 04:06:19.536982 - Training Epoch: 0 Training Iteration: 51  average_loss : nan  step_loss : nan  learning_rate : 0.0001529309211029378 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 04:06:23.024175] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3492.580
DLL 2022-11-26 04:06:23.032961 - Training Epoch: 0 Training Iteration: 52  average_loss : nan  step_loss : nan  learning_rate : 0.00015592956661476012 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 04:06:26.452975] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3428.728
DLL 2022-11-26 04:06:26.458587 - Training Epoch: 0 Training Iteration: 53  average_loss : nan  step_loss : nan  learning_rate : 0.0001589282121265824 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 04:06:29.999414] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3546.442
DLL 2022-11-26 04:06:30.004790 - Training Epoch: 0 Training Iteration: 54  average_loss : nan  step_loss : nan  learning_rate : 0.00016192685763840475 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 04:06:33.493709] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3494.205
DLL 2022-11-26 04:06:33.499104 - Training Epoch: 0 Training Iteration: 55  average_loss : nan  step_loss : nan  learning_rate : 0.00016492550315022706 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 04:06:36.002696] Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2508.941
DLL 2022-11-26 04:06:36.007751 - Training Epoch: 0 Training Iteration: 56  average_loss : nan  step_loss : nan  learning_rate : 0.00016792414866204937 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 04:06:38.448262] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2445.543
DLL 2022-11-26 04:06:38.453456 - Training Epoch: 0 Training Iteration: 57  average_loss : nan  step_loss : nan  learning_rate : 0.00017092279417387166 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 04:06:40.958255] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2510.004
DLL 2022-11-26 04:06:40.963390 - Training Epoch: 0 Training Iteration: 58  average_loss : nan  step_loss : nan  learning_rate : 0.00017392143968569397 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 04:06:43.426175] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2467.884
DLL 2022-11-26 04:06:43.431362 - Training Epoch: 0 Training Iteration: 59  average_loss : nan  step_loss : nan  learning_rate : 0.0001769200851975163 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 04:06:45.968769] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2542.519
DLL 2022-11-26 04:06:45.973511 - Training Epoch: 0 Training Iteration: 60  average_loss : nan  step_loss : nan  learning_rate : 0.0001799187307093386 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 04:06:48.498540] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2529.752
DLL 2022-11-26 04:06:48.503396 - Training Epoch: 0 Training Iteration: 61  average_loss : nan  step_loss : nan  learning_rate : 0.00018291737622116094 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 04:06:51.032341] Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2533.776
DLL 2022-11-26 04:06:51.037276 - Training Epoch: 0 Training Iteration: 62  average_loss : nan  step_loss : nan  learning_rate : 0.0001859160217329832 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 04:06:53.529719] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2497.345
DLL 2022-11-26 04:06:53.534711 - Training Epoch: 0 Training Iteration: 63  average_loss : nan  step_loss : nan  learning_rate : 0.00018891466724480554 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 04:06:56.013479] Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2483.738
DLL 2022-11-26 04:06:56.018435 - Training Epoch: 0 Training Iteration: 64  average_loss : nan  step_loss : nan  learning_rate : 0.00019191331275662782 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 04:06:58.600005] Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2586.483
DLL 2022-11-26 04:06:58.605324 - Training Epoch: 0 Training Iteration: 65  average_loss : nan  step_loss : nan  learning_rate : 0.00019491195826845016 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 04:07:01.035330] Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2435.307
DLL 2022-11-26 04:07:01.040501 - Training Epoch: 0 Training Iteration: 66  average_loss : nan  step_loss : nan  learning_rate : 0.00019791060378027245 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 04:07:03.507607] Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2472.248
DLL 2022-11-26 04:07:03.512665 - Training Epoch: 0 Training Iteration: 67  average_loss : nan  step_loss : nan  learning_rate : 0.00020090924929209476 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 04:07:05.951212] Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2443.576
DLL 2022-11-26 04:07:05.956084 - Training Epoch: 0 Training Iteration: 68  average_loss : nan  step_loss : nan  learning_rate : 0.00020390789480391708 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 04:07:08.398399] Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2447.183
DLL 2022-11-26 04:07:08.403304 - Training Epoch: 0 Training Iteration: 69  average_loss : nan  step_loss : nan  learning_rate : 0.0002069065403157394 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:10.862647] Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2464.198
DLL 2022-11-26 04:07:10.867527 - Training Epoch: 0 Training Iteration: 70  average_loss : nan  step_loss : nan  learning_rate : 0.0002099051858275617 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:13.278826] Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2416.151
DLL 2022-11-26 04:07:13.284069 - Training Epoch: 0 Training Iteration: 71  average_loss : nan  step_loss : nan  learning_rate : 0.00021290383133938402 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:15.741682] Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2462.839
DLL 2022-11-26 04:07:15.746994 - Training Epoch: 0 Training Iteration: 72  average_loss : nan  step_loss : nan  learning_rate : 0.00021590247685120633 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:18.159055] Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2417.328
DLL 2022-11-26 04:07:18.164171 - Training Epoch: 0 Training Iteration: 73  average_loss : nan  step_loss : nan  learning_rate : 0.00021890112236302861 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:20.634775] Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2475.692
DLL 2022-11-26 04:07:20.640043 - Training Epoch: 0 Training Iteration: 74  average_loss : nan  step_loss : nan  learning_rate : 0.00022189976787485096 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:23.153576] Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2518.756
DLL 2022-11-26 04:07:23.158726 - Training Epoch: 0 Training Iteration: 75  average_loss : nan  step_loss : nan  learning_rate : 0.00022489841338667327 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:25.572793] Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2419.232
DLL 2022-11-26 04:07:25.577739 - Training Epoch: 0 Training Iteration: 76  average_loss : nan  step_loss : nan  learning_rate : 0.00022789705889849555 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:07:28.078638] Finished iteration 76, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 2505.797
DLL 2022-11-26 04:07:28.083598 - Training Epoch: 0 Training Iteration: 77  average_loss : nan  step_loss : nan  learning_rate : 0.00023089570441031787 
2022-11-26 04:07:28.083827 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:07:28.083927 - PARAMETER checkpoint_step : 77 
Opt ckpt time 7.339909553527832
Process done with return code 0
Parent process ID: 55435 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2325752.685546875 0
End of simulation:  Mini-batch time (usec) = 4300398
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 50 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 228 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 809 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1263 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3202 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 7085 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22045 microseconds

{1: 4.300398, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:08:23.232386 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=93, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=77, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;', chunk_size=8, batch_size=93, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.17682456970214844
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 04:08:33.515508 resume step from  77
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:09:24.453455 - Finished loading checkpoint, takes 50.910 secs
DLL 2022-11-26 04:09:24.454428 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:09:24.454550 - PARAMETER train_start : True 
DLL 2022-11-26 04:09:24.454604 - PARAMETER batch_size_per_gpu : 93 
DLL 2022-11-26 04:09:24.454640 - PARAMETER learning_rate : 0.006 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Process done with return code 1
Parent process ID: 56953 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2300677.490234375 0
End of simulation:  Mini-batch time (usec) = 4275323
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 50 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 230 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 601 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1217 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3439 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 7029 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22622 microseconds

{1: 4.275323, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:10:39.940844 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=93, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=77, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;', chunk_size=8, batch_size=93, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.004243373870849609
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 04:10:50.233154 resume step from  77
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:11:40.017008 - Finished loading checkpoint, takes 49.757 secs
DLL 2022-11-26 04:11:40.017892 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:11:40.018008 - PARAMETER train_start : True 
DLL 2022-11-26 04:11:40.018068 - PARAMETER batch_size_per_gpu : 93 
DLL 2022-11-26 04:11:40.018103 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:11:54.807112] Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4753.939
DLL 2022-11-26 04:11:54.812522 - Training Epoch: 0 Training Iteration: 78  average_loss : nan  step_loss : nan  learning_rate : 0.00023389434992214015 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 04:11:57.418632] Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2611.255
DLL 2022-11-26 04:11:57.423398 - Training Epoch: 0 Training Iteration: 79  average_loss : nan  step_loss : nan  learning_rate : 0.00023689299543396247 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 04:12:00.007605] Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2588.974
DLL 2022-11-26 04:12:00.015815 - Training Epoch: 0 Training Iteration: 80  average_loss : nan  step_loss : nan  learning_rate : 0.0002398916409457848 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 04:12:02.613854] Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2606.186
DLL 2022-11-26 04:12:02.618957 - Training Epoch: 0 Training Iteration: 81  average_loss : nan  step_loss : nan  learning_rate : 0.00024289028645760712 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 04:12:05.910357] Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3296.450
DLL 2022-11-26 04:12:05.915417 - Training Epoch: 0 Training Iteration: 82  average_loss : nan  step_loss : nan  learning_rate : 0.0002458889319694294 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 04:12:08.495128] Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2584.749
DLL 2022-11-26 04:12:08.499964 - Training Epoch: 0 Training Iteration: 83  average_loss : nan  step_loss : nan  learning_rate : 0.0002488875774812517 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 04:12:11.025230] Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2530.074
DLL 2022-11-26 04:12:11.030177 - Training Epoch: 0 Training Iteration: 84  average_loss : nan  step_loss : nan  learning_rate : 0.00025188622299307403 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 04:12:13.621209] Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2595.953
DLL 2022-11-26 04:12:13.626149 - Training Epoch: 0 Training Iteration: 85  average_loss : nan  step_loss : nan  learning_rate : 0.00025488486850489635 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 04:12:16.197872] Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.634
DLL 2022-11-26 04:12:16.202740 - Training Epoch: 0 Training Iteration: 86  average_loss : nan  step_loss : nan  learning_rate : 0.00025788351401671866 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 04:12:18.782236] Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2584.361
DLL 2022-11-26 04:12:18.787291 - Training Epoch: 0 Training Iteration: 87  average_loss : nan  step_loss : nan  learning_rate : 0.00026088215952854097 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 04:12:21.340062] Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2557.773
DLL 2022-11-26 04:12:21.345049 - Training Epoch: 0 Training Iteration: 88  average_loss : nan  step_loss : nan  learning_rate : 0.0002638808050403633 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 04:12:23.910131] Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2570.082
DLL 2022-11-26 04:12:23.915100 - Training Epoch: 0 Training Iteration: 89  average_loss : nan  step_loss : nan  learning_rate : 0.0002668794505521856 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 04:12:26.498240] Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2588.092
DLL 2022-11-26 04:12:26.503172 - Training Epoch: 0 Training Iteration: 90  average_loss : nan  step_loss : nan  learning_rate : 0.0002698780960640079 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 04:12:29.048188] Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2549.884
DLL 2022-11-26 04:12:29.053240 - Training Epoch: 0 Training Iteration: 91  average_loss : nan  step_loss : nan  learning_rate : 0.00027287674157583017 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 04:12:31.603684] Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2555.462
DLL 2022-11-26 04:12:31.608749 - Training Epoch: 0 Training Iteration: 92  average_loss : nan  step_loss : nan  learning_rate : 0.00027587538708765254 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 04:12:34.201105] Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2597.377
DLL 2022-11-26 04:12:34.206013 - Training Epoch: 0 Training Iteration: 93  average_loss : nan  step_loss : nan  learning_rate : 0.00027887403259947485 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 04:12:36.766095] Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2565.010
DLL 2022-11-26 04:12:36.771223 - Training Epoch: 0 Training Iteration: 94  average_loss : nan  step_loss : nan  learning_rate : 0.0002818726781112971 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 04:12:39.325953] Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2559.777
DLL 2022-11-26 04:12:39.330706 - Training Epoch: 0 Training Iteration: 95  average_loss : nan  step_loss : nan  learning_rate : 0.0002848713236231194 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 04:12:41.939410] Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2613.435
DLL 2022-11-26 04:12:41.944569 - Training Epoch: 0 Training Iteration: 96  average_loss : nan  step_loss : nan  learning_rate : 0.0002878699691349418 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:12:44.502953] Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2563.519
DLL 2022-11-26 04:12:44.507775 - Training Epoch: 0 Training Iteration: 97  average_loss : nan  step_loss : nan  learning_rate : 0.00029086861464676405 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:12:47.119593] Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2616.610
DLL 2022-11-26 04:12:47.124386 - Training Epoch: 0 Training Iteration: 98  average_loss : nan  step_loss : nan  learning_rate : 0.00029386726015858636 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:12:49.686285] Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2566.670
DLL 2022-11-26 04:12:49.691305 - Training Epoch: 0 Training Iteration: 99  average_loss : nan  step_loss : nan  learning_rate : 0.0002968659056704087 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:12:52.256572] Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2570.243
DLL 2022-11-26 04:12:52.261720 - Training Epoch: 0 Training Iteration: 100  average_loss : nan  step_loss : nan  learning_rate : 0.000299864551182231 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:12:54.880604] Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2624.018
DLL 2022-11-26 04:12:54.885569 - Training Epoch: 0 Training Iteration: 101  average_loss : nan  step_loss : nan  learning_rate : 0.0003028631966940533 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:12:57.485540] Finished iteration 101, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 2604.909
DLL 2022-11-26 04:12:57.490691 - Training Epoch: 0 Training Iteration: 102  average_loss : nan  step_loss : nan  learning_rate : 0.0003058618422058756 
2022-11-26 04:12:57.490822 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:12:57.490839 - PARAMETER checkpoint_step : 102 
Opt ckpt time 8.145398616790771
Process done with return code 0
Parent process ID: 58863 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2258812.98828125 0
End of simulation:  Mini-batch time (usec) = 4233458
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 209 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 517 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 1082 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2362 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4659 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11014 microseconds

{1: 4.233458, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 102
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:13:20.995514 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=85, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=102, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;', chunk_size=8, batch_size=85, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.10367012023925781
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_102.pt
2022-11-26 04:13:31.300173 resume step from  102
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:14:18.540745 - Finished loading checkpoint, takes 47.213 secs
DLL 2022-11-26 04:14:18.541612 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:14:18.541732 - PARAMETER train_start : True 
DLL 2022-11-26 04:14:18.541795 - PARAMETER batch_size_per_gpu : 85 
DLL 2022-11-26 04:14:18.541834 - PARAMETER learning_rate : 0.006 
2022-11-26 04:14:28.486338 Begin to exit
Process done with return code 0
Parent process ID: 60297 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 47 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 212 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 559 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 1036 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2379 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4499 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10789 microseconds

{1: 4.144382, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 102
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:14:39.118238 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=85, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=102, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;', chunk_size=8, batch_size=85, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.17437434196472168
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_102.pt
2022-11-26 04:14:49.460926 resume step from  102
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:15:37.471455 - Finished loading checkpoint, takes 47.984 secs
DLL 2022-11-26 04:15:37.472297 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:15:37.472410 - PARAMETER train_start : True 
DLL 2022-11-26 04:15:37.472469 - PARAMETER batch_size_per_gpu : 85 
DLL 2022-11-26 04:15:37.472504 - PARAMETER learning_rate : 0.006 
2022-11-26 04:15:48.147010 Begin to exit
Process done with return code 0
Parent process ID: 61649 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2350970.703125 0
End of simulation:  Mini-batch time (usec) = 4325616
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 56 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 244 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 605 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1372 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3330 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6991 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 23050 microseconds

{1: 4.325616, 2: 4.315108, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 102
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:16:28.553735 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=102, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=102, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;', chunk_size=8, batch_size=102, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.18070054054260254
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_102.pt
2022-11-26 04:16:38.853039 resume step from  102
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:17:27.878145 - Finished loading checkpoint, takes 48.998 secs
DLL 2022-11-26 04:17:27.879187 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:17:27.879311 - PARAMETER train_start : True 
DLL 2022-11-26 04:17:27.879373 - PARAMETER batch_size_per_gpu : 102 
DLL 2022-11-26 04:17:27.879427 - PARAMETER learning_rate : 0.006 
2022-11-26 04:17:37.709109 Begin to exit
Process done with return code 0
Parent process ID: 62973 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 70 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 254 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1308 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3210 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 7481 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21818 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 102
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:18:19.817849 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=102, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=102, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;', chunk_size=8, batch_size=102, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.17314720153808594
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_102.pt
2022-11-26 04:18:30.184863 resume step from  102
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:19:17.355186 - Finished loading checkpoint, takes 47.143 secs
DLL 2022-11-26 04:19:17.356213 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:19:17.356332 - PARAMETER train_start : True 
DLL 2022-11-26 04:19:17.356393 - PARAMETER batch_size_per_gpu : 102 
DLL 2022-11-26 04:19:17.356431 - PARAMETER learning_rate : 0.006 
2022-11-26 04:19:24.156877 Begin to exit
Process done with return code 0
Parent process ID: 64285 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2350970.703125 0
End of simulation:  Mini-batch time (usec) = 4325616
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 45 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 301 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 656 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1308 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3516 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6841 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21778 microseconds

{1: 4.325616, 2: 4.315108, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 102
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:19:34.890968 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=102, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=102, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;', chunk_size=8, batch_size=102, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.36673760414123535
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_102.pt
2022-11-26 04:19:45.348285 resume step from  102
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:20:32.161931 - Finished loading checkpoint, takes 46.786 secs
DLL 2022-11-26 04:20:32.162935 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:20:32.163052 - PARAMETER train_start : True 
DLL 2022-11-26 04:20:32.163113 - PARAMETER batch_size_per_gpu : 102 
DLL 2022-11-26 04:20:32.163150 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:20:54.402344] Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5862.277
DLL 2022-11-26 04:20:54.407896 - Training Epoch: 0 Training Iteration: 103  average_loss : nan  step_loss : nan  learning_rate : 0.000308860487717698 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 04:20:57.188359] Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2785.792
DLL 2022-11-26 04:20:57.193502 - Training Epoch: 0 Training Iteration: 104  average_loss : nan  step_loss : nan  learning_rate : 0.00031185913322952024 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 04:20:59.985866] Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2797.476
DLL 2022-11-26 04:20:59.990989 - Training Epoch: 0 Training Iteration: 105  average_loss : nan  step_loss : nan  learning_rate : 0.0003148577787413425 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 04:21:02.774891] Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2788.997
DLL 2022-11-26 04:21:02.779777 - Training Epoch: 0 Training Iteration: 106  average_loss : nan  step_loss : nan  learning_rate : 0.0003178564242531648 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 04:21:06.622809] Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.894
DLL 2022-11-26 04:21:06.628193 - Training Epoch: 0 Training Iteration: 107  average_loss : nan  step_loss : nan  learning_rate : 0.0003208550697649872 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 04:21:09.414455] Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2791.616
DLL 2022-11-26 04:21:09.419229 - Training Epoch: 0 Training Iteration: 108  average_loss : nan  step_loss : nan  learning_rate : 0.0003238537152768095 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 04:21:12.224805] Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2810.320
DLL 2022-11-26 04:21:12.229750 - Training Epoch: 0 Training Iteration: 109  average_loss : nan  step_loss : nan  learning_rate : 0.0003268523607886318 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 04:21:14.999722] Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2774.850
DLL 2022-11-26 04:21:15.004553 - Training Epoch: 0 Training Iteration: 110  average_loss : nan  step_loss : nan  learning_rate : 0.0003298510063004541 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 04:21:17.738055] Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2738.311
DLL 2022-11-26 04:21:17.742954 - Training Epoch: 0 Training Iteration: 111  average_loss : nan  step_loss : nan  learning_rate : 0.00033284965181227643 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 04:21:20.553867] Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2815.760
DLL 2022-11-26 04:21:20.558783 - Training Epoch: 0 Training Iteration: 112  average_loss : nan  step_loss : nan  learning_rate : 0.00033584829732409875 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 04:21:23.332828] Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2778.925
DLL 2022-11-26 04:21:23.337694 - Training Epoch: 0 Training Iteration: 113  average_loss : nan  step_loss : nan  learning_rate : 0.000338846942835921 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 04:21:26.150783] Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2817.935
DLL 2022-11-26 04:21:26.155680 - Training Epoch: 0 Training Iteration: 114  average_loss : nan  step_loss : nan  learning_rate : 0.0003418455883477433 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 04:21:28.940564] Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2789.743
DLL 2022-11-26 04:21:28.945460 - Training Epoch: 0 Training Iteration: 115  average_loss : nan  step_loss : nan  learning_rate : 0.0003448442338595657 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 04:21:31.690012] Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2749.425
DLL 2022-11-26 04:21:31.694900 - Training Epoch: 0 Training Iteration: 116  average_loss : nan  step_loss : nan  learning_rate : 0.00034784287937138794 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 04:21:34.448878] Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2758.834
DLL 2022-11-26 04:21:34.454079 - Training Epoch: 0 Training Iteration: 117  average_loss : nan  step_loss : nan  learning_rate : 0.00035084152488321026 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 04:21:37.216727] Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2767.817
DLL 2022-11-26 04:21:37.221798 - Training Epoch: 0 Training Iteration: 118  average_loss : nan  step_loss : nan  learning_rate : 0.0003538401703950326 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 04:21:40.012945] Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2796.191
DLL 2022-11-26 04:21:40.018129 - Training Epoch: 0 Training Iteration: 119  average_loss : nan  step_loss : nan  learning_rate : 0.0003568388159068549 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 04:21:42.807830] Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2794.848
DLL 2022-11-26 04:21:42.812769 - Training Epoch: 0 Training Iteration: 120  average_loss : nan  step_loss : nan  learning_rate : 0.0003598374614186772 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 04:21:45.583943] Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2776.093
DLL 2022-11-26 04:21:45.588899 - Training Epoch: 0 Training Iteration: 121  average_loss : nan  step_loss : nan  learning_rate : 0.00036283610693049946 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:21:48.392500] Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2808.528
DLL 2022-11-26 04:21:48.397442 - Training Epoch: 0 Training Iteration: 122  average_loss : nan  step_loss : nan  learning_rate : 0.0003658347524423219 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:21:51.148177] Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2755.672
DLL 2022-11-26 04:21:51.153052 - Training Epoch: 0 Training Iteration: 123  average_loss : nan  step_loss : nan  learning_rate : 0.00036883339795414414 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:21:53.957540] Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2809.318
DLL 2022-11-26 04:21:53.962215 - Training Epoch: 0 Training Iteration: 124  average_loss : nan  step_loss : nan  learning_rate : 0.0003718320434659664 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:21:56.729752] Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2772.166
DLL 2022-11-26 04:21:56.734876 - Training Epoch: 0 Training Iteration: 125  average_loss : nan  step_loss : nan  learning_rate : 0.00037483068897778876 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:21:59.540984] Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2811.208
DLL 2022-11-26 04:21:59.545854 - Training Epoch: 0 Training Iteration: 126  average_loss : nan  step_loss : nan  learning_rate : 0.0003778293344896111 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:02.318425] Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2777.431
DLL 2022-11-26 04:22:02.323449 - Training Epoch: 0 Training Iteration: 127  average_loss : nan  step_loss : nan  learning_rate : 0.0003808279800014334 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:05.114059] Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2795.674
DLL 2022-11-26 04:22:05.118852 - Training Epoch: 0 Training Iteration: 128  average_loss : nan  step_loss : nan  learning_rate : 0.00038382662551325565 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:07.877220] Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2763.043
DLL 2022-11-26 04:22:07.881979 - Training Epoch: 0 Training Iteration: 129  average_loss : nan  step_loss : nan  learning_rate : 0.00038682527102507796 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:10.675012] Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2797.773
DLL 2022-11-26 04:22:10.680027 - Training Epoch: 0 Training Iteration: 130  average_loss : nan  step_loss : nan  learning_rate : 0.00038982391653690033 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:13.482688] Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2807.641
DLL 2022-11-26 04:22:13.487527 - Training Epoch: 0 Training Iteration: 131  average_loss : nan  step_loss : nan  learning_rate : 0.0003928225620487226 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:16.229632] Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2746.919
DLL 2022-11-26 04:22:16.234867 - Training Epoch: 0 Training Iteration: 132  average_loss : nan  step_loss : nan  learning_rate : 0.0003958212075605449 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:19.024881] Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2795.214
DLL 2022-11-26 04:22:19.029719 - Training Epoch: 0 Training Iteration: 133  average_loss : nan  step_loss : nan  learning_rate : 0.00039881985307236727 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:21.835313] Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2810.404
DLL 2022-11-26 04:22:21.840203 - Training Epoch: 0 Training Iteration: 134  average_loss : nan  step_loss : nan  learning_rate : 0.0004018184985841895 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:24.608426] Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2773.075
DLL 2022-11-26 04:22:24.613264 - Training Epoch: 0 Training Iteration: 135  average_loss : nan  step_loss : nan  learning_rate : 0.00040481714409601184 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:27.358298] Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2749.854
DLL 2022-11-26 04:22:27.363199 - Training Epoch: 0 Training Iteration: 136  average_loss : nan  step_loss : nan  learning_rate : 0.00040781578960783415 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:30.135723] Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2777.395
DLL 2022-11-26 04:22:30.140580 - Training Epoch: 0 Training Iteration: 137  average_loss : nan  step_loss : nan  learning_rate : 0.0004108144351196565 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:32.907477] Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2771.723
DLL 2022-11-26 04:22:32.912260 - Training Epoch: 0 Training Iteration: 138  average_loss : nan  step_loss : nan  learning_rate : 0.0004138130806314788 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:35.671257] Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2763.763
DLL 2022-11-26 04:22:35.676042 - Training Epoch: 0 Training Iteration: 139  average_loss : nan  step_loss : nan  learning_rate : 0.00041681172614330104 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:38.428448] Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2757.131
DLL 2022-11-26 04:22:38.433240 - Training Epoch: 0 Training Iteration: 140  average_loss : nan  step_loss : nan  learning_rate : 0.0004198103716551234 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:41.190474] Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2762.016
DLL 2022-11-26 04:22:41.195389 - Training Epoch: 0 Training Iteration: 141  average_loss : nan  step_loss : nan  learning_rate : 0.0004228090171669457 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:43.961934] Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2771.428
DLL 2022-11-26 04:22:43.966934 - Training Epoch: 0 Training Iteration: 142  average_loss : nan  step_loss : nan  learning_rate : 0.00042580766267876803 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:47.349441] Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3387.511
DLL 2022-11-26 04:22:47.354359 - Training Epoch: 0 Training Iteration: 143  average_loss : nan  step_loss : nan  learning_rate : 0.0004288063081905903 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:50.145302] Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2795.824
DLL 2022-11-26 04:22:50.150180 - Training Epoch: 0 Training Iteration: 144  average_loss : nan  step_loss : nan  learning_rate : 0.00043180495370241266 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:52.972601] Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2827.257
DLL 2022-11-26 04:22:52.977614 - Training Epoch: 0 Training Iteration: 145  average_loss : nan  step_loss : nan  learning_rate : 0.00043480359921423497 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:55.741514] Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2768.899
DLL 2022-11-26 04:22:55.746368 - Training Epoch: 0 Training Iteration: 146  average_loss : nan  step_loss : nan  learning_rate : 0.00043780224472605723 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:22:58.513453] Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2771.900
DLL 2022-11-26 04:22:58.518608 - Training Epoch: 0 Training Iteration: 147  average_loss : nan  step_loss : nan  learning_rate : 0.0004408008902378796 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:01.299842] Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2786.363
DLL 2022-11-26 04:23:01.305042 - Training Epoch: 0 Training Iteration: 148  average_loss : nan  step_loss : nan  learning_rate : 0.0004437995357497019 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:04.106382] Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2806.490
DLL 2022-11-26 04:23:04.111391 - Training Epoch: 0 Training Iteration: 149  average_loss : nan  step_loss : nan  learning_rate : 0.0004467981812615243 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:07.081529] Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2975.108
DLL 2022-11-26 04:23:07.086567 - Training Epoch: 0 Training Iteration: 150  average_loss : nan  step_loss : nan  learning_rate : 0.00044979682677334654 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:09.884567] Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2803.014
DLL 2022-11-26 04:23:09.889636 - Training Epoch: 0 Training Iteration: 151  average_loss : nan  step_loss : nan  learning_rate : 0.0004527954722851688 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:12.663402] Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2778.803
DLL 2022-11-26 04:23:12.668176 - Training Epoch: 0 Training Iteration: 152  average_loss : nan  step_loss : nan  learning_rate : 0.0004557941177969911 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:15.462945] Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2799.522
DLL 2022-11-26 04:23:15.467798 - Training Epoch: 0 Training Iteration: 153  average_loss : nan  step_loss : nan  learning_rate : 0.00045879276330881337 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:18.293332] Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2830.347
DLL 2022-11-26 04:23:18.298189 - Training Epoch: 0 Training Iteration: 154  average_loss : nan  step_loss : nan  learning_rate : 0.00046179140882063573 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:21.102101] Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2808.775
DLL 2022-11-26 04:23:21.107109 - Training Epoch: 0 Training Iteration: 155  average_loss : nan  step_loss : nan  learning_rate : 0.00046479005433245805 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:23.906008] Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2803.846
DLL 2022-11-26 04:23:23.911224 - Training Epoch: 0 Training Iteration: 156  average_loss : nan  step_loss : nan  learning_rate : 0.0004677886998442803 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:26.844876] Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2938.843
DLL 2022-11-26 04:23:26.849739 - Training Epoch: 0 Training Iteration: 157  average_loss : nan  step_loss : nan  learning_rate : 0.0004707873453561027 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:29.659969] Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2815.095
DLL 2022-11-26 04:23:29.664942 - Training Epoch: 0 Training Iteration: 158  average_loss : nan  step_loss : nan  learning_rate : 0.00047378599086792493 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:32.455986] Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2795.956
DLL 2022-11-26 04:23:32.461093 - Training Epoch: 0 Training Iteration: 159  average_loss : nan  step_loss : nan  learning_rate : 0.0004767846363797473 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:35.292543] Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2836.515
DLL 2022-11-26 04:23:35.297654 - Training Epoch: 0 Training Iteration: 160  average_loss : nan  step_loss : nan  learning_rate : 0.0004797832818915696 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:38.076340] Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2783.776
DLL 2022-11-26 04:23:38.081562 - Training Epoch: 0 Training Iteration: 161  average_loss : nan  step_loss : nan  learning_rate : 0.00048278192740339187 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:40.942656] Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2866.293
DLL 2022-11-26 04:23:40.947880 - Training Epoch: 0 Training Iteration: 162  average_loss : nan  step_loss : nan  learning_rate : 0.00048578057291521424 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:43.753099] Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2810.408
DLL 2022-11-26 04:23:43.758116 - Training Epoch: 0 Training Iteration: 163  average_loss : nan  step_loss : nan  learning_rate : 0.0004887792184270365 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:46.542690] Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2789.577
DLL 2022-11-26 04:23:46.547820 - Training Epoch: 0 Training Iteration: 164  average_loss : nan  step_loss : nan  learning_rate : 0.0004917778639388588 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:49.305739] Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2763.005
DLL 2022-11-26 04:23:49.310441 - Training Epoch: 0 Training Iteration: 165  average_loss : nan  step_loss : nan  learning_rate : 0.0004947765094506812 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:52.052388] Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2746.616
DLL 2022-11-26 04:23:52.057183 - Training Epoch: 0 Training Iteration: 166  average_loss : nan  step_loss : nan  learning_rate : 0.0004977751549625034 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:54.817997] Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2765.583
DLL 2022-11-26 04:23:54.822935 - Training Epoch: 0 Training Iteration: 167  average_loss : nan  step_loss : nan  learning_rate : 0.0005007738004743258 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:23:57.584270] Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2766.243
DLL 2022-11-26 04:23:57.589277 - Training Epoch: 0 Training Iteration: 168  average_loss : nan  step_loss : nan  learning_rate : 0.0005037724459861481 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:00.354469] Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2770.174
DLL 2022-11-26 04:24:00.359238 - Training Epoch: 0 Training Iteration: 169  average_loss : nan  step_loss : nan  learning_rate : 0.0005067710914979704 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:03.169537] Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2815.019
DLL 2022-11-26 04:24:03.174358 - Training Epoch: 0 Training Iteration: 170  average_loss : nan  step_loss : nan  learning_rate : 0.0005097697370097927 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:05.952527] Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2782.985
DLL 2022-11-26 04:24:05.957494 - Training Epoch: 0 Training Iteration: 171  average_loss : nan  step_loss : nan  learning_rate : 0.0005127683825216149 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:08.717289] Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2764.717
DLL 2022-11-26 04:24:08.722063 - Training Epoch: 0 Training Iteration: 172  average_loss : nan  step_loss : nan  learning_rate : 0.0005157670280334373 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:11.500381] Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2783.067
DLL 2022-11-26 04:24:11.505349 - Training Epoch: 0 Training Iteration: 173  average_loss : nan  step_loss : nan  learning_rate : 0.0005187656735452596 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:14.256851] Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2756.444
DLL 2022-11-26 04:24:14.261796 - Training Epoch: 0 Training Iteration: 174  average_loss : nan  step_loss : nan  learning_rate : 0.0005217643190570819 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:17.021056] Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2764.160
DLL 2022-11-26 04:24:17.025860 - Training Epoch: 0 Training Iteration: 175  average_loss : nan  step_loss : nan  learning_rate : 0.0005247629645689043 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:19.831742] Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2810.696
DLL 2022-11-26 04:24:19.836501 - Training Epoch: 0 Training Iteration: 176  average_loss : nan  step_loss : nan  learning_rate : 0.0005277616100807266 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:22.582651] Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2750.873
DLL 2022-11-26 04:24:22.587759 - Training Epoch: 0 Training Iteration: 177  average_loss : nan  step_loss : nan  learning_rate : 0.0005307602555925489 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:25.339604] Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2756.902
DLL 2022-11-26 04:24:25.344572 - Training Epoch: 0 Training Iteration: 178  average_loss : nan  step_loss : nan  learning_rate : 0.0005337589011043712 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:28.182447] Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2842.828
DLL 2022-11-26 04:24:28.187551 - Training Epoch: 0 Training Iteration: 179  average_loss : nan  step_loss : nan  learning_rate : 0.0005367575466161935 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:30.943856] Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2761.371
DLL 2022-11-26 04:24:30.948953 - Training Epoch: 0 Training Iteration: 180  average_loss : nan  step_loss : nan  learning_rate : 0.0005397561921280158 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:33.719722] Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2775.859
DLL 2022-11-26 04:24:33.724541 - Training Epoch: 0 Training Iteration: 181  average_loss : nan  step_loss : nan  learning_rate : 0.0005427548376398381 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:36.527204] Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2807.446
DLL 2022-11-26 04:24:36.532075 - Training Epoch: 0 Training Iteration: 182  average_loss : nan  step_loss : nan  learning_rate : 0.0005457534831516603 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:39.254040] Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2726.855
DLL 2022-11-26 04:24:39.258941 - Training Epoch: 0 Training Iteration: 183  average_loss : nan  step_loss : nan  learning_rate : 0.0005487521286634827 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:42.020236] Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2766.117
DLL 2022-11-26 04:24:42.025421 - Training Epoch: 0 Training Iteration: 184  average_loss : nan  step_loss : nan  learning_rate : 0.0005517507741753051 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:44.809544] Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2789.245
DLL 2022-11-26 04:24:44.814601 - Training Epoch: 0 Training Iteration: 185  average_loss : nan  step_loss : nan  learning_rate : 0.0005547494196871273 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:47.571473] Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2761.914
DLL 2022-11-26 04:24:47.576437 - Training Epoch: 0 Training Iteration: 186  average_loss : nan  step_loss : nan  learning_rate : 0.0005577480651989497 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:50.362895] Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2791.410
DLL 2022-11-26 04:24:50.367671 - Training Epoch: 0 Training Iteration: 187  average_loss : nan  step_loss : nan  learning_rate : 0.000560746710710772 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:53.141473] Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2778.524
DLL 2022-11-26 04:24:53.146695 - Training Epoch: 0 Training Iteration: 188  average_loss : nan  step_loss : nan  learning_rate : 0.0005637453562225942 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:55.902989] Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2761.485
DLL 2022-11-26 04:24:55.907813 - Training Epoch: 0 Training Iteration: 189  average_loss : nan  step_loss : nan  learning_rate : 0.0005667440017344166 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:24:58.644026] Finished iteration 189, CKPT_AND_STOP: True, flag: tensor([2], dtype=torch.int32), speed: 2741.026
DLL 2022-11-26 04:24:58.648919 - Training Epoch: 0 Training Iteration: 190  average_loss : nan  step_loss : nan  learning_rate : 0.0005697426472462388 
2022-11-26 04:24:58.649056 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:24:58.649079 - PARAMETER checkpoint_step : 190 
Opt ckpt time 9.705780506134033
Process done with return code 0
Parent process ID: 65973 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2325752.685546875 0
End of simulation:  Mini-batch time (usec) = 4300398
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 51 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 226 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 597 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1178 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3205 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6719 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22046 microseconds

{1: 4.300398, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 190
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:25:20.578569 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=93, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=190, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;', chunk_size=8, batch_size=93, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.1663494110107422
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_190.pt
2022-11-26 04:25:30.877573 resume step from  190
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:26:16.894310 - Finished loading checkpoint, takes 45.989 secs
DLL 2022-11-26 04:26:16.895395 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:26:16.895521 - PARAMETER train_start : True 
DLL 2022-11-26 04:26:16.895615 - PARAMETER batch_size_per_gpu : 93 
DLL 2022-11-26 04:26:16.895684 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:26:39.928032] Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4667.120
DLL 2022-11-26 04:26:39.933546 - Training Epoch: 0 Training Iteration: 191  average_loss : nan  step_loss : nan  learning_rate : 0.0005727412927580613 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 04:26:42.505204] Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.940
DLL 2022-11-26 04:26:42.510520 - Training Epoch: 0 Training Iteration: 192  average_loss : nan  step_loss : nan  learning_rate : 0.0005757399382698836 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 04:26:46.094655] Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3589.457
DLL 2022-11-26 04:26:46.100034 - Training Epoch: 0 Training Iteration: 193  average_loss : nan  step_loss : nan  learning_rate : 0.0005787385837817058 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 04:26:48.690991] Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2596.282
DLL 2022-11-26 04:26:48.699402 - Training Epoch: 0 Training Iteration: 194  average_loss : nan  step_loss : nan  learning_rate : 0.0005817372292935281 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 04:26:52.276495] Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3585.502
DLL 2022-11-26 04:26:52.281922 - Training Epoch: 0 Training Iteration: 195  average_loss : nan  step_loss : nan  learning_rate : 0.0005847358748053504 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 04:26:54.906531] Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2629.989
DLL 2022-11-26 04:26:54.911506 - Training Epoch: 0 Training Iteration: 196  average_loss : nan  step_loss : nan  learning_rate : 0.0005877345203171727 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 04:26:57.550834] Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2644.228
DLL 2022-11-26 04:26:57.555563 - Training Epoch: 0 Training Iteration: 197  average_loss : nan  step_loss : nan  learning_rate : 0.000590733165828995 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 04:27:00.163904] Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2613.053
DLL 2022-11-26 04:27:00.169100 - Training Epoch: 0 Training Iteration: 198  average_loss : nan  step_loss : nan  learning_rate : 0.0005937318113408173 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 04:27:02.754298] Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2590.347
DLL 2022-11-26 04:27:02.759271 - Training Epoch: 0 Training Iteration: 199  average_loss : nan  step_loss : nan  learning_rate : 0.0005967304568526397 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 04:27:05.301523] Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2547.200
DLL 2022-11-26 04:27:05.306696 - Training Epoch: 0 Training Iteration: 200  average_loss : nan  step_loss : nan  learning_rate : 0.000599729102364462 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 04:27:07.837991] Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2536.440
DLL 2022-11-26 04:27:07.843247 - Training Epoch: 0 Training Iteration: 201  average_loss : nan  step_loss : nan  learning_rate : 0.0006027277478762843 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 04:27:10.396546] Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2558.537
DLL 2022-11-26 04:27:10.401412 - Training Epoch: 0 Training Iteration: 202  average_loss : nan  step_loss : nan  learning_rate : 0.0006057263933881066 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 04:27:12.949226] Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2552.662
DLL 2022-11-26 04:27:12.954399 - Training Epoch: 0 Training Iteration: 203  average_loss : nan  step_loss : nan  learning_rate : 0.0006087250388999289 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 04:27:15.552373] Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2603.108
DLL 2022-11-26 04:27:15.557452 - Training Epoch: 0 Training Iteration: 204  average_loss : nan  step_loss : nan  learning_rate : 0.0006117236844117512 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 04:27:18.421789] Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2869.386
DLL 2022-11-26 04:27:18.426893 - Training Epoch: 0 Training Iteration: 205  average_loss : nan  step_loss : nan  learning_rate : 0.0006147223299235735 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 04:27:20.989177] Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2567.381
DLL 2022-11-26 04:27:20.994599 - Training Epoch: 0 Training Iteration: 206  average_loss : nan  step_loss : nan  learning_rate : 0.000617720975435396 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 04:27:23.573810] Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2584.615
DLL 2022-11-26 04:27:23.579144 - Training Epoch: 0 Training Iteration: 207  average_loss : nan  step_loss : nan  learning_rate : 0.0006207196209472182 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 04:27:26.150611] Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.757
DLL 2022-11-26 04:27:26.155489 - Training Epoch: 0 Training Iteration: 208  average_loss : nan  step_loss : nan  learning_rate : 0.0006237182664590405 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 04:27:28.702372] Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2551.714
DLL 2022-11-26 04:27:28.707328 - Training Epoch: 0 Training Iteration: 209  average_loss : nan  step_loss : nan  learning_rate : 0.0006267169119708628 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:31.259846] Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2557.456
DLL 2022-11-26 04:27:31.264894 - Training Epoch: 0 Training Iteration: 210  average_loss : nan  step_loss : nan  learning_rate : 0.000629715557482685 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:33.846692] Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2586.817
DLL 2022-11-26 04:27:33.851499 - Training Epoch: 0 Training Iteration: 211  average_loss : nan  step_loss : nan  learning_rate : 0.0006327142029945074 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:36.384965] Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2538.234
DLL 2022-11-26 04:27:36.389914 - Training Epoch: 0 Training Iteration: 212  average_loss : nan  step_loss : nan  learning_rate : 0.0006357128485063296 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:38.955141] Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2570.149
DLL 2022-11-26 04:27:38.959968 - Training Epoch: 0 Training Iteration: 213  average_loss : nan  step_loss : nan  learning_rate : 0.000638711494018152 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:41.493920] Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2538.772
DLL 2022-11-26 04:27:41.498799 - Training Epoch: 0 Training Iteration: 214  average_loss : nan  step_loss : nan  learning_rate : 0.0006417101395299744 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:44.069452] Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2575.472
DLL 2022-11-26 04:27:44.074320 - Training Epoch: 0 Training Iteration: 215  average_loss : nan  step_loss : nan  learning_rate : 0.0006447087850417966 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:46.641644] Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2572.171
DLL 2022-11-26 04:27:46.646641 - Training Epoch: 0 Training Iteration: 216  average_loss : nan  step_loss : nan  learning_rate : 0.000647707430553619 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:49.186529] Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2544.850
DLL 2022-11-26 04:27:49.191271 - Training Epoch: 0 Training Iteration: 217  average_loss : nan  step_loss : nan  learning_rate : 0.0006507060760654413 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:51.730102] Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2543.585
DLL 2022-11-26 04:27:51.735306 - Training Epoch: 0 Training Iteration: 218  average_loss : nan  step_loss : nan  learning_rate : 0.0006537047215772636 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:54.346713] Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2616.549
DLL 2022-11-26 04:27:54.351550 - Training Epoch: 0 Training Iteration: 219  average_loss : nan  step_loss : nan  learning_rate : 0.0006567033670890859 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:56.907950] Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2561.197
DLL 2022-11-26 04:27:56.912964 - Training Epoch: 0 Training Iteration: 220  average_loss : nan  step_loss : nan  learning_rate : 0.0006597020126009082 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:27:59.515946] Finished iteration 220, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 2607.997
DLL 2022-11-26 04:27:59.520727 - Training Epoch: 0 Training Iteration: 221  average_loss : nan  step_loss : nan  learning_rate : 0.0006627006581127306 
2022-11-26 04:27:59.520879 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:27:59.520900 - PARAMETER checkpoint_step : 221 
Opt ckpt time 8.271714687347412
Process done with return code 0
Parent process ID: 67504 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 49 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 9 0 970375.4272460938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3283515
Min send: 10000000, max send 0
Min long send: 249051, max long send 268766
Min fwd: 79750, max fwd 86627; min bwd 87708, max bwd 94754
Min long fwd: 58189, max long fwd 63385; min long bwd 92867, max long bwd 97330
Time taken by simulation: 190 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 13 0 485476.6540527344 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3469734
Min send: 10000000, max send 0
Min long send: 248801, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 53890, max bwd 64826
Min long fwd: 37108, max long fwd 45662; min long bwd 64919, max long bwd 72155
Time taken by simulation: 448 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 19 0 352272.7355957031 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4275253
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 22410, max fwd 61283; min bwd 39748, max bwd 51303
Min long fwd: 29385, max long fwd 37648; min long bwd 47928, max long bwd 54735
Time taken by simulation: 938 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 26 0 254962.82958984375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5944030
Min send: 10000000, max send 0
Min long send: 248735, max long send 278723
Min fwd: 10836, max fwd 44441; min bwd 24199, max bwd 35953
Min long fwd: 21303, max long fwd 29429; min long bwd 33298, max long bwd 40795
Time taken by simulation: 1981 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4564 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10822 microseconds

{1: 4.144382, 2: 3.283515, 3: 3.469734, 4: 4.275253, 6: 5.94403, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.283515
15 per stage
30 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 15
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29;
World size is 30
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29; --batch-size=68 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 221
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:28:22.259320 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=68, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=221, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24,26,28;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29;', chunk_size=8, batch_size=68, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.18011093139648438
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
9 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_221.pt
2022-11-26 04:28:32.660812 resume step from  221
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
2022-11-26 04:29:20.154669 - Finished loading checkpoint, takes 47.466 secs
DLL 2022-11-26 04:29:20.155491 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:29:20.155607 - PARAMETER train_start : True 
DLL 2022-11-26 04:29:20.155666 - PARAMETER batch_size_per_gpu : 68 
DLL 2022-11-26 04:29:20.155701 - PARAMETER learning_rate : 0.006 
2022-11-26 04:29:40.048470 Begin to exit
Process done with return code 0
Parent process ID: 68858 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 4 0 3598379.5166015625 0
End of simulation:  Mini-batch time (usec) = 4923429
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 142174, max long fwd 144458; min long bwd 186536, max long bwd 191040
Time taken by simulation: 45 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 8 0 954047.119140625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3096062
Min send: 10000000, max send 0
Min long send: 249051, max long send 268766
Min fwd: 79507, max fwd 86627; min bwd 88131, max bwd 94131
Min long fwd: 58556, max long fwd 63385; min long bwd 92867, max long bwd 97582
Time taken by simulation: 222 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 13 0 485476.6540527344 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3469734
Min send: 10000000, max send 0
Min long send: 248801, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 53890, max bwd 64826
Min long fwd: 37108, max long fwd 45662; min long bwd 64919, max long bwd 72155
Time taken by simulation: 473 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 364488.09814453125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3900556
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 20622, max fwd 60312; min bwd 40311, max bwd 50179
Min long fwd: 29744, max long fwd 36491; min long bwd 47928, max long bwd 56568
Time taken by simulation: 732 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 26 0 254962.82958984375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5944030
Min send: 10000000, max send 0
Min long send: 248735, max long send 278723
Min fwd: 10836, max fwd 44441; min bwd 24199, max bwd 35953
Min long fwd: 21303, max long fwd 29429; min long bwd 33298, max long bwd 40795
Time taken by simulation: 1944 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 32 0 244280.30395507812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7674225
Min send: 10000000, max send 0
Min long send: 248719, max long send 278723
Min fwd: 6779, max fwd 41103; min bwd 15994, max bwd 28589
Min long fwd: 18906, max long fwd 26091; min long bwd 25267, max long bwd 32480
Time taken by simulation: 3388 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11182 microseconds

{1: 4.923429, 2: 3.096062, 3: 3.469734, 4: 3.900556, 6: 5.94403, 8: 7.674225, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.096062
16 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 16
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31; --batch-size=64 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 221
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:29:50.715048 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=64, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=221, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;', chunk_size=8, batch_size=64, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.1880512237548828
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
8 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_221.pt
2022-11-26 04:30:01.145840 resume step from  221
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:30:47.485577 - Finished loading checkpoint, takes 46.312 secs
DLL 2022-11-26 04:30:47.486575 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:30:47.486695 - PARAMETER train_start : True 
DLL 2022-11-26 04:30:47.486771 - PARAMETER batch_size_per_gpu : 64 
DLL 2022-11-26 04:30:47.486850 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:31:09.082515] Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4239.479
DLL 2022-11-26 04:31:09.088318 - Training Epoch: 0 Training Iteration: 222  average_loss : nan  step_loss : nan  learning_rate : 0.0006656993036245529 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 04:31:12.284097] Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3201.412
DLL 2022-11-26 04:31:12.289821 - Training Epoch: 0 Training Iteration: 223  average_loss : nan  step_loss : nan  learning_rate : 0.0006686979491363751 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 04:31:14.421325] Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2137.162
DLL 2022-11-26 04:31:14.426537 - Training Epoch: 0 Training Iteration: 224  average_loss : nan  step_loss : nan  learning_rate : 0.0006716965946481975 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 04:31:18.037113] Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3615.773
DLL 2022-11-26 04:31:18.042591 - Training Epoch: 0 Training Iteration: 225  average_loss : nan  step_loss : nan  learning_rate : 0.0006746952401600198 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 04:31:21.212027] Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3174.924
DLL 2022-11-26 04:31:21.217428 - Training Epoch: 0 Training Iteration: 226  average_loss : nan  step_loss : nan  learning_rate : 0.000677693885671842 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 04:31:23.368664] Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2156.535
DLL 2022-11-26 04:31:23.373869 - Training Epoch: 0 Training Iteration: 227  average_loss : nan  step_loss : nan  learning_rate : 0.0006806925311836643 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 04:31:25.503163] Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2134.482
DLL 2022-11-26 04:31:25.508532 - Training Epoch: 0 Training Iteration: 228  average_loss : nan  step_loss : nan  learning_rate : 0.0006836911766954866 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 04:31:27.643305] Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2140.099
DLL 2022-11-26 04:31:27.648307 - Training Epoch: 0 Training Iteration: 229  average_loss : nan  step_loss : nan  learning_rate : 0.0006866898222073091 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 04:31:29.828594] Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2185.251
DLL 2022-11-26 04:31:29.833720 - Training Epoch: 0 Training Iteration: 230  average_loss : nan  step_loss : nan  learning_rate : 0.0006896884677191314 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 04:31:32.006498] Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2177.884
DLL 2022-11-26 04:31:32.011708 - Training Epoch: 0 Training Iteration: 231  average_loss : nan  step_loss : nan  learning_rate : 0.0006926871132309536 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 04:31:34.232586] Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2226.072
DLL 2022-11-26 04:31:34.237719 - Training Epoch: 0 Training Iteration: 232  average_loss : nan  step_loss : nan  learning_rate : 0.0006956857587427759 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 04:31:36.495439] Finished iteration 232, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2262.848
DLL 2022-11-26 04:31:36.500631 - Training Epoch: 0 Training Iteration: 233  average_loss : nan  step_loss : nan  learning_rate : 0.0006986844042545982 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 04:31:38.689960] Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2194.474
DLL 2022-11-26 04:31:38.695046 - Training Epoch: 0 Training Iteration: 234  average_loss : nan  step_loss : nan  learning_rate : 0.0007016830497664205 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 04:31:40.857074] Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2167.073
DLL 2022-11-26 04:31:40.862111 - Training Epoch: 0 Training Iteration: 235  average_loss : nan  step_loss : nan  learning_rate : 0.0007046816952782429 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 04:31:43.062386] Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2205.357
DLL 2022-11-26 04:31:43.069533 - Training Epoch: 0 Training Iteration: 236  average_loss : nan  step_loss : nan  learning_rate : 0.0007076803407900652 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 04:31:45.947513] Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2885.036
DLL 2022-11-26 04:31:45.952541 - Training Epoch: 0 Training Iteration: 237  average_loss : nan  step_loss : nan  learning_rate : 0.0007106789863018875 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 04:31:48.129312] Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2181.744
DLL 2022-11-26 04:31:48.134290 - Training Epoch: 0 Training Iteration: 238  average_loss : nan  step_loss : nan  learning_rate : 0.0007136776318137098 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 04:31:50.346928] Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2217.594
DLL 2022-11-26 04:31:50.352413 - Training Epoch: 0 Training Iteration: 239  average_loss : nan  step_loss : nan  learning_rate : 0.0007166762773255321 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 04:31:52.562107] Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2215.176
DLL 2022-11-26 04:31:52.567570 - Training Epoch: 0 Training Iteration: 240  average_loss : nan  step_loss : nan  learning_rate : 0.0007196749228373544 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:31:54.715896] Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2153.761
DLL 2022-11-26 04:31:54.721081 - Training Epoch: 0 Training Iteration: 241  average_loss : nan  step_loss : nan  learning_rate : 0.0007226735683491767 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:31:56.920892] Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2204.963
DLL 2022-11-26 04:31:56.926378 - Training Epoch: 0 Training Iteration: 242  average_loss : nan  step_loss : nan  learning_rate : 0.0007256722138609989 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:31:59.140934] Finished iteration 242, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2219.990
DLL 2022-11-26 04:31:59.146339 - Training Epoch: 0 Training Iteration: 243  average_loss : nan  step_loss : nan  learning_rate : 0.0007286708593728212 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:01.344877] Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2203.918
DLL 2022-11-26 04:32:01.350004 - Training Epoch: 0 Training Iteration: 244  average_loss : nan  step_loss : nan  learning_rate : 0.0007316695048846438 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:03.587362] Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2242.460
DLL 2022-11-26 04:32:03.592482 - Training Epoch: 0 Training Iteration: 245  average_loss : nan  step_loss : nan  learning_rate : 0.000734668150396466 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:05.791852] Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2204.462
DLL 2022-11-26 04:32:05.796727 - Training Epoch: 0 Training Iteration: 246  average_loss : nan  step_loss : nan  learning_rate : 0.0007376667959082883 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:07.996525] Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2204.652
DLL 2022-11-26 04:32:08.001715 - Training Epoch: 0 Training Iteration: 247  average_loss : nan  step_loss : nan  learning_rate : 0.0007406654414201106 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:10.220567] Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2223.998
DLL 2022-11-26 04:32:10.225711 - Training Epoch: 0 Training Iteration: 248  average_loss : nan  step_loss : nan  learning_rate : 0.0007436640869319328 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:12.406293] Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2185.709
DLL 2022-11-26 04:32:12.411328 - Training Epoch: 0 Training Iteration: 249  average_loss : nan  step_loss : nan  learning_rate : 0.0007466627324437552 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:14.617591] Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2211.250
DLL 2022-11-26 04:32:14.622701 - Training Epoch: 0 Training Iteration: 250  average_loss : nan  step_loss : nan  learning_rate : 0.0007496613779555775 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:16.816435] Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2198.820
DLL 2022-11-26 04:32:16.821790 - Training Epoch: 0 Training Iteration: 251  average_loss : nan  step_loss : nan  learning_rate : 0.0007526600234673998 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:19.050347] Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2233.885
DLL 2022-11-26 04:32:19.055514 - Training Epoch: 0 Training Iteration: 252  average_loss : nan  step_loss : nan  learning_rate : 0.0007556586689792222 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:21.255903] Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2205.523
DLL 2022-11-26 04:32:21.260766 - Training Epoch: 0 Training Iteration: 253  average_loss : nan  step_loss : nan  learning_rate : 0.0007586573144910444 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:23.419395] Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2163.463
DLL 2022-11-26 04:32:23.424804 - Training Epoch: 0 Training Iteration: 254  average_loss : nan  step_loss : nan  learning_rate : 0.0007616559600028668 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:26.357166] Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2937.732
DLL 2022-11-26 04:32:26.362119 - Training Epoch: 0 Training Iteration: 255  average_loss : nan  step_loss : nan  learning_rate : 0.0007646546055146891 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:28.542329] Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2185.148
DLL 2022-11-26 04:32:28.547265 - Training Epoch: 0 Training Iteration: 256  average_loss : nan  step_loss : nan  learning_rate : 0.0007676532510265113 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:30.716202] Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2173.861
DLL 2022-11-26 04:32:30.721695 - Training Epoch: 0 Training Iteration: 257  average_loss : nan  step_loss : nan  learning_rate : 0.0007706518965383336 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:32.921479] Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2205.242
DLL 2022-11-26 04:32:32.926419 - Training Epoch: 0 Training Iteration: 258  average_loss : nan  step_loss : nan  learning_rate : 0.0007736505420501559 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:35.129641] Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2208.137
DLL 2022-11-26 04:32:35.134720 - Training Epoch: 0 Training Iteration: 259  average_loss : nan  step_loss : nan  learning_rate : 0.0007766491875619783 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:37.323173] Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2193.491
DLL 2022-11-26 04:32:37.328201 - Training Epoch: 0 Training Iteration: 260  average_loss : nan  step_loss : nan  learning_rate : 0.0007796478330738007 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:39.503907] Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2180.715
DLL 2022-11-26 04:32:39.509044 - Training Epoch: 0 Training Iteration: 261  average_loss : nan  step_loss : nan  learning_rate : 0.0007826464785856229 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:41.665208] Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2161.275
DLL 2022-11-26 04:32:41.670658 - Training Epoch: 0 Training Iteration: 262  average_loss : nan  step_loss : nan  learning_rate : 0.0007856451240974452 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:43.904373] Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2239.136
DLL 2022-11-26 04:32:43.909481 - Training Epoch: 0 Training Iteration: 263  average_loss : nan  step_loss : nan  learning_rate : 0.0007886437696092675 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:46.130576] Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2226.182
DLL 2022-11-26 04:32:46.135765 - Training Epoch: 0 Training Iteration: 264  average_loss : nan  step_loss : nan  learning_rate : 0.0007916424151210898 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:48.309525] Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2178.904
DLL 2022-11-26 04:32:48.314617 - Training Epoch: 0 Training Iteration: 265  average_loss : nan  step_loss : nan  learning_rate : 0.0007946410606329121 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:50.490123] Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2180.613
DLL 2022-11-26 04:32:50.495596 - Training Epoch: 0 Training Iteration: 266  average_loss : nan  step_loss : nan  learning_rate : 0.0007976397061447345 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:52.706606] Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2216.421
DLL 2022-11-26 04:32:52.711642 - Training Epoch: 0 Training Iteration: 267  average_loss : nan  step_loss : nan  learning_rate : 0.0008006383516565567 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:54.906710] Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2200.073
DLL 2022-11-26 04:32:54.912219 - Training Epoch: 0 Training Iteration: 268  average_loss : nan  step_loss : nan  learning_rate : 0.000803636997168379 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:57.129602] Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2222.885
DLL 2022-11-26 04:32:57.134774 - Training Epoch: 0 Training Iteration: 269  average_loss : nan  step_loss : nan  learning_rate : 0.0008066356426802014 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:32:59.337701] Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2208.071
DLL 2022-11-26 04:32:59.342686 - Training Epoch: 0 Training Iteration: 270  average_loss : nan  step_loss : nan  learning_rate : 0.0008096342881920237 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:01.575570] Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2237.857
DLL 2022-11-26 04:33:01.580505 - Training Epoch: 0 Training Iteration: 271  average_loss : nan  step_loss : nan  learning_rate : 0.000812632933703846 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:03.775140] Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2199.539
DLL 2022-11-26 04:33:03.780239 - Training Epoch: 0 Training Iteration: 272  average_loss : nan  step_loss : nan  learning_rate : 0.0008156315792156683 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:05.974129] Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2198.999
DLL 2022-11-26 04:33:05.979539 - Training Epoch: 0 Training Iteration: 273  average_loss : nan  step_loss : nan  learning_rate : 0.0008186302247274905 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:08.179159] Finished iteration 273, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2204.941
DLL 2022-11-26 04:33:08.184742 - Training Epoch: 0 Training Iteration: 274  average_loss : nan  step_loss : nan  learning_rate : 0.000821628870239313 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:10.434874] Finished iteration 274, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2255.682
DLL 2022-11-26 04:33:10.440268 - Training Epoch: 0 Training Iteration: 275  average_loss : nan  step_loss : nan  learning_rate : 0.0008246275157511352 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:12.704403] Finished iteration 275, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2269.498
DLL 2022-11-26 04:33:12.709315 - Training Epoch: 0 Training Iteration: 276  average_loss : nan  step_loss : nan  learning_rate : 0.0008276261612629576 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:14.891187] Finished iteration 276, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2186.779
DLL 2022-11-26 04:33:14.896527 - Training Epoch: 0 Training Iteration: 277  average_loss : nan  step_loss : nan  learning_rate : 0.0008306248067747799 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:17.061056] Finished iteration 277, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2169.830
DLL 2022-11-26 04:33:17.066373 - Training Epoch: 0 Training Iteration: 278  average_loss : nan  step_loss : nan  learning_rate : 0.0008336234522866021 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:19.241194] Finished iteration 278, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2180.113
DLL 2022-11-26 04:33:19.246632 - Training Epoch: 0 Training Iteration: 279  average_loss : nan  step_loss : nan  learning_rate : 0.0008366220977984245 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:21.415545] Finished iteration 279, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2174.303
DLL 2022-11-26 04:33:21.420819 - Training Epoch: 0 Training Iteration: 280  average_loss : nan  step_loss : nan  learning_rate : 0.0008396207433102468 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:23.657658] Finished iteration 280, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2242.113
DLL 2022-11-26 04:33:23.663202 - Training Epoch: 0 Training Iteration: 281  average_loss : nan  step_loss : nan  learning_rate : 0.000842619388822069 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:25.886368] Finished iteration 281, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2228.667
DLL 2022-11-26 04:33:25.891452 - Training Epoch: 0 Training Iteration: 282  average_loss : nan  step_loss : nan  learning_rate : 0.0008456180343338914 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:28.032943] Finished iteration 282, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2146.559
DLL 2022-11-26 04:33:28.038401 - Training Epoch: 0 Training Iteration: 283  average_loss : nan  step_loss : nan  learning_rate : 0.0008486166798457137 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:30.195946] Finished iteration 283, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2162.981
DLL 2022-11-26 04:33:30.201348 - Training Epoch: 0 Training Iteration: 284  average_loss : nan  step_loss : nan  learning_rate : 0.0008516153253575361 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:32.407931] Finished iteration 284, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2211.936
DLL 2022-11-26 04:33:32.413401 - Training Epoch: 0 Training Iteration: 285  average_loss : nan  step_loss : nan  learning_rate : 0.0008546139708693584 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:34.585880] Finished iteration 285, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2177.930
DLL 2022-11-26 04:33:34.591463 - Training Epoch: 0 Training Iteration: 286  average_loss : nan  step_loss : nan  learning_rate : 0.0008576126163811806 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:36.743122] Finished iteration 286, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2157.210
DLL 2022-11-26 04:33:36.748163 - Training Epoch: 0 Training Iteration: 287  average_loss : nan  step_loss : nan  learning_rate : 0.000860611261893003 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:38.933936] Finished iteration 287, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2190.789
DLL 2022-11-26 04:33:38.939039 - Training Epoch: 0 Training Iteration: 288  average_loss : nan  step_loss : nan  learning_rate : 0.0008636099074048253 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:41.179647] Finished iteration 288, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2245.693
DLL 2022-11-26 04:33:41.185171 - Training Epoch: 0 Training Iteration: 289  average_loss : nan  step_loss : nan  learning_rate : 0.0008666085529166475 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:43.300722] Finished iteration 289, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2121.017
DLL 2022-11-26 04:33:43.305587 - Training Epoch: 0 Training Iteration: 290  average_loss : nan  step_loss : nan  learning_rate : 0.0008696071984284699 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:45.463106] Finished iteration 290, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2162.375
DLL 2022-11-26 04:33:45.468145 - Training Epoch: 0 Training Iteration: 291  average_loss : nan  step_loss : nan  learning_rate : 0.0008726058439402921 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:47.659839] Finished iteration 291, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2196.696
DLL 2022-11-26 04:33:47.665090 - Training Epoch: 0 Training Iteration: 292  average_loss : nan  step_loss : nan  learning_rate : 0.0008756044894521145 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:49.924141] Finished iteration 292, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2264.272
DLL 2022-11-26 04:33:49.929365 - Training Epoch: 0 Training Iteration: 293  average_loss : nan  step_loss : nan  learning_rate : 0.0008786031349639369 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:52.140160] Finished iteration 293, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2216.070
DLL 2022-11-26 04:33:52.223785 - Training Epoch: 0 Training Iteration: 294  average_loss : nan  step_loss : nan  learning_rate : 0.0008816017804757592 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:54.420411] Finished iteration 294, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2280.168
DLL 2022-11-26 04:33:54.425414 - Training Epoch: 0 Training Iteration: 295  average_loss : nan  step_loss : nan  learning_rate : 0.0008846004259875814 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:56.616882] Finished iteration 295, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2196.444
DLL 2022-11-26 04:33:56.622066 - Training Epoch: 0 Training Iteration: 296  average_loss : nan  step_loss : nan  learning_rate : 0.0008875990714994038 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:33:58.797090] Finished iteration 296, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2180.195
DLL 2022-11-26 04:33:58.802723 - Training Epoch: 0 Training Iteration: 297  average_loss : nan  step_loss : nan  learning_rate : 0.0008905977170112259 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:01.266449] Finished iteration 297, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2469.326
DLL 2022-11-26 04:34:01.272179 - Training Epoch: 0 Training Iteration: 298  average_loss : nan  step_loss : nan  learning_rate : 0.0008935963625230486 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:04.016912] Finished iteration 298, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2750.387
DLL 2022-11-26 04:34:04.022006 - Training Epoch: 0 Training Iteration: 299  average_loss : nan  step_loss : nan  learning_rate : 0.0008965950080348708 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:06.204907] Finished iteration 299, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2187.961
DLL 2022-11-26 04:34:06.209996 - Training Epoch: 0 Training Iteration: 300  average_loss : nan  step_loss : nan  learning_rate : 0.0008995936535466931 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:08.456466] Finished iteration 300, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2251.550
DLL 2022-11-26 04:34:08.461625 - Training Epoch: 0 Training Iteration: 301  average_loss : nan  step_loss : nan  learning_rate : 0.0009025922990585153 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:10.628558] Finished iteration 301, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2172.054
DLL 2022-11-26 04:34:10.633607 - Training Epoch: 0 Training Iteration: 302  average_loss : nan  step_loss : nan  learning_rate : 0.0009055909445703376 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:12.813121] Finished iteration 302, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2184.562
DLL 2022-11-26 04:34:12.818335 - Training Epoch: 0 Training Iteration: 303  average_loss : nan  step_loss : nan  learning_rate : 0.00090858959008216 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:14.971014] Finished iteration 303, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2157.836
DLL 2022-11-26 04:34:14.976255 - Training Epoch: 0 Training Iteration: 304  average_loss : nan  step_loss : nan  learning_rate : 0.0009115882355939822 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:17.174671] Finished iteration 304, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2203.638
DLL 2022-11-26 04:34:17.180016 - Training Epoch: 0 Training Iteration: 305  average_loss : nan  step_loss : nan  learning_rate : 0.0009145868811058045 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:19.387406] Finished iteration 305, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2212.726
DLL 2022-11-26 04:34:19.393042 - Training Epoch: 0 Training Iteration: 306  average_loss : nan  step_loss : nan  learning_rate : 0.0009175855266176267 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:21.596961] Finished iteration 306, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2209.514
DLL 2022-11-26 04:34:21.601959 - Training Epoch: 0 Training Iteration: 307  average_loss : nan  step_loss : nan  learning_rate : 0.0009205841721294492 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:23.813532] Finished iteration 307, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2216.556
DLL 2022-11-26 04:34:23.819021 - Training Epoch: 0 Training Iteration: 308  average_loss : nan  step_loss : nan  learning_rate : 0.0009235828176412715 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:26.000937] Finished iteration 308, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2187.380
DLL 2022-11-26 04:34:26.006588 - Training Epoch: 0 Training Iteration: 309  average_loss : nan  step_loss : nan  learning_rate : 0.0009265814631530939 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:28.151869] Finished iteration 309, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2150.880
DLL 2022-11-26 04:34:28.156861 - Training Epoch: 0 Training Iteration: 310  average_loss : nan  step_loss : nan  learning_rate : 0.0009295801086649161 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:30.347814] Finished iteration 310, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2195.919
DLL 2022-11-26 04:34:30.352764 - Training Epoch: 0 Training Iteration: 311  average_loss : nan  step_loss : nan  learning_rate : 0.0009325787541767384 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:32.526434] Finished iteration 311, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2178.602
DLL 2022-11-26 04:34:32.531593 - Training Epoch: 0 Training Iteration: 312  average_loss : nan  step_loss : nan  learning_rate : 0.0009355773996885606 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:34.680424] Finished iteration 312, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2153.965
DLL 2022-11-26 04:34:34.685589 - Training Epoch: 0 Training Iteration: 313  average_loss : nan  step_loss : nan  learning_rate : 0.0009385760452003831 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:36.833787] Finished iteration 313, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2153.315
DLL 2022-11-26 04:34:36.838872 - Training Epoch: 0 Training Iteration: 314  average_loss : nan  step_loss : nan  learning_rate : 0.0009415746907122053 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:38.979019] Finished iteration 314, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2145.221
DLL 2022-11-26 04:34:38.984269 - Training Epoch: 0 Training Iteration: 315  average_loss : nan  step_loss : nan  learning_rate : 0.0009445733362240278 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:41.110572] Finished iteration 315, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2131.508
DLL 2022-11-26 04:34:41.115432 - Training Epoch: 0 Training Iteration: 316  average_loss : nan  step_loss : nan  learning_rate : 0.0009475719817358499 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:43.510459] Finished iteration 316, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2399.904
DLL 2022-11-26 04:34:43.515931 - Training Epoch: 0 Training Iteration: 317  average_loss : nan  step_loss : nan  learning_rate : 0.0009505706272476723 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:45.766062] Finished iteration 317, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2255.555
DLL 2022-11-26 04:34:45.771377 - Training Epoch: 0 Training Iteration: 318  average_loss : nan  step_loss : nan  learning_rate : 0.0009535692727594946 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:47.959234] Finished iteration 318, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2193.112
DLL 2022-11-26 04:34:47.964614 - Training Epoch: 0 Training Iteration: 319  average_loss : nan  step_loss : nan  learning_rate : 0.0009565679182713168 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:50.117951] Finished iteration 319, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2158.684
DLL 2022-11-26 04:34:50.123329 - Training Epoch: 0 Training Iteration: 320  average_loss : nan  step_loss : nan  learning_rate : 0.0009595665637831392 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:52.256253] Finished iteration 320, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2138.277
DLL 2022-11-26 04:34:52.261122 - Training Epoch: 0 Training Iteration: 321  average_loss : nan  step_loss : nan  learning_rate : 0.0009625652092949614 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:54.437640] Finished iteration 321, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2181.358
DLL 2022-11-26 04:34:54.442837 - Training Epoch: 0 Training Iteration: 322  average_loss : nan  step_loss : nan  learning_rate : 0.0009655638548067837 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:56.582997] Finished iteration 322, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2145.335
DLL 2022-11-26 04:34:56.587866 - Training Epoch: 0 Training Iteration: 323  average_loss : nan  step_loss : nan  learning_rate : 0.0009685625003186062 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:34:58.979429] Finished iteration 323, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2396.393
DLL 2022-11-26 04:34:58.984667 - Training Epoch: 0 Training Iteration: 324  average_loss : nan  step_loss : nan  learning_rate : 0.0009715611458304285 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:01.189994] Finished iteration 324, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2210.530
DLL 2022-11-26 04:35:01.195236 - Training Epoch: 0 Training Iteration: 325  average_loss : nan  step_loss : nan  learning_rate : 0.0009745597913422507 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:03.407720] Finished iteration 325, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2217.703
DLL 2022-11-26 04:35:03.412788 - Training Epoch: 0 Training Iteration: 326  average_loss : nan  step_loss : nan  learning_rate : 0.000977558436854073 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:05.543802] Finished iteration 326, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2136.059
DLL 2022-11-26 04:35:05.548836 - Training Epoch: 0 Training Iteration: 327  average_loss : nan  step_loss : nan  learning_rate : 0.0009805570823658953 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:07.778855] Finished iteration 327, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2235.030
DLL 2022-11-26 04:35:07.784318 - Training Epoch: 0 Training Iteration: 328  average_loss : nan  step_loss : nan  learning_rate : 0.0009835557278777176 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:09.933604] Finished iteration 328, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2154.707
DLL 2022-11-26 04:35:09.938938 - Training Epoch: 0 Training Iteration: 329  average_loss : nan  step_loss : nan  learning_rate : 0.00098655437338954 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:12.107510] Finished iteration 329, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2173.873
DLL 2022-11-26 04:35:12.112545 - Training Epoch: 0 Training Iteration: 330  average_loss : nan  step_loss : nan  learning_rate : 0.0009895530189013625 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:14.271515] Finished iteration 330, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2163.980
DLL 2022-11-26 04:35:14.276704 - Training Epoch: 0 Training Iteration: 331  average_loss : nan  step_loss : nan  learning_rate : 0.0009925516644131846 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:16.412176] Finished iteration 331, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2140.635
DLL 2022-11-26 04:35:16.417216 - Training Epoch: 0 Training Iteration: 332  average_loss : nan  step_loss : nan  learning_rate : 0.0009955503099250069 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:18.605889] Finished iteration 332, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2193.690
DLL 2022-11-26 04:35:18.611019 - Training Epoch: 0 Training Iteration: 333  average_loss : nan  step_loss : nan  learning_rate : 0.0009985489554368292 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:20.820857] Finished iteration 333, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2214.942
DLL 2022-11-26 04:35:20.825853 - Training Epoch: 0 Training Iteration: 334  average_loss : nan  step_loss : nan  learning_rate : 0.0010015476009486515 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:23.001810] Finished iteration 334, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2180.930
DLL 2022-11-26 04:35:23.007383 - Training Epoch: 0 Training Iteration: 335  average_loss : nan  step_loss : nan  learning_rate : 0.0010045462464604738 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:25.205502] Finished iteration 335, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2203.659
DLL 2022-11-26 04:35:25.210652 - Training Epoch: 0 Training Iteration: 336  average_loss : nan  step_loss : nan  learning_rate : 0.0010075448919722961 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:35:27.901708] Finished iteration 336, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 2696.177
DLL 2022-11-26 04:35:27.907186 - Training Epoch: 0 Training Iteration: 337  average_loss : nan  step_loss : nan  learning_rate : 0.0010105435374841184 
2022-11-26 04:35:27.907357 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:35:27.907415 - PARAMETER checkpoint_step : 337 
Opt ckpt time 7.04638934135437
Process done with return code 0
Parent process ID: 70709 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 51 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 9 0 970375.4272460938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3283515
Min send: 10000000, max send 0
Min long send: 249051, max long send 268766
Min fwd: 79750, max fwd 86627; min bwd 87708, max bwd 94754
Min long fwd: 58189, max long fwd 63385; min long bwd 92867, max long bwd 97330
Time taken by simulation: 211 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 13 0 485476.6540527344 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3469734
Min send: 10000000, max send 0
Min long send: 248801, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 53890, max bwd 64826
Min long fwd: 37108, max long fwd 45662; min long bwd 64919, max long bwd 72155
Time taken by simulation: 426 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 19 0 352272.7355957031 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4275253
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 22410, max fwd 61283; min bwd 39748, max bwd 51303
Min long fwd: 29385, max long fwd 37648; min long bwd 47928, max long bwd 54735
Time taken by simulation: 919 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 26 0 254962.82958984375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5944030
Min send: 10000000, max send 0
Min long send: 248735, max long send 278723
Min fwd: 10836, max fwd 44441; min bwd 24199, max bwd 35953
Min long fwd: 21303, max long fwd 29429; min long bwd 33298, max long bwd 40795
Time taken by simulation: 2011 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4865 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11451 microseconds

{1: 4.144382, 2: 3.283515, 3: 3.469734, 4: 4.275253, 6: 5.94403, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.283515
15 per stage
30 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 15
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29;
World size is 30
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29; --batch-size=68 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 337
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:36:28.083846 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=68, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=337, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24,26,28;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29;', chunk_size=8, batch_size=68, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.19822430610656738
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
9 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_337.pt
2022-11-26 04:36:38.584953 resume step from  337
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:37:39.589447 - Finished loading checkpoint, takes 60.978 secs
DLL 2022-11-26 04:37:39.590342 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:37:39.590462 - PARAMETER train_start : True 
DLL 2022-11-26 04:37:39.590522 - PARAMETER batch_size_per_gpu : 68 
DLL 2022-11-26 04:37:39.590560 - PARAMETER learning_rate : 0.006 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:38:06.286679] Finished iteration 337, CKPT_AND_STOP: True, flag: tensor([14], dtype=torch.int32), speed: 4410.225
DLL 2022-11-26 04:38:06.292591 - Training Epoch: 0 Training Iteration: 338  average_loss : nan  step_loss : nan  learning_rate : 0.0010135421829959408 
2022-11-26 04:38:06.292837 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:38:06.292865 - PARAMETER checkpoint_step : 338 
Opt ckpt time 6.582860231399536
Process done with return code 0
Parent process ID: 72566 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 4 0 3598379.5166015625 0
End of simulation:  Mini-batch time (usec) = 4923429
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 142174, max long fwd 144458; min long bwd 186536, max long bwd 191040
Time taken by simulation: 46 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 8 0 954047.119140625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3096062
Min send: 10000000, max send 0
Min long send: 249051, max long send 268766
Min fwd: 79507, max fwd 86627; min bwd 88131, max bwd 94131
Min long fwd: 58556, max long fwd 63385; min long bwd 92867, max long bwd 97582
Time taken by simulation: 245 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 13 0 485476.6540527344 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3469734
Min send: 10000000, max send 0
Min long send: 248801, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 53890, max bwd 64826
Min long fwd: 37108, max long fwd 45662; min long bwd 64919, max long bwd 72155
Time taken by simulation: 428 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 364488.09814453125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3900556
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 20622, max fwd 60312; min bwd 40311, max bwd 50179
Min long fwd: 29744, max long fwd 36491; min long bwd 47928, max long bwd 56568
Time taken by simulation: 727 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 26 0 254962.82958984375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5944030
Min send: 10000000, max send 0
Min long send: 248735, max long send 278723
Min fwd: 10836, max fwd 44441; min bwd 24199, max bwd 35953
Min long fwd: 21303, max long fwd 29429; min long bwd 33298, max long bwd 40795
Time taken by simulation: 1943 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 32 0 244280.30395507812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7674225
Min send: 10000000, max send 0
Min long send: 248719, max long send 278723
Min fwd: 6779, max fwd 41103; min bwd 15994, max bwd 28589
Min long fwd: 18906, max long fwd 26091; min long bwd 25267, max long bwd 32480
Time taken by simulation: 3211 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10567 microseconds

{1: 4.923429, 2: 3.096062, 3: 3.469734, 4: 3.900556, 6: 5.94403, 8: 7.674225, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.096062
16 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 16
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31; --batch-size=64 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 338
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-26 04:38:27.115861 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=64, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=338, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;', chunk_size=8, batch_size=64, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.10700535774230957
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
8 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_338.pt
2022-11-26 04:38:37.597646 resume step from  338
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-26 04:39:33.455732 - Finished loading checkpoint, takes 55.831 secs
DLL 2022-11-26 04:39:33.456623 - PARAMETER SEED : 12439 
DLL 2022-11-26 04:39:33.456739 - PARAMETER train_start : True 
DLL 2022-11-26 04:39:33.456787 - PARAMETER batch_size_per_gpu : 64 
DLL 2022-11-26 04:39:33.456820 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 04:40:09.755844] Finished iteration 338, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5336.159
DLL 2022-11-26 04:40:09.761236 - Training Epoch: 0 Training Iteration: 339  average_loss : nan  step_loss : nan  learning_rate : 0.001016540828507763 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 04:40:12.944061] Finished iteration 339, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3187.984
DLL 2022-11-26 04:40:12.949720 - Training Epoch: 0 Training Iteration: 340  average_loss : nan  step_loss : nan  learning_rate : 0.0010195394740195854 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 04:40:16.098813] Finished iteration 340, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3154.772
DLL 2022-11-26 04:40:16.104380 - Training Epoch: 0 Training Iteration: 341  average_loss : nan  step_loss : nan  learning_rate : 0.0010225381195314077 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 04:40:19.252004] Finished iteration 341, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3153.110
DLL 2022-11-26 04:40:19.257304 - Training Epoch: 0 Training Iteration: 342  average_loss : nan  step_loss : nan  learning_rate : 0.0010255367650432298 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 04:40:21.407023] Finished iteration 342, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2154.956
DLL 2022-11-26 04:40:21.412163 - Training Epoch: 0 Training Iteration: 343  average_loss : nan  step_loss : nan  learning_rate : 0.0010285354105550523 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 04:40:23.552069] Finished iteration 343, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2145.028
DLL 2022-11-26 04:40:23.557097 - Training Epoch: 0 Training Iteration: 344  average_loss : nan  step_loss : nan  learning_rate : 0.0010315340560668746 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 04:40:25.705678] Finished iteration 344, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2153.569
DLL 2022-11-26 04:40:25.710762 - Training Epoch: 0 Training Iteration: 345  average_loss : nan  step_loss : nan  learning_rate : 0.001034532701578697 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 04:40:27.925149] Finished iteration 345, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2219.507
DLL 2022-11-26 04:40:27.931641 - Training Epoch: 0 Training Iteration: 346  average_loss : nan  step_loss : nan  learning_rate : 0.0010375313470905193 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 04:40:30.071185] Finished iteration 346, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2145.966
DLL 2022-11-26 04:40:30.076116 - Training Epoch: 0 Training Iteration: 347  average_loss : nan  step_loss : nan  learning_rate : 0.0010405299926023416 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 04:40:32.263554] Finished iteration 347, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2192.319
DLL 2022-11-26 04:40:32.268831 - Training Epoch: 0 Training Iteration: 348  average_loss : nan  step_loss : nan  learning_rate : 0.0010435286381141639 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 04:40:34.445336] Finished iteration 348, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2181.751
DLL 2022-11-26 04:40:34.450328 - Training Epoch: 0 Training Iteration: 349  average_loss : nan  step_loss : nan  learning_rate : 0.0010465272836259862 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 04:40:36.629905] Finished iteration 349, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2184.539
DLL 2022-11-26 04:40:36.634985 - Training Epoch: 0 Training Iteration: 350  average_loss : nan  step_loss : nan  learning_rate : 0.0010495259291378085 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 04:40:38.814439] Finished iteration 350, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2184.515
DLL 2022-11-26 04:40:38.819533 - Training Epoch: 0 Training Iteration: 351  average_loss : nan  step_loss : nan  learning_rate : 0.0010525245746496306 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 04:40:40.955818] Finished iteration 351, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2141.345
DLL 2022-11-26 04:40:40.961309 - Training Epoch: 0 Training Iteration: 352  average_loss : nan  step_loss : nan  learning_rate : 0.0010555232201614531 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 04:40:43.115813] Finished iteration 352, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2159.962
DLL 2022-11-26 04:40:43.121086 - Training Epoch: 0 Training Iteration: 353  average_loss : nan  step_loss : nan  learning_rate : 0.0010585218656732755 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 04:40:45.257689] Finished iteration 353, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2141.846
DLL 2022-11-26 04:40:45.262858 - Training Epoch: 0 Training Iteration: 354  average_loss : nan  step_loss : nan  learning_rate : 0.0010615205111850978 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 04:40:47.388054] Finished iteration 354, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2130.328
DLL 2022-11-26 04:40:47.392989 - Training Epoch: 0 Training Iteration: 355  average_loss : nan  step_loss : nan  learning_rate : 0.00106451915669692 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 04:40:49.528436] Finished iteration 355, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2140.366
DLL 2022-11-26 04:40:49.533450 - Training Epoch: 0 Training Iteration: 356  average_loss : nan  step_loss : nan  learning_rate : 0.0010675178022087424 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 04:40:51.688812] Finished iteration 356, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2160.363
DLL 2022-11-26 04:40:51.693713 - Training Epoch: 0 Training Iteration: 357  average_loss : nan  step_loss : nan  learning_rate : 0.0010705164477205647 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:40:53.870393] Finished iteration 357, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2181.568
DLL 2022-11-26 04:40:53.875326 - Training Epoch: 0 Training Iteration: 358  average_loss : nan  step_loss : nan  learning_rate : 0.001073515093232387 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:40:56.055911] Finished iteration 358, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2185.459
DLL 2022-11-26 04:40:56.060912 - Training Epoch: 0 Training Iteration: 359  average_loss : nan  step_loss : nan  learning_rate : 0.0010765137387442093 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:40:58.224120] Finished iteration 359, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2168.169
DLL 2022-11-26 04:40:58.229078 - Training Epoch: 0 Training Iteration: 360  average_loss : nan  step_loss : nan  learning_rate : 0.0010795123842560316 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:00.418840] Finished iteration 360, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2194.709
DLL 2022-11-26 04:41:00.424045 - Training Epoch: 0 Training Iteration: 361  average_loss : nan  step_loss : nan  learning_rate : 0.0010825110297678537 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:02.581159] Finished iteration 361, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2162.292
DLL 2022-11-26 04:41:02.586245 - Training Epoch: 0 Training Iteration: 362  average_loss : nan  step_loss : nan  learning_rate : 0.0010855096752796763 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:04.778001] Finished iteration 362, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2196.808
DLL 2022-11-26 04:41:04.783033 - Training Epoch: 0 Training Iteration: 363  average_loss : nan  step_loss : nan  learning_rate : 0.0010885083207914986 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:06.929908] Finished iteration 363, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2151.878
DLL 2022-11-26 04:41:06.935030 - Training Epoch: 0 Training Iteration: 364  average_loss : nan  step_loss : nan  learning_rate : 0.0010915069663033207 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:09.108799] Finished iteration 364, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2178.855
DLL 2022-11-26 04:41:09.114288 - Training Epoch: 0 Training Iteration: 365  average_loss : nan  step_loss : nan  learning_rate : 0.0010945056118151432 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:11.265721] Finished iteration 365, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2156.906
DLL 2022-11-26 04:41:11.270901 - Training Epoch: 0 Training Iteration: 366  average_loss : nan  step_loss : nan  learning_rate : 0.0010975042573269653 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:13.422332] Finished iteration 366, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2156.576
DLL 2022-11-26 04:41:13.427602 - Training Epoch: 0 Training Iteration: 367  average_loss : nan  step_loss : nan  learning_rate : 0.0011005029028387878 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:15.916168] Finished iteration 367, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2493.804
DLL 2022-11-26 04:41:15.921040 - Training Epoch: 0 Training Iteration: 368  average_loss : nan  step_loss : nan  learning_rate : 0.0011035015483506101 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:18.126488] Finished iteration 368, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2210.293
DLL 2022-11-26 04:41:18.131511 - Training Epoch: 0 Training Iteration: 369  average_loss : nan  step_loss : nan  learning_rate : 0.0011065001938624325 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:20.381660] Finished iteration 369, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2255.152
DLL 2022-11-26 04:41:20.387086 - Training Epoch: 0 Training Iteration: 370  average_loss : nan  step_loss : nan  learning_rate : 0.0011094988393742546 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:22.519097] Finished iteration 370, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2137.408
DLL 2022-11-26 04:41:22.524598 - Training Epoch: 0 Training Iteration: 371  average_loss : nan  step_loss : nan  learning_rate : 0.001112497484886077 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:24.726537] Finished iteration 371, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2207.414
DLL 2022-11-26 04:41:24.731619 - Training Epoch: 0 Training Iteration: 372  average_loss : nan  step_loss : nan  learning_rate : 0.0011154961303978994 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:26.891871] Finished iteration 372, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2165.318
DLL 2022-11-26 04:41:26.896824 - Training Epoch: 0 Training Iteration: 373  average_loss : nan  step_loss : nan  learning_rate : 0.0011184947759097215 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:29.090555] Finished iteration 373, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2198.665
DLL 2022-11-26 04:41:29.095553 - Training Epoch: 0 Training Iteration: 374  average_loss : nan  step_loss : nan  learning_rate : 0.001121493421421544 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:31.333216] Finished iteration 374, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2242.622
DLL 2022-11-26 04:41:31.338204 - Training Epoch: 0 Training Iteration: 375  average_loss : nan  step_loss : nan  learning_rate : 0.0011244920669333661 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:33.470180] Finished iteration 375, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2136.939
DLL 2022-11-26 04:41:33.475124 - Training Epoch: 0 Training Iteration: 376  average_loss : nan  step_loss : nan  learning_rate : 0.0011274907124451884 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:35.668498] Finished iteration 376, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2198.258
DLL 2022-11-26 04:41:35.673588 - Training Epoch: 0 Training Iteration: 377  average_loss : nan  step_loss : nan  learning_rate : 0.0011304893579570108 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:37.821551] Finished iteration 377, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2153.027
DLL 2022-11-26 04:41:37.826637 - Training Epoch: 0 Training Iteration: 378  average_loss : nan  step_loss : nan  learning_rate : 0.0011334880034688333 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:40.013641] Finished iteration 378, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2192.086
DLL 2022-11-26 04:41:40.018921 - Training Epoch: 0 Training Iteration: 379  average_loss : nan  step_loss : nan  learning_rate : 0.0011364866489806554 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:42.397242] Finished iteration 379, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2383.535
DLL 2022-11-26 04:41:42.402336 - Training Epoch: 0 Training Iteration: 380  average_loss : nan  step_loss : nan  learning_rate : 0.0011394852944924777 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:44.552834] Finished iteration 380, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2155.577
DLL 2022-11-26 04:41:44.558855 - Training Epoch: 0 Training Iteration: 381  average_loss : nan  step_loss : nan  learning_rate : 0.0011424839400043 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:46.747017] Finished iteration 381, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2194.164
DLL 2022-11-26 04:41:46.752037 - Training Epoch: 0 Training Iteration: 382  average_loss : nan  step_loss : nan  learning_rate : 0.0011454825855161225 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:48.887617] Finished iteration 382, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2140.556
DLL 2022-11-26 04:41:48.892576 - Training Epoch: 0 Training Iteration: 383  average_loss : nan  step_loss : nan  learning_rate : 0.0011484812310279446 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:51.025183] Finished iteration 383, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2137.536
DLL 2022-11-26 04:41:51.030529 - Training Epoch: 0 Training Iteration: 384  average_loss : nan  step_loss : nan  learning_rate : 0.0011514798765397672 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:53.224888] Finished iteration 384, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2199.662
DLL 2022-11-26 04:41:53.230341 - Training Epoch: 0 Training Iteration: 385  average_loss : nan  step_loss : nan  learning_rate : 0.0011544785220515893 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:55.421844] Finished iteration 385, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2196.939
DLL 2022-11-26 04:41:55.427116 - Training Epoch: 0 Training Iteration: 386  average_loss : nan  step_loss : nan  learning_rate : 0.0011574771675634116 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:57.613804] Finished iteration 386, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2191.933
DLL 2022-11-26 04:41:57.618898 - Training Epoch: 0 Training Iteration: 387  average_loss : nan  step_loss : nan  learning_rate : 0.001160475813075234 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:41:59.813491] Finished iteration 387, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2199.659
DLL 2022-11-26 04:41:59.818955 - Training Epoch: 0 Training Iteration: 388  average_loss : nan  step_loss : nan  learning_rate : 0.0011634744585870562 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:01.970465] Finished iteration 388, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2156.942
DLL 2022-11-26 04:42:01.976069 - Training Epoch: 0 Training Iteration: 389  average_loss : nan  step_loss : nan  learning_rate : 0.0011664731040988785 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:04.330402] Finished iteration 389, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2359.898
DLL 2022-11-26 04:42:04.335400 - Training Epoch: 0 Training Iteration: 390  average_loss : nan  step_loss : nan  learning_rate : 0.0011694717496107008 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:06.811053] Finished iteration 390, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2480.635
DLL 2022-11-26 04:42:06.816272 - Training Epoch: 0 Training Iteration: 391  average_loss : nan  step_loss : nan  learning_rate : 0.0011724703951225231 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:08.971701] Finished iteration 391, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2160.612
DLL 2022-11-26 04:42:08.976802 - Training Epoch: 0 Training Iteration: 392  average_loss : nan  step_loss : nan  learning_rate : 0.0011754690406343454 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:11.124607] Finished iteration 392, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2152.883
DLL 2022-11-26 04:42:11.130156 - Training Epoch: 0 Training Iteration: 393  average_loss : nan  step_loss : nan  learning_rate : 0.001178467686146168 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:13.293826] Finished iteration 393, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2169.191
DLL 2022-11-26 04:42:13.298897 - Training Epoch: 0 Training Iteration: 394  average_loss : nan  step_loss : nan  learning_rate : 0.00118146633165799 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:15.460157] Finished iteration 394, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2166.295
DLL 2022-11-26 04:42:15.465274 - Training Epoch: 0 Training Iteration: 395  average_loss : nan  step_loss : nan  learning_rate : 0.0011844649771698124 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:17.667049] Finished iteration 395, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2206.870
DLL 2022-11-26 04:42:17.672094 - Training Epoch: 0 Training Iteration: 396  average_loss : nan  step_loss : nan  learning_rate : 0.0011874636226816347 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:19.892022] Finished iteration 396, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2224.946
DLL 2022-11-26 04:42:19.897006 - Training Epoch: 0 Training Iteration: 397  average_loss : nan  step_loss : nan  learning_rate : 0.0011904622681934572 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:22.078559] Finished iteration 397, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2186.505
DLL 2022-11-26 04:42:22.083937 - Training Epoch: 0 Training Iteration: 398  average_loss : nan  step_loss : nan  learning_rate : 0.0011934609137052793 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:24.261959] Finished iteration 398, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2183.374
DLL 2022-11-26 04:42:24.267383 - Training Epoch: 0 Training Iteration: 399  average_loss : nan  step_loss : nan  learning_rate : 0.0011964595592171016 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:26.392202] Finished iteration 399, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2130.223
DLL 2022-11-26 04:42:26.397434 - Training Epoch: 0 Training Iteration: 400  average_loss : nan  step_loss : nan  learning_rate : 0.001199458204728924 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:28.599897] Finished iteration 400, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2207.684
DLL 2022-11-26 04:42:28.604843 - Training Epoch: 0 Training Iteration: 401  average_loss : nan  step_loss : nan  learning_rate : 0.0012024568502407463 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:30.795195] Finished iteration 401, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2195.249
DLL 2022-11-26 04:42:30.800338 - Training Epoch: 0 Training Iteration: 402  average_loss : nan  step_loss : nan  learning_rate : 0.0012054554957525686 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:33.001490] Finished iteration 402, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2206.256
DLL 2022-11-26 04:42:33.006561 - Training Epoch: 0 Training Iteration: 403  average_loss : nan  step_loss : nan  learning_rate : 0.001208454141264391 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:35.167496] Finished iteration 403, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2165.977
DLL 2022-11-26 04:42:35.172545 - Training Epoch: 0 Training Iteration: 404  average_loss : nan  step_loss : nan  learning_rate : 0.0012114527867762132 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:37.325776] Finished iteration 404, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2158.313
DLL 2022-11-26 04:42:37.333362 - Training Epoch: 0 Training Iteration: 405  average_loss : nan  step_loss : nan  learning_rate : 0.0012144514322880353 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:39.492499] Finished iteration 405, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2166.652
DLL 2022-11-26 04:42:39.497529 - Training Epoch: 0 Training Iteration: 406  average_loss : nan  step_loss : nan  learning_rate : 0.0012174500777998578 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:41.666419] Finished iteration 406, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2173.873
DLL 2022-11-26 04:42:41.671468 - Training Epoch: 0 Training Iteration: 407  average_loss : nan  step_loss : nan  learning_rate : 0.0012204487233116801 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:43.884553] Finished iteration 407, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2218.100
DLL 2022-11-26 04:42:43.890128 - Training Epoch: 0 Training Iteration: 408  average_loss : nan  step_loss : nan  learning_rate : 0.0012234473688235025 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:46.064251] Finished iteration 408, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2179.684
DLL 2022-11-26 04:42:46.069142 - Training Epoch: 0 Training Iteration: 409  average_loss : nan  step_loss : nan  learning_rate : 0.0012264460143353248 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:48.273157] Finished iteration 409, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2208.853
DLL 2022-11-26 04:42:48.278540 - Training Epoch: 0 Training Iteration: 410  average_loss : nan  step_loss : nan  learning_rate : 0.001229444659847147 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:50.444256] Finished iteration 410, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2171.094
DLL 2022-11-26 04:42:50.449332 - Training Epoch: 0 Training Iteration: 411  average_loss : nan  step_loss : nan  learning_rate : 0.0012324433053589694 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:52.661995] Finished iteration 411, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2217.722
DLL 2022-11-26 04:42:52.667101 - Training Epoch: 0 Training Iteration: 412  average_loss : nan  step_loss : nan  learning_rate : 0.001235441950870792 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:54.865771] Finished iteration 412, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2203.726
DLL 2022-11-26 04:42:54.871019 - Training Epoch: 0 Training Iteration: 413  average_loss : nan  step_loss : nan  learning_rate : 0.001238440596382614 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:57.051525] Finished iteration 413, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2185.733
DLL 2022-11-26 04:42:57.057054 - Training Epoch: 0 Training Iteration: 414  average_loss : nan  step_loss : nan  learning_rate : 0.0012414392418944363 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:42:59.284926] Finished iteration 414, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2233.354
DLL 2022-11-26 04:42:59.289887 - Training Epoch: 0 Training Iteration: 415  average_loss : nan  step_loss : nan  learning_rate : 0.0012444378874062586 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:01.435797] Finished iteration 415, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2150.853
DLL 2022-11-26 04:43:01.440870 - Training Epoch: 0 Training Iteration: 416  average_loss : nan  step_loss : nan  learning_rate : 0.001247436532918081 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:03.694976] Finished iteration 416, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2259.159
DLL 2022-11-26 04:43:03.699817 - Training Epoch: 0 Training Iteration: 417  average_loss : nan  step_loss : nan  learning_rate : 0.0012504351784299033 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:05.892687] Finished iteration 417, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2197.669
DLL 2022-11-26 04:43:05.898057 - Training Epoch: 0 Training Iteration: 418  average_loss : nan  step_loss : nan  learning_rate : 0.0012534338239417256 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:08.074637] Finished iteration 418, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2181.924
DLL 2022-11-26 04:43:08.079642 - Training Epoch: 0 Training Iteration: 419  average_loss : nan  step_loss : nan  learning_rate : 0.001256432469453548 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:10.230120] Finished iteration 419, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2155.487
DLL 2022-11-26 04:43:10.235231 - Training Epoch: 0 Training Iteration: 420  average_loss : nan  step_loss : nan  learning_rate : 0.00125943111496537 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:12.435159] Finished iteration 420, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2204.978
DLL 2022-11-26 04:43:12.440127 - Training Epoch: 0 Training Iteration: 421  average_loss : nan  step_loss : nan  learning_rate : 0.0012624297604771925 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:14.570566] Finished iteration 421, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2135.369
DLL 2022-11-26 04:43:14.575756 - Training Epoch: 0 Training Iteration: 422  average_loss : nan  step_loss : nan  learning_rate : 0.0012654284059890148 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:16.792481] Finished iteration 422, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2221.890
DLL 2022-11-26 04:43:16.797798 - Training Epoch: 0 Training Iteration: 423  average_loss : nan  step_loss : nan  learning_rate : 0.0012684270515008372 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:19.000603] Finished iteration 423, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2208.101
DLL 2022-11-26 04:43:19.005717 - Training Epoch: 0 Training Iteration: 424  average_loss : nan  step_loss : nan  learning_rate : 0.0012714256970126593 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:21.190211] Finished iteration 424, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2189.607
DLL 2022-11-26 04:43:21.195412 - Training Epoch: 0 Training Iteration: 425  average_loss : nan  step_loss : nan  learning_rate : 0.0012744243425244818 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:23.373788] Finished iteration 425, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2183.516
DLL 2022-11-26 04:43:23.378802 - Training Epoch: 0 Training Iteration: 426  average_loss : nan  step_loss : nan  learning_rate : 0.001277422988036304 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:25.545965] Finished iteration 426, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2172.147
DLL 2022-11-26 04:43:25.551238 - Training Epoch: 0 Training Iteration: 427  average_loss : nan  step_loss : nan  learning_rate : 0.0012804216335481264 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 04:43:27.733180] Finished iteration 427, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 2187.195
DLL 2022-11-26 04:43:27.738591 - Training Epoch: 0 Training Iteration: 428  average_loss : nan  step_loss : nan  learning_rate : 0.0012834202790599487 
2022-11-26 04:43:27.738773 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-26 04:43:27.738802 - PARAMETER checkpoint_step : 428 
Opt ckpt time 7.639758348464966
Process done with return code 0
Parent process ID: 76072 node: 172.31.28.108
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 4144382
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 141043, max long fwd 144458; min long bwd 184920, max long bwd 191040
Time taken by simulation: 46 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 10 0 892117.1264648438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3456563
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 199 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 15 0 476084.3811035156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3889823
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34426, max fwd 67951; min bwd 54355, max bwd 65681
Min long fwd: 38932, max long fwd 44034; min long bwd 64405, max long bwd 71935
Time taken by simulation: 491 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 19 0 352272.7355957031 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4275253
Min send: 10000000, max send 0
Min long send: 248773, max long send 273424
Min fwd: 22410, max fwd 61283; min bwd 39748, max bwd 51303
Min long fwd: 29385, max long fwd 37648; min long bwd 47928, max long bwd 54735
Time taken by simulation: 886 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2401 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4636 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11031 microseconds

{1: 4.144382, 2: 3.456563, 3: 3.889823, 4: 4.275253, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.456563
14 per stage
28 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 14
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26;1,3,5,7,9,11,13,15,17,19,21,23,25,27;
World size is 28
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26;1,3,5,7,9,11,13,15,17,19,21,23,25,27; --batch-size=73 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 428
Signal handler called with signal 10


 STOPPING VARUNA !!



