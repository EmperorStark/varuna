Parent process ID: 9724 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 282 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 698 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1446 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3239 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6720 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21537 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.06229734420776367
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
15 chunks
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:05:33.183009] Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4501.880
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:05:37.226288] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4043.054
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:05:41.202529] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3976.209
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:05:44.208209] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3005.574
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:05:47.222036] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3013.798
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:05:50.258248] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.188
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:05:53.232916] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2974.637
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:05:56.305305] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3072.363
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:05:59.386327] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3080.985
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:06:02.386813] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.461
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:06:05.398044] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3011.217
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:06:08.387806] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.717
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:06:11.377941] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2990.109
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:06:14.384146] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.188
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:06:17.379706] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.551
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:06:20.361390] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.648
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:06:23.343083] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.641
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:06:26.365881] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3022.769
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:06:29.346311] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2980.404
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:32.342517] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2996.171
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:35.309670] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.127
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:38.365218] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.517
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:41.387188] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.943
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:44.411623] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3024.402
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:47.425943] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.294
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:50.481481] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.509
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:53.517891] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.375
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:56.546746] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3028.828
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:59.602182] Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.404
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:02.610217] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.015
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:05.627593] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3017.339
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:08.680833] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.231
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:11.717226] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.341
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:14.757409] Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3040.158
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:17.751689] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.252
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:20.751209] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2999.496
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:23.749639] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2998.398
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:26.731370] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.702
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:29.732370] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.973
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:32.766123] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.725
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:35.854205] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3088.082
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:38.908523] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3054.258
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:41.916336] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.781
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:44.951115] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.758
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:47.956273] Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3005.127
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:50.926952] Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2970.655
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:53.922935] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.952
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:56.966828] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3043.863
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:00.069437] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3102.582
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:03.098483] Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.018
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:06.133966] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.451
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:09.105881] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2971.888
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:12.134935] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.028
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:15.179823] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3044.876
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:18.231863] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3051.986
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:21.266707] Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.818
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:24.357545] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3090.811
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:27.399008] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3041.433
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:30.462441] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.409
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:33.515175] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.700
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:36.584620] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3069.450
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:39.648375] Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.692
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:42.745534] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3097.135
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:45.814167] Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3068.598
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:48.867552] Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.357
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:51.968290] Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3100.709
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:55.021104] Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.794
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:58.029352] Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.215
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:01.081885] Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.503
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:04.103357] Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.433
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:07.140298] Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.917
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:10.188169] Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3047.837
2022-11-26 03:09:10.192625 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.574990749359131
Process done with return code 0
Parent process ID: 10287 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 248 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1173 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3154 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6660 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21634 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 72
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.0629117488861084
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_72.pt
2022-11-26 03:09:45.214942 resume step from  72
2022-11-26 03:10:29.293907 - Finished loading checkpoint, takes 44.052 secs
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:10:58.922565] Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9616.125
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:11:02.739049] Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3816.199
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:11:05.544407] Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2805.262
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:11:08.372225] Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2827.785
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:11:11.249072] Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2876.859
2022-11-26 03:11:11.253513 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 4.759885549545288
Process done with return code 0
Parent process ID: 11464 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2350970.703125 0
End of simulation:  Mini-batch time (usec) = 4325616
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 292 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 601 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1177 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3219 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6698 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21577 microseconds

{1: 4.325616, 2: 4.315108, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.0450587272644043
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:11:41.947105 resume step from  77
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 16 signal handler called with signal 10
2022-11-26 03:12:27.406610 - Finished loading checkpoint, takes 45.433 secs
2022-11-26 03:12:47.097890 Begin to exit
Process done with return code 0
Parent process ID: 12609 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2300677.490234375 0
End of simulation:  Mini-batch time (usec) = 4275323
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 88 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 227 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 599 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1175 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3201 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6723 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21554 microseconds

{1: 4.275323, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.2555851936340332
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:13:13.964040 resume step from  77
2022-11-26 03:13:58.396358 - Finished loading checkpoint, takes 44.404 secs
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:14:24.019046] Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9622.672
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:14:26.634198] Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2614.847
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:14:30.230541] Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.377
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:14:32.813695] Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2583.068
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:14:35.391445] Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2577.729
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:14:38.003990] Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2612.522
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:14:40.578698] Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2574.653
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:14:43.143056] Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.325
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:14:45.719741] Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.654
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:14:48.321315] Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2601.547
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:14:50.888314] Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2566.968
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:14:53.434571] Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2546.225
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:14:55.998821] Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.218
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:14:58.572742] Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2573.935
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:15:01.129230] Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2556.431
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:15:03.716341] Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2587.082
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:15:06.294983] Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2578.609
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:15:08.845198] Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2550.172
2022-11-26 03:15:08.849726 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 4.69573712348938
Process done with return code 0
Parent process ID: 13860 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2325752.685546875 0
End of simulation:  Mini-batch time (usec) = 4300398
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 51 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 226 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 605 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1238 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3193 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6732 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21541 microseconds

{1: 4.300398, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.20361590385437012
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:15:42.422670 resume step from  95
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 16 signal handler called with signal 10
2022-11-26 03:16:30.134356 - Finished loading checkpoint, takes 47.684 secs
2022-11-26 03:16:44.909505 Begin to exit
Process done with return code 0
Parent process ID: 15053 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2258812.98828125 0
End of simulation:  Mini-batch time (usec) = 4233458
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 45 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 210 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 520 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 993 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2337 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4437 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10493 microseconds

{1: 4.233458, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.2325420379638672
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:17:10.839645 resume step from  95
2022-11-26 03:17:57.293145 - Finished loading checkpoint, takes 46.426 secs
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:18:18.060036] Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 8905.923
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:18:21.544442] Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.092
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:18:25.003711] Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3459.227
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:18:28.452108] Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3448.359
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:18:31.916275] Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3464.132
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:18:34.423096] Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.724
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:18:36.886144] Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2463.168
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:18:39.358992] Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2472.672
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:18:41.877495] Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2518.476
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:18:44.362804] Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2485.279
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:18:46.832007] Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2469.172
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:18:49.362837] Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2530.804
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:18:51.851579] Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2488.709
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:18:54.322075] Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2470.471
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:18:56.819875] Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2497.773
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:18:59.258886] Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2438.980
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:19:01.764939] Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.029
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:19:04.222240] Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2457.258
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:19:06.661799] Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2439.532
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:09.138057] Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2476.222
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:11.600109] Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2462.030
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:14.095058] Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2494.914
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:16.589383] Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2494.298
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:19.019024] Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2429.609
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:21.464587] Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2445.577
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:23.931462] Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2466.842
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:26.390683] Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2459.164
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:28.910372] Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2519.652
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:31.345959] Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2435.562
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:33.783602] Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2437.611
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:36.275010] Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2491.378
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:38.694899] Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2419.878
2022-11-26 03:19:38.699339 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.6875269412994385
Process done with return code 0
Parent process ID: 7221 node: 172.31.23.137
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 65 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 252 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 700 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1172 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3197 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6703 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21521 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.004475593566894531
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:20:44.038201 resume step from  127
2022-11-26 03:21:36.229924 - Finished loading checkpoint, takes 52.162 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 16 signal handler called with signal 10
2022-11-26 03:21:53.769767 Begin to exit
Process done with return code 0
Parent process ID: 10603 node: 172.31.16.5
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 308 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1456 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3219 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6705 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21523 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 167
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.17385268211364746
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_167.pt
2022-11-26 03:41:43.014929 resume step from  167
2022-11-26 03:42:21.444863 - Finished loading checkpoint, takes 38.402 secs
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:42:48.527737] Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6078.884
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:42:52.538554] Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4010.516
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:42:55.529565] Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2990.923
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:42:58.524001] Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.442
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:43:02.502281] Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3978.284
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:43:05.443520] Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2941.141
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:43:08.412608] Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.062
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:43:11.389469] Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.834
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:43:14.334747] Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2945.246
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:43:17.291995] Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2957.220
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:43:20.254084] Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2962.069
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:43:23.207809] Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2953.693
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:43:26.223546] Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.717
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:43:29.199980] Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.414
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:43:32.220194] Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3020.176
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:43:35.216059] Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.842
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:43:38.167462] Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2951.374
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:43:41.176994] Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3009.502
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:43:44.119092] Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2942.074
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:47.082757] Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2963.638
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:50.021685] Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2938.898
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:52.989569] Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.939
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:55.958494] Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2968.868
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:58.885862] Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2927.285
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:01.889055] Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3003.167
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:04.855304] Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.245
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:07.889174] Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.828
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:10.865350] Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2976.159
2022-11-26 03:44:10.869864 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 6.3587727546691895
Process done with return code 0
Parent process ID: 11718 node: 172.31.16.5
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2490411.62109375 0
End of simulation:  Mini-batch time (usec) = 4792107
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 57 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 312 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 700 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1444 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3232 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6646 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21547 microseconds

{1: 4.792107, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(16, 17)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=16 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 195
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.176208257675171
SHARED WEIGHTS ARE
[(0, 1)]
this rank  16 is part of pipeline replica  8
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_195.pt
2022-11-26 03:44:41.363776 resume step from  195
2022-11-26 03:45:25.096298 - Finished loading checkpoint, takes 43.704 secs
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:45:54.563335] Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6095.139
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:45:57.509719] Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2946.059
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:46:00.543807] Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.057
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:46:03.551691] Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.847
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:46:06.540392] Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.656
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:46:09.507398] Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.977
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:46:12.495599] Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.168
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:46:15.502234] Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.617
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:46:18.491788] Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.524
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:46:22.309917] Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3818.077
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:46:25.325403] Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.467
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:46:29.080822] Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3755.424
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:46:32.095458] Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.554
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:46:35.088046] Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2992.562
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:46:38.072449] Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2984.444
16 Overflow !!
16 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:46:41.063130] Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2990.626
2022-11-26 03:46:41.067688 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.773577451705933
Process done with return code 0
Parent process ID: 18981 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2595254.8828125 0
End of simulation:  Mini-batch time (usec) = 5226764
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 58 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 295 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 815 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1475 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4809 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6710 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22735 microseconds

{1: 5.226764, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
16 16 I am of no use!
