Parent process ID: 9737 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 55 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 276 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 694 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1483 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3229 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6867 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21615 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.06181740760803223
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
15 chunks
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:05:33.185495] Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4501.715
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:05:37.229974] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4044.348
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:05:41.205191] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3975.138
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:05:44.210787] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3005.487
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:05:47.224623] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3013.815
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:05:50.260644] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.997
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:05:53.235281] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2974.589
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:05:56.307930] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3072.623
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:05:59.388850] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3080.894
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:06:02.389291] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.430
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:06:05.400438] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3011.115
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:06:08.390412] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.944
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:06:11.380454] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.996
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:06:14.386656] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.172
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:06:17.382289] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.617
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:06:20.363867] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.522
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:06:23.345608] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.717
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:06:26.368420] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3022.802
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:06:29.348566] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2980.090
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:32.345061] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2996.462
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:35.312060] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.970
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:38.367706] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.615
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:41.389589] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.852
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:44.414151] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3024.527
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:47.428376] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.194
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:50.483961] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.561
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:53.520607] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.606
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:56.549189] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3028.551
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:59.604806] Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.583
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:02.612691] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.862
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:05.629828] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3017.116
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:08.683137] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.278
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:11.719668] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.496
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:14.760007] Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3040.307
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:17.754249] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.231
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:20.753711] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2999.422
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:23.752219] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2998.482
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:26.733785] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.528
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:29.734670] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.856
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:32.768546] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.833
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:35.856625] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3088.047
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:38.910846] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3054.192
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:41.918647] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.765
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:44.953913] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.269
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:47.958871] Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3004.907
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:50.929337] Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2970.419
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:53.925369] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2996.016
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:56.969293] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3043.887
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:00.071737] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3102.412
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:03.100811] Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.050
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:06.136407] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.566
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:09.108413] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2971.977
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:12.137438] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.007
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:15.182254] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3044.779
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:18.234292] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.000
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:21.269044] Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.717
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:24.359909] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3090.840
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:27.401572] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3041.656
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:30.464871] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.247
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:33.517771] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.886
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:36.587053] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3069.240
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:39.650678] Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.592
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:42.747954] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3097.242
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:45.816690] Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3068.714
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:48.870137] Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.417
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:51.970646] Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3100.465
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:55.023579] Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.906
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:58.031677] Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.074
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:01.084253] Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.536
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:04.105706] Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.422
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:07.142790] Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3037.059
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:10.190764] Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3047.940
2022-11-26 03:09:10.197844 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
Opt ckpt time 6.371293067932129
Process done with return code 0
Parent process ID: 10299 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 56 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 253 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1204 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3237 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6714 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21583 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 72
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.03228473663330078
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_72.pt
2022-11-26 03:09:45.204232 resume step from  72
2022-11-26 03:10:39.882657 - Finished loading checkpoint, takes 54.650 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:10:58.925596] Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9616.641
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:11:02.741523] Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3815.753
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:11:05.546848] Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2805.193
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:11:08.374728] Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2827.879
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:11:11.251439] Finished iteration 76, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 2876.675
2022-11-26 03:11:11.255865 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 4.926398754119873
Process done with return code 0
Parent process ID: 11477 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2350970.703125 0
End of simulation:  Mini-batch time (usec) = 4325616
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 49 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 253 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 604 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1201 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3194 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6701 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21560 microseconds

{1: 4.325616, 2: 4.315108, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.007830619812011719
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:11:42.004506 resume step from  77
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:12:27.645026 - Finished loading checkpoint, takes 45.613 secs
2022-11-26 03:12:47.100738 Begin to exit
Process done with return code 0
Parent process ID: 12622 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2300677.490234375 0
End of simulation:  Mini-batch time (usec) = 4275323
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 56 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 228 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 603 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1205 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3186 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6759 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21596 microseconds

{1: 4.275323, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.23009514808654785
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:13:13.953517 resume step from  77
2022-11-26 03:14:04.346366 - Finished loading checkpoint, takes 50.365 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:14:24.021528] Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9621.137
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:14:26.636691] Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2615.060
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:14:30.233131] Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.450
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:14:32.816048] Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2582.823
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:14:35.394038] Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2577.989
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:14:38.006318] Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2612.247
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:14:40.581056] Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2574.672
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:14:43.145405] Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.320
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:14:45.722313] Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.875
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:14:48.323635] Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2601.295
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:14:50.890638] Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2566.966
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:14:53.437056] Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2546.383
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:14:56.001423] Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.335
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:14:58.575108] Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2573.647
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:15:01.131526] Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2556.415
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:15:03.718766] Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2587.210
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:15:06.297524] Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2578.692
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:15:08.847718] Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2550.165
2022-11-26 03:15:08.852167 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
Opt ckpt time 4.244382381439209
Process done with return code 0
Parent process ID: 13874 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2325752.685546875 0
End of simulation:  Mini-batch time (usec) = 4300398
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 57 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 223 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 599 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1172 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3151 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6725 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21479 microseconds

{1: 4.300398, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.12478423118591309
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:15:42.434724 resume step from  95
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:16:44.904925 - Finished loading checkpoint, takes 62.443 secs
2022-11-26 03:16:44.912877 Begin to exit
Process done with return code 0
Parent process ID: 15066 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2258812.98828125 0
End of simulation:  Mini-batch time (usec) = 4233458
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 84 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 210 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 518 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 995 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2405 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4453 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 11018 microseconds

{1: 4.233458, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.19047880172729492
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:17:10.870171 resume step from  95
2022-11-26 03:17:58.647222 - Finished loading checkpoint, takes 47.748 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:18:18.062426] Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 8905.430
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:18:21.546713] Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.033
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:18:25.007034] Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3460.343
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:18:28.454482] Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3447.345
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:18:31.918543] Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3464.091
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:18:34.425618] Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.963
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:18:36.888368] Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2462.733
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:18:39.361202] Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2472.772
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:18:41.879977] Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2518.784
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:18:44.365109] Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2485.067
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:18:46.834452] Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2469.311
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:18:49.365082] Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2530.650
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:18:51.853942] Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2488.833
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:18:54.324312] Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2470.307
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:18:56.822089] Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2497.777
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:18:59.261077] Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2438.907
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:19:01.767321] Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.219
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:19:04.224362] Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2457.008
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:19:06.663897] Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2439.503
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:09.140310] Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2476.420
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:11.602239] Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2461.899
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:14.097340] Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2495.068
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:16.591600] Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2494.219
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:19.021379] Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2429.722
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:21.466946] Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2445.539
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:23.933729] Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2466.786
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:26.392833] Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2459.057
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:28.912776] Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2519.924
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:31.348084] Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2435.281
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:33.785784] Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2437.639
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:36.277470] Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2491.680
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:38.697109] Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2419.624
2022-11-26 03:19:38.701742 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
Opt ckpt time 5.886826515197754
Process done with return code 0
Parent process ID: 16404 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 50 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 247 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 696 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1176 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3268 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6678 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21722 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.4001905918121338
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:20:44.010981 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:21:45.584874 - Finished loading checkpoint, takes 61.546 secs
2022-11-26 03:21:53.774762 Begin to exit
Process done with return code 0
Parent process ID: 17664 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 5124879
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 60 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 297 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 819 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1500 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4800 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6712 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21542 microseconds

{1: 5.124879, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.0037059783935546875
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:22:44.065911 resume step from  127
2022-11-26 03:23:33.684874 - Finished loading checkpoint, takes 49.591 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:23:58.045210 Begin to exit
Process done with return code 0
Parent process ID: 18913 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 2234690.185546875 0
End of simulation:  Mini-batch time (usec) = 5528603
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 66 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 337 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1003 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1977 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4806 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13798 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21628 microseconds

{1: 5.528603, 2: 6.142701, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.142701
7 per stage
14 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.18485093116760254
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
19 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:24:48.495640 resume step from  127
2022-11-26 03:25:38.392838 - Finished loading checkpoint, takes 49.868 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:25:52.808969 Begin to exit
Process done with return code 0
Parent process ID: 20163 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 66 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 387 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 996 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1999 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4801 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13656 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21523 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.0038881301879882812
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:26:43.355718 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:27:47.668980 - Finished loading checkpoint, takes 64.285 secs
2022-11-26 03:27:47.675365 Begin to exit
Process done with return code 0
Parent process ID: 21443 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 12 0 1301385.1318359375 0
End of simulation:  Mini-batch time (usec) = 5252680
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145763; min long bwd 182761, max long bwd 191040
Time taken by simulation: 73 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 26 0 555338.134765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7296961
Min send: 10000000, max send 0
Min long send: 249051, max long send 272408
Min fwd: 78412, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 56472, max long fwd 64412; min long bwd 91514, max long bwd 98662
Time taken by simulation: 455 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 43 0 338854.248046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8832625
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 50125, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 63246, max long bwd 71935
Time taken by simulation: 1318 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 64 0 207089.01977539062 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10812652
Min send: 10000000, max send 0
Min long send: 248838, max long send 278723
Min fwd: 20508, max fwd 60312; min bwd 38425, max bwd 50813
Min long fwd: 28341, max long fwd 38392; min long bwd 47725, max long bwd 56957
Time taken by simulation: 2864 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 18586236
Min send: 10000000, max send 0
Min long send: 248758, max long send 288457
Min fwd: 9602, max fwd 45672; min bwd 22890, max bwd 38402
Min long fwd: 18236, max long fwd 30331; min long bwd 30481, max long bwd 41890
Time taken by simulation: 9639 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13687 microseconds

can't have 12 stages!
{1: 5.25268, 2: 7.296961, 3: 8.832625, 4: 10.812652, 6: 18.586236, 8: 19.658022}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8}
best config is: 2 8
expected time is 7.296961
5 per stage
10 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 5
stage to rank map: 0,2,4,6,8;1,3,5,7,9;
World size is 10
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8;1,3,5,7,9; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.1967003345489502
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:28:38.006431 resume step from  127
Parent process ID: 22227 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 12 0 1301385.1318359375 0
End of simulation:  Mini-batch time (usec) = 5252680
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145763; min long bwd 182761, max long bwd 191040
Time taken by simulation: 113 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 26 0 555338.134765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7296961
Min send: 10000000, max send 0
Min long send: 249051, max long send 272408
Min fwd: 78412, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 56472, max long fwd 64412; min long bwd 91514, max long bwd 98662
Time taken by simulation: 450 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 43 0 338854.248046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8832625
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 50125, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 63246, max long bwd 71935
Time taken by simulation: 1317 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 64 0 207089.01977539062 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10812652
Min send: 10000000, max send 0
Min long send: 248838, max long send 278723
Min fwd: 20508, max fwd 60312; min bwd 38425, max bwd 50813
Min long fwd: 28341, max long fwd 38392; min long bwd 47725, max long bwd 56957
Time taken by simulation: 2923 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 18586236
Min send: 10000000, max send 0
Min long send: 248758, max long send 288457
Min fwd: 9602, max fwd 45672; min bwd 22890, max bwd 38402
Min long fwd: 18236, max long fwd 30331; min long bwd 30481, max long bwd 41890
Time taken by simulation: 9680 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13698 microseconds

can't have 12 stages!
{1: 5.25268, 2: 7.296961, 3: 8.832625, 4: 10.812652, 6: 18.586236, 8: 19.658022}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8}
best config is: 2 8
expected time is 7.296961
5 per stage
10 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 5
stage to rank map: 0,2,4,6,8;1,3,5,7,9;
World size is 10
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8;1,3,5,7,9; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.15273165702819824
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:29:29.782237 resume step from  127
2022-11-26 03:30:25.278923 - Finished loading checkpoint, takes 55.467 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:31:14.698170 Begin to exit
Process done with return code 0
Parent process ID: 23468 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 75 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 392 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1007 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1985 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4844 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13726 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21515 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.3292436599731445
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:31:36.378289 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:32:30.960117 - Finished loading checkpoint, takes 54.553 secs
2022-11-26 03:32:40.721944 Begin to exit
Process done with return code 0
Parent process ID: 24709 node: 172.31.18.152
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1703856.3232421875 0
End of simulation:  Mini-batch time (usec) = 4997769
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 63 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 425 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1002 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 2007 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4779 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 14544 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21622 microseconds

{1: 4.997769, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.25435566902160645
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:33:01.240879 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:33:56.001300 - Finished loading checkpoint, takes 54.730 secs
2022-11-26 03:34:05.817968 Begin to exit
Process done with return code 0
Parent process ID: 26005 node: 172.31.27.216
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 63 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 393 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1035 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1935 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4848 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13680 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21577 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.11556529998779297
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:34:56.191008 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2022-11-26 03:35:51.619827 - Finished loading checkpoint, takes 55.400 secs
2022-11-26 03:36:01.141295 Begin to exit
Process done with return code 0
Parent process ID: 27246 node: 172.31.27.216
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1703856.3232421875 0
End of simulation:  Mini-batch time (usec) = 4997769
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 64 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 392 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 995 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 2030 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4785 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13895 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21538 microseconds

{1: 4.997769, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.17507433891296387
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:36:21.483491 resume step from  127
2022-11-26 03:37:11.157852 - Finished loading checkpoint, takes 49.645 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:37:24.130735] Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5799.990
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:37:29.046492] Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4915.656
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:37:32.946851] Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3900.225
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:37:36.794580] Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.704
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:37:40.661999] Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3867.395
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:37:44.503206] Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3841.160
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:37:48.353479] Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3850.259
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:37:52.220900] Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3867.389
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:37:56.037748] Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3816.803
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:37:59.938881] Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3901.119
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:38:03.800070] Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3861.187
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:38:07.689280] Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3889.141
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:38:11.560065] Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3870.813
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:38:15.417687] Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3857.509
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:38:19.332856] Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3915.136
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:38:23.205419] Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3872.522
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:38:27.100563] Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3895.239
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:38:30.948153] Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.453
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:38:34.824326] Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3876.130
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:38.702210] Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3877.841
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:42.633401] Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3931.173
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:46.546177] Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3912.754
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:50.423489] Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3877.299
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:54.262784] Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3839.235
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:58.096213] Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3833.395
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:01.969626] Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3873.377
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:05.881042] Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3911.401
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:09.780071] Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3899.017
2022-11-26 03:39:09.787352 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!


Rank 2 signal handler called with signal 10

Opt ckpt time 9.128617525100708
Process done with return code 0
Parent process ID: 28611 node: 172.31.27.216
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 2234690.185546875 0
End of simulation:  Mini-batch time (usec) = 5528603
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 64 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 343 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1060 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1942 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4816 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13719 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22912 microseconds

{1: 5.528603, 2: 6.142701, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.142701
7 per stage
14 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 155
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.1596686840057373
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
19 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_155.pt
2022-11-26 03:39:41.106694 resume step from  155
2022-11-26 03:40:22.147264 - Finished loading checkpoint, takes 41.014 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:40:31.903805] Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5572.732
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:40:35.452905] Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3548.854
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:40:39.019566] Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3566.651
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:40:42.538744] Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3519.123
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:40:46.066975] Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3528.201
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:40:49.565549] Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3498.567
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:40:53.138978] Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3573.379
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:40:56.715431] Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3576.424
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:41:00.255266] Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3539.807
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:41:03.740156] Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.868
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:41:07.353190] Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3612.987
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:41:10.951313] Finished iteration 166, CKPT_AND_STOP: True, flag: tensor([4], dtype=torch.int32), speed: 3598.138
2022-11-26 03:41:10.955871 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.788268327713013
Process done with return code 0
Parent process ID: 29677 node: 172.31.27.216
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 280 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 747 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1457 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3238 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6717 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21602 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 167
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.20109939575195312
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_167.pt
2022-11-26 03:41:43.067219 resume step from  167
2022-11-26 03:42:20.271953 - Finished loading checkpoint, takes 37.178 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:42:48.535520] Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6079.062
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:42:52.545948] Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4010.192
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:42:55.536697] Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2990.655
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:42:58.531091] Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.461
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:43:02.510566] Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3979.461
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:43:05.450482] Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2939.805
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:43:08.419662] Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.107
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:43:11.396346] Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.657
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:43:14.341708] Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2945.337
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:43:17.299308] Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2957.586
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:43:20.261018] Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2961.667
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:43:23.214839] Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2953.800
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:43:26.230316] Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.438
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:43:29.206808] Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.440
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:43:32.227278] Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3020.428
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:43:35.223149] Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.832
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:43:38.174578] Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2951.396
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:43:41.184134] Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3009.515
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:43:44.126464] Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2942.486
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:47.089821] Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2963.165
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:50.028564] Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2938.708
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:52.996466] Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.882
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:55.965702] Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.194
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:58.892610] Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2926.869
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:01.895836] Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3003.210
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:04.862197] Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.327
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:07.896100] Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.881
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:10.872263] Finished iteration 194, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 2976.164
2022-11-26 03:44:10.876880 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.488075494766235
Process done with return code 0
Parent process ID: 30795 node: 172.31.27.216
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2490411.62109375 0
End of simulation:  Mini-batch time (usec) = 4792107
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 280 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1455 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3192 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6721 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21656 microseconds

{1: 4.792107, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 195
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.197129726409912
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_195.pt
2022-11-26 03:44:41.374762 resume step from  195
2022-11-26 03:45:33.305510 - Finished loading checkpoint, takes 51.902 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:45:54.570615] Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6094.292
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:45:57.516877] Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2946.015
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:46:00.550639] Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.717
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:46:03.558727] Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.068
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:46:06.547738] Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.969
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:46:09.514489] Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.772
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:46:12.502611] Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.058
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:46:15.509145] Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.500
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:46:18.498792] Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.666
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:46:22.316980] Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3818.161
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:46:25.332337] Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.282
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:46:29.087901] Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3755.522
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:46:32.102498] Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.702
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:46:35.094980] Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2992.323
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:46:38.079333] Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2984.341
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:46:41.070169] Finished iteration 210, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 2990.826
2022-11-26 03:46:41.074849 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.460753679275513
Process done with return code 0
Parent process ID: 32076 node: 172.31.22.165
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 5124879
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 57 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 298 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 824 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1493 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4798 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6683 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21645 microseconds

{1: 5.124879, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 211
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.1750795841217041
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_211.pt
2022-11-26 03:47:40.905293 resume step from  211
2022-11-26 03:48:24.772674 - Finished loading checkpoint, takes 43.839 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:48:40.229347] Finished iteration 211, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 5270.364
2022-11-26 03:48:40.234484 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 8.083301067352295
Process done with return code 0
Parent process ID: 33254 node: 172.31.22.165
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 1937331.4208984375 0
End of simulation:  Mini-batch time (usec) = 4900183
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 64 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 343 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 814 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 2023 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4844 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13691 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21606 microseconds

{1: 4.900183, 2: 6.142701, 3: 5.992289, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 3 8
expected time is 5.992289
5 per stage
15 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 3
chunk_size: 8
data depth: 5
stage to rank map: 0,3,6,9,12;1,4,7,10,13;2,5,8,11,14;
World size is 15
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,3,6,9,12;1,4,7,10,13;2,5,8,11,14; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 212
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.13461017608642578
SHARED WEIGHTS ARE
[(0, 2)]
this rank  2 is part of pipeline replica  0
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_212.pt
2022-11-26 03:49:40.559454 resume step from  212
2022-11-26 03:50:10.148824 - Finished loading checkpoint, takes 29.568 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:50:26.367568] Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7490.000
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:50:30.769243] Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4401.457
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:50:35.177552] Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4408.261
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:50:38.580068] Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3402.474
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:50:42.953974] Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4373.885
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:50:46.304707] Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3350.705
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:50:49.767975] Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3463.237
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:50:53.309109] Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3541.103
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:50:56.747159] Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3438.030
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:51:00.206192] Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3459.043
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:51:03.686292] Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3480.062
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:51:07.133473] Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3447.123
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:51:10.572006] Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3438.499
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:51:13.981348] Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3409.308
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:51:17.359892] Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3378.522
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:51:20.702468] Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3342.563
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:51:24.130061] Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3427.563
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:51:27.574518] Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3444.436
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:51:30.988257] Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3413.685
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:34.376675] Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3388.387
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:37.776019] Finished iteration 232, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3399.320
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:41.159242] Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3383.193
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:44.569532] Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3410.263
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:48.021620] Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3452.057
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:51.405200] Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3383.541
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:54.862246] Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3457.060
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:58.292942] Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3430.637
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:01.707097] Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3414.122
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:05.100700] Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3393.570
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:08.528613] Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3427.899
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 2 signal handler called with signal 10
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:11.957189] Finished iteration 242, CKPT_AND_STOP: True, flag: tensor([5], dtype=torch.int32), speed: 3428.568
2022-11-26 03:52:11.960590 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 9.786790609359741
Process done with return code 0
Parent process ID: 34211 node: 172.31.22.165
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2595254.8828125 0
End of simulation:  Mini-batch time (usec) = 5226764
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 56 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 295 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 862 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1453 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4793 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6664 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21563 microseconds

{1: 5.226764, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(2, 3)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=2 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 243
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.17205071449279785
SHARED WEIGHTS ARE
[(0, 1)]
this rank  2 is part of pipeline replica  1
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_243.pt
2022-11-26 03:52:42.196344 resume step from  243
2022-11-26 03:53:16.153160 - Finished loading checkpoint, takes 33.930 secs
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:53:31.594732] Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5345.649
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:53:34.796213] Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3201.276
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:53:38.093333] Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3297.073
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:53:42.346365] Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4253.039
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:53:46.582566] Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4236.143
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:53:49.752233] Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3169.630
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:53:52.959557] Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3207.297
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:53:56.207531] Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3247.946
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:53:59.525317] Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3317.755
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:54:02.753335] Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3227.989
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:54:05.980942] Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3227.578
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:54:09.188605] Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3207.629
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:54:12.391065] Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3202.435
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:54:15.619132] Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3228.060
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:54:18.836608] Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3217.416
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:54:22.057965] Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3221.340
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:54:25.244693] Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3186.685
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:54:28.386525] Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3141.797
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:54:31.606279] Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3219.766
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:34.912386] Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3306.032
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:38.072920] Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3160.547
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:41.256261] Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3183.269
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:44.434035] Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3177.739
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:47.568642] Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3134.580
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:50.785190] Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3216.518
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:53.960974] Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3175.752
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:57.147450] Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3186.450
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:00.356651] Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3209.176
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:03.569306] Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3212.617
2 Overflow !!
2 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:06.774487] Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3205.160
