Parent process ID: 9803 node: 172.31.17.44
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 50 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 275 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 694 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1503 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3156 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6687 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21639 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.07691049575805664
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
15 chunks
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:05:33.182384] Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4501.982
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:05:37.225923] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4043.318
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:05:41.201962] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3976.084
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:05:44.207566] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3005.419
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:05:47.221357] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3013.760
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:05:50.257423] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.039
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:05:53.232106] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2974.659
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:05:56.304812] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3072.797
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:05:59.385615] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3080.684
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:06:02.386018] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.331
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:06:05.397314] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3011.276
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:06:08.387134] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.800
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:06:11.377195] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2990.009
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:06:14.383462] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.288
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:06:17.378881] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.328
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:06:20.360558] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.674
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:06:23.342246] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.652
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:06:26.365246] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3022.944
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:06:29.345478] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2980.227
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:32.341761] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2996.273
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:35.308911] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.072
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:38.364540] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.601
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:41.386394] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.823
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:44.410986] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3024.564
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:47.425195] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.177
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:50.480781] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.556
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:53.517321] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.578
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:56.546041] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3028.625
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:59.601550] Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.611
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:02.609490] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.779
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:05.626741] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3017.226
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:08.679990] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.216
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:11.716572] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.557
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:14.756631] Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3040.022
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:17.750967] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.311
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:20.750471] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2999.476
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:23.748884] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2998.416
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:26.730713] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.768
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:29.731517] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.774
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:32.765357] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.850
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:35.853383] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3087.961
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:38.907730] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3054.319
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:41.915498] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.742
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:44.950477] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.014
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:47.955520] Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3004.939
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:50.926162] Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2970.611
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:53.922131] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.942
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:56.966118] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3043.981
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:00.068625] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3102.454
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:03.097684] Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.063
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:06.133135] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.398
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:09.105062] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2971.900
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:12.134224] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.129
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:15.179062] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3044.812
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:18.231092] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3051.993
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:21.265894] Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.782
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:24.356730] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3090.800
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:27.398388] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3041.660
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:30.461655] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.265
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:33.514581] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.835
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:36.583777] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3069.168
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:39.647512] Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.712
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:42.744735] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3097.187
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:45.813526] Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3068.913
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:48.866866] Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.163
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:51.967482] Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3100.600
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:55.020381] Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.857
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:58.028521] Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.142
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:01.081032] Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.449
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:04.102574] Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.519
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:07.139625] Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3037.077
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:10.187526] Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3047.883
2022-11-26 03:09:10.191999 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Opt ckpt time 6.003840923309326
Process done with return code 0
Parent process ID: 10366 node: 172.31.17.44
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 260 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 701 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1188 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3191 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6758 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21590 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 72
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.047034263610839844
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_72.pt
2022-11-26 03:09:45.177834 resume step from  72
2022-11-26 03:10:29.711586 - Finished loading checkpoint, takes 44.505 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:10:58.922188] Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9616.263
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:11:02.738251] Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3815.813
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:11:05.543539] Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2805.215
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:11:08.371536] Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2827.969
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:11:11.248266] Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2876.726
2022-11-26 03:11:11.324872 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Opt ckpt time 5.6427531242370605
Process done with return code 0
Parent process ID: 11547 node: 172.31.17.44
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2350970.703125 0
End of simulation:  Mini-batch time (usec) = 4325616
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 253 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 604 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1183 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3207 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6927 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21465 microseconds

{1: 4.325616, 2: 4.315108, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.028483152389526367
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:11:41.972998 resume step from  77
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:12:32.857904 - Finished loading checkpoint, takes 50.857 secs
2022-11-26 03:12:47.097521 Begin to exit
Process done with return code 0
Parent process ID: 12693 node: 172.31.17.44
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2300677.490234375 0
End of simulation:  Mini-batch time (usec) = 4275323
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 45 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 226 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 602 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1241 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3249 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6718 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21460 microseconds

{1: 4.275323, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.23880958557128906
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:13:13.966387 resume step from  77
2022-11-26 03:13:59.339537 - Finished loading checkpoint, takes 45.344 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:14:24.018256] Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9621.741
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:14:26.633553] Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2615.201
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:14:30.229853] Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.191
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:14:32.812901] Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2582.985
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:14:35.390793] Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2577.891
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:14:38.003212] Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2612.369
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:14:40.577967] Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2574.718
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:14:43.142218] Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.222
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:14:45.719205] Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.956
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:14:48.320457] Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2601.228
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:14:50.887441] Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2566.941
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:14:53.433926] Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2546.467
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:14:55.998159] Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.187
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:14:58.571891] Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2573.714
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:15:01.128403] Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2556.495
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:15:03.715528] Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2587.072
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:15:06.294336] Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2578.856
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:15:08.844460] Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2550.029
2022-11-26 03:15:08.848973 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.544248580932617
Process done with return code 0
Parent process ID: 13949 node: 172.31.17.44
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2325752.685546875 0
End of simulation:  Mini-batch time (usec) = 4300398
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 49 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 223 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 640 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1177 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3220 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6721 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21530 microseconds

{1: 4.300398, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.19693875312805176
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:15:42.428729 resume step from  95
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:16:30.160559 - Finished loading checkpoint, takes 47.704 secs
2022-11-26 03:16:44.909225 Begin to exit
Process done with return code 0
Parent process ID: 15148 node: 172.31.17.44
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2258812.98828125 0
End of simulation:  Mini-batch time (usec) = 4233458
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 215 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 514 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 995 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2403 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4449 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10476 microseconds

{1: 4.233458, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.20520567893981934
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:17:10.878988 resume step from  95
2022-11-26 03:17:59.679514 - Finished loading checkpoint, takes 48.771 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:18:18.059247] Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 8905.625
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:18:21.543566] Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.064
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:18:25.003175] Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3459.571
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:18:28.451254] Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3448.068
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:18:31.915492] Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3464.237
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:18:34.422365] Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.751
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:18:36.885220] Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2462.827
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:18:39.358078] Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2472.837
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:18:41.876672] Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2518.616
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:18:44.361987] Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2485.200
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:18:46.831135] Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2469.115
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:18:49.361947] Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2530.813
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:18:51.850778] Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2488.871
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:18:54.321152] Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2470.240
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:18:56.818958] Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2497.781
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:18:59.257940] Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2438.948
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:19:01.764082] Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.134
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:19:04.221280] Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2457.150
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:19:06.660842] Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2439.539
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:09.137146] Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2476.264
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:11.599157] Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2462.030
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:14.094215] Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2495.009
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:16.588535] Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2494.297
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:19.018181] Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2429.621
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:21.463765] Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2445.564
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:23.930651] Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2466.809
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:26.389710] Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2459.067
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:28.909531] Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2519.757
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:31.344999] Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2435.439
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:33.782668] Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2437.642
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:36.274181] Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2491.484
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:38.694038] Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2419.838
2022-11-26 03:19:38.698635 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 6.0752503871917725
Process done with return code 0
Parent process ID: 16435 node: 172.31.19.112
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 89 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 246 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1177 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3213 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6736 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21759 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.4271352291107178
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:20:43.979231 resume step from  127
2022-11-26 03:21:35.783366 - Finished loading checkpoint, takes 51.776 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:21:53.769846 Begin to exit
Process done with return code 0
Parent process ID: 17640 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 5124879
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 57 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 291 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 829 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1506 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4803 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6723 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21641 microseconds

{1: 5.124879, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.22729039192199707
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:22:43.637287 resume step from  127
2022-11-26 03:23:33.096485 - Finished loading checkpoint, takes 49.431 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:23:58.043444 Begin to exit
Process done with return code 0
Parent process ID: 9717 node: 172.31.23.137
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 2234690.185546875 0
End of simulation:  Mini-batch time (usec) = 5528603
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 75 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 357 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1022 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1993 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4837 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13790 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22007 microseconds

{1: 5.528603, 2: 6.142701, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.142701
7 per stage
14 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.003938913345336914
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
19 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:24:48.527122 resume step from  127
2022-11-26 03:25:45.098013 - Finished loading checkpoint, takes 56.541 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:25:52.804928 Begin to exit
Process done with return code 0
Parent process ID: 8207 node: 172.31.28.143
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 69 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 422 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 995 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1978 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4771 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13658 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21548 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.23344635963439941
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:26:43.391277 resume step from  127
2022-11-26 03:27:35.844467 - Finished loading checkpoint, takes 52.423 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:27:47.670218 Begin to exit
Process done with return code 0
Parent process ID: 16572 node: 172.31.31.40
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 70 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 415 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 994 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1932 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4795 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13791 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21610 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.3128321170806885
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:31:36.371340 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:32:40.710983 - Finished loading checkpoint, takes 64.311 secs
2022-11-26 03:32:40.716974 Begin to exit
Process done with return code 0
Parent process ID: 17819 node: 172.31.31.40
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1703856.3232421875 0
End of simulation:  Mini-batch time (usec) = 4997769
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 63 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 389 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1023 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1937 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4805 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13654 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21456 microseconds

{1: 4.997769, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.2332170009613037
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:33:01.225001 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:33:55.295484 - Finished loading checkpoint, takes 54.042 secs
2022-11-26 03:34:05.812831 Begin to exit
Process done with return code 0
Parent process ID: 17453 node: 172.31.16.100
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 63 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 394 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1000 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1986 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4786 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13692 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21495 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.19135046005249023
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:34:56.154805 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
2022-11-26 03:35:51.435287 - Finished loading checkpoint, takes 55.252 secs
2022-11-26 03:36:01.135821 Begin to exit
Process done with return code 0
Parent process ID: 18699 node: 172.31.16.100
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1703856.3232421875 0
End of simulation:  Mini-batch time (usec) = 4997769
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 61 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 396 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 997 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1949 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4810 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13665 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 22634 microseconds

{1: 4.997769, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.17806339263916016
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:36:21.423100 resume step from  127
2022-11-26 03:37:10.185142 - Finished loading checkpoint, takes 48.732 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:37:24.126019] Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5800.573
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:37:29.041329] Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4915.092
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:37:32.942070] Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3900.634
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:37:36.789852] Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.778
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:37:40.657289] Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3867.402
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:37:44.498319] Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3840.986
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:37:48.348753] Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3850.425
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:37:52.216066] Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3867.267
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:37:56.032939] Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3816.887
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:37:59.934135] Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3901.131
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:38:03.795375] Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3861.199
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:38:07.684573] Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3889.156
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:38:11.555177] Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3870.587
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:38:15.412891] Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3857.728
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:38:19.328186] Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3915.200
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:38:23.200787] Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3872.587
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:38:27.095697] Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3894.877
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:38:30.943398] Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.677
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:38:34.819457] Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3876.014
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:38.697436] Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3877.959
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:42.628759] Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3931.287
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:46.541442] Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3912.654
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:50.418643] Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3877.173
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:54.258121] Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3839.442
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:58.091471] Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3833.309
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:01.965034] Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3873.572
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:05.876415] Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3911.291
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:09.775429] Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3899.005
2022-11-26 03:39:09.780216 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Parent process ID: 20065 node: 172.31.16.100
Killing process on gpu with nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 2234690.185546875 0
End of simulation:  Mini-batch time (usec) = 5528603
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 62 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 405 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1009 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1991 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4843 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13714 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21541 microseconds

{1: 5.528603, 2: 6.142701, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.142701
7 per stage
14 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 155
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.16591763496398926
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
19 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_155.pt
2022-11-26 03:39:41.102740 resume step from  155
2022-11-26 03:40:15.010170 - Finished loading checkpoint, takes 33.878 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:40:31.898932] Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5573.687
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:40:35.448201] Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3548.958
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:40:39.014737] Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3566.501
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:40:42.534024] Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3519.259
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:40:46.062299] Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3528.255
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:40:49.560805] Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3498.492
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:40:53.134165] Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3573.322
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:40:56.710595] Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3576.358
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:41:00.250530] Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3539.923
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:41:03.735424] Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.861
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:41:07.348392] Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3612.968
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:41:10.946594] Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([4], dtype=torch.int32), speed: 3598.154
2022-11-26 03:41:10.951206 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Opt ckpt time 6.878856182098389
Process done with return code 0
Parent process ID: 21089 node: 172.31.16.100
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 60 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 279 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 697 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1452 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3224 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6712 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21601 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 167
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.18184566497802734
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_167.pt
2022-11-26 03:41:43.067195 resume step from  167
2022-11-26 03:42:21.569626 - Finished loading checkpoint, takes 38.473 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:42:48.530549] Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6079.331
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:42:52.540971] Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4010.149
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:42:55.531829] Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2990.711
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:42:58.526238] Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.381
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:43:02.504989] Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3978.806
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:43:05.445676] Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2940.562
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:43:08.414846] Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.126
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:43:11.391674] Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.799
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:43:14.336940] Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2945.262
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:43:17.294217] Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2957.218
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:43:20.256317] Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2962.057
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:43:23.210103] Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2953.757
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:43:26.225725] Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.593
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:43:29.202160] Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.409
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:43:32.222440] Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3020.249
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:43:35.218339] Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.883
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:43:38.169837] Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2951.456
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:43:41.179301] Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3009.431
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:43:44.121624] Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2942.294
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:47.085031] Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2963.413
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:50.023868] Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2938.770
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:52.991711] Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.821
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:55.960824] Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.082
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:58.888020] Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2927.173
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:01.891204] Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3003.156
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:04.857496] Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.246
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:07.891366] Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.820
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:10.867512] Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2976.140
2022-11-26 03:44:10.872140 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Opt ckpt time 5.879492282867432
Process done with return code 0
Parent process ID: 22213 node: 172.31.16.100
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2490411.62109375 0
End of simulation:  Mini-batch time (usec) = 4792107
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 57 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 279 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 742 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1451 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3278 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6696 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21527 microseconds

{1: 4.792107, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 195
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.1775410175323486
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_195.pt
2022-11-26 03:44:41.357043 resume step from  195
2022-11-26 03:45:35.139162 - Finished loading checkpoint, takes 53.753 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:45:54.565639] Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6095.071
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:45:57.511927] Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2945.936
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:46:00.545976] Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.029
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:46:03.554024] Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.016
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:46:06.542695] Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.642
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:46:09.509762] Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.033
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:46:12.497929] Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.146
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:46:15.504432] Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.480
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:46:18.493999] Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.531
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:46:22.312169] Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3818.145
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:46:25.327555] Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.360
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:46:29.083112] Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3755.528
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:46:32.097674] Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.534
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:46:35.090294] Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2992.592
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:46:38.074591] Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2984.261
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:46:41.065325] Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2990.711
2022-11-26 03:46:41.069849 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Opt ckpt time 5.756442546844482
Process done with return code 0
Parent process ID: 14373 node: 172.31.24.208
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 5124879
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 296 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 816 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1496 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4786 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6722 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21694 microseconds

{1: 5.124879, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 211
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.008085012435913086
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_211.pt
2022-11-26 03:47:40.921562 resume step from  211
2022-11-26 03:48:24.983275 - Finished loading checkpoint, takes 44.032 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:48:40.230228] Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 5270.627
2022-11-26 03:48:40.235518 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 5.328689336776733
Signal handler called with signal 10


 STOPPING VARUNA !!



Process done with return code -10
Parent process ID: 21335 node: 172.31.19.112
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 1937331.4208984375 0
End of simulation:  Mini-batch time (usec) = 4900183
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 60 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 338 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 843 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1936 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4828 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13766 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21549 microseconds

{1: 4.900183, 2: 6.142701, 3: 5.992289, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 3 8
expected time is 5.992289
5 per stage
15 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 3
chunk_size: 8
data depth: 5
stage to rank map: 0,3,6,9,12;1,4,7,10,13;2,5,8,11,14;
World size is 15
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,3,6,9,12;1,4,7,10,13;2,5,8,11,14; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 212
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.14864325523376465
SHARED WEIGHTS ARE
[(0, 2)]
this rank  11 is part of pipeline replica  3
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_212.pt
2022-11-26 03:49:40.551098 resume step from  212
2022-11-26 03:50:10.306132 - Finished loading checkpoint, takes 29.735 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:50:26.366236] Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7490.521
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:50:30.767828] Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4401.290
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:50:35.176031] Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4408.189
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:50:38.578786] Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3402.605
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:50:42.952107] Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4373.349
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:50:46.303512] Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3351.324
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:50:49.766728] Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3463.170
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:50:53.307949] Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3541.245
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:50:56.745941] Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3437.901
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:51:00.204968] Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3459.001
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:51:03.684964] Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3479.959
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:51:07.132236] Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3447.247
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:51:10.570762] Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3438.490
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:51:13.980102] Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3409.376
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:51:17.358643] Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3378.451
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:51:20.701188] Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3342.515
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:51:24.128754] Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3427.518
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:51:27.573299] Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3444.518
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:51:30.987057] Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3413.721
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:34.375546] Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3388.456
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:37.774788] Finished iteration 232, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3399.265
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:41.157960] Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3383.088
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:44.568327] Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3410.355
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:48.020448] Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3452.080
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:51.404074] Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3383.694
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:54.861173] Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3457.070
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:58.291792] Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3430.470
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:01.705925] Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3414.097
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:05.099593] Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3393.731
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:08.527457] Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3427.732
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:11.955991] Finished iteration 242, CKPT_AND_STOP: False, flag: tensor([5], dtype=torch.int32), speed: 3428.519
2022-11-26 03:52:11.959346 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Opt ckpt time 4.460171937942505
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 11 signal handler called with signal 10
Process done with return code 0
Parent process ID: 22293 node: 172.31.19.112
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2595254.8828125 0
End of simulation:  Mini-batch time (usec) = 5226764
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 300 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 821 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1504 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4840 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6676 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21656 microseconds

{1: 5.226764, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(11, 12)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=11 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 243
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.18793463706970215
SHARED WEIGHTS ARE
[(0, 1)]
this rank  11 is part of pipeline replica  5
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_243.pt
2022-11-26 03:52:42.200968 resume step from  243
2022-11-26 03:53:16.221422 - Finished loading checkpoint, takes 33.992 secs
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:53:31.593488] Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5346.014
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:53:34.795053] Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3201.405
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:53:38.092031] Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3296.885
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:53:42.344445] Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4252.439
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:53:46.581348] Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4236.831
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:53:49.751038] Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3169.627
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:53:52.958170] Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3207.098
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:53:56.206286] Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3248.086
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:53:59.524144] Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3317.834
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:54:02.752100] Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3227.931
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:54:05.979787] Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3227.654
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:54:09.187405] Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3207.652
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:54:12.389647] Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3202.163
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:54:15.617911] Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3228.221
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:54:18.835455] Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3217.535
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:54:22.056770] Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3221.282
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:54:25.243364] Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3186.551
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:54:28.385301] Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3141.911
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:54:31.604965] Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3219.629
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:34.911169] Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3306.178
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:38.071762] Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3160.664
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:41.254975] Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3183.076
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:44.432829] Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3177.838
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:47.567391] Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3134.528
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:50.784032] Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3216.613
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:53.959678] Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3175.617
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:57.146141] Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3186.490
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:00.355479] Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3209.250
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:03.568189] Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3212.734
11 Overflow !!
11 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:06.773275] Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3205.003
