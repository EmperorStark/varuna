Parent process ID: 8388 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 58 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 284 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 693 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1567 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3194 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6713 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21546 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.04244065284729004
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
15 chunks
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:05:33.185441] Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4501.887
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:05:37.229753] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4044.019
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:05:41.205094] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3975.371
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:05:44.210749] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3005.545
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:05:47.224599] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3013.827
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:05:50.260620] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.988
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:05:53.235254] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2974.607
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:05:56.307831] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3072.540
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:05:59.388717] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3080.859
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:06:02.389241] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.492
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:06:05.400434] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3011.171
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:06:08.390381] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.912
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:06:11.380371] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.975
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:06:14.386538] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.140
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:06:17.382039] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.446
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:06:20.363726] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.664
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:06:23.345500] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.743
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:06:26.368407] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3022.892
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:06:29.348549] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2980.128
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:32.345046] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2996.458
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:35.312034] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.960
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:38.367669] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.578
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:41.389548] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.845
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:44.414051] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3024.479
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:47.428286] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.243
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:50.483896] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.561
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:53.520499] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.592
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:56.549173] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3028.620
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:06:59.604772] Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3055.573
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:02.612617] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.846
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:05.629784] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3017.109
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:08.683022] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.213
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:11.719627] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3036.607
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:14.759833] Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3040.151
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:17.754099] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.226
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:20.753644] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2999.531
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:23.751956] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2998.284
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:26.733756] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2981.748
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:29.734593] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3000.815
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:32.768464] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.865
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:35.856597] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3088.094
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:38.910800] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3054.163
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:41.918542] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3007.712
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:44.953596] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.025
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:47.958806] Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3005.193
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:50.929305] Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2970.464
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:53.925257] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.939
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:07:56.969238] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3043.935
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:00.071645] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3102.395
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:03.100755] Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.073
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:06.136352] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3035.574
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:09.108366] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2971.978
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:12.137390] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3029.016
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:15.182144] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3044.707
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:18.234208] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.036
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:21.268998] Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3034.751
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:24.359837] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3090.813
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:27.401445] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3041.583
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:30.464733] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.310
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:33.517705] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.899
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:36.586903] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3069.175
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:39.650590] Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3063.677
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:42.747769] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3097.130
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:45.816632] Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3068.853
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:48.870034] Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3053.359
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:51.970567] Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3100.522
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:55.023456] Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.847
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:08:58.031614] Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.130
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:01.084202] Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3052.595
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:04.105652] Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3021.406
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:07.142748] Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3037.041
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:09:10.190612] Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3047.836
2022-11-26 03:09:10.195181 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 10.275490045547485
Parent process ID: 8949 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 51 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 249 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 699 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1175 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3223 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6737 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21687 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 72
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.05017542839050293
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_72.pt
2022-11-26 03:09:45.211685 resume step from  72
2022-11-26 03:10:29.728877 - Finished loading checkpoint, takes 44.488 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:10:58.925454] Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9616.629
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:11:02.741398] Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3815.706
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:11:05.546557] Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2805.034
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:11:08.374645] Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2828.061
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:11:11.251333] Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2876.670
2022-11-26 03:11:11.259253 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 4.873149394989014
Process done with return code 0
Parent process ID: 10129 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2350970.703125 0
End of simulation:  Mini-batch time (usec) = 4325616
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 45 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 254 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 604 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1214 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3176 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6715 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21694 microseconds

{1: 4.325616, 2: 4.315108, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.02122807502746582
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:11:42.002406 resume step from  77
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:12:28.346173 - Finished loading checkpoint, takes 46.315 secs
2022-11-26 03:12:47.100626 Begin to exit
Process done with return code 0
Parent process ID: 11277 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2300677.490234375 0
End of simulation:  Mini-batch time (usec) = 4275323
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 54 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 236 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 614 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1191 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3243 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6710 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21719 microseconds

{1: 4.275323, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 77
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.2513298988342285
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_77.pt
2022-11-26 03:13:13.959006 resume step from  77
2022-11-26 03:13:59.202277 - Finished loading checkpoint, takes 45.215 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:14:24.021360] Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9621.225
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:14:26.636487] Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2614.866
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:14:30.232862] Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.399
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:14:32.815893] Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2582.942
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:14:35.393811] Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2577.887
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:14:38.006176] Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2612.344
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:14:40.580950] Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2574.745
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:14:43.145286] Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.322
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:14:45.722134] Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2576.813
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:14:48.323504] Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2601.333
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:14:50.890494] Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2566.965
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:14:53.436955] Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2546.431
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:14:56.001294] Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2564.312
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:14:58.574937] Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2573.611
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:15:01.131376] Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2556.402
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:15:03.718562] Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2587.163
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:15:06.297304] Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2578.704
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:15:08.847543] Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2550.250
2022-11-26 03:15:08.852194 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 4.528059244155884
Process done with return code 0
Parent process ID: 12529 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2325752.685546875 0
End of simulation:  Mini-batch time (usec) = 4300398
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 49 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 637508.1176757812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4128575
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 57100, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 227 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 19 0 455501.3732910156 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4359024
Min send: 10000000, max send 0
Min long send: 248801, max long send 272408
Min fwd: 34386, max fwd 67951; min bwd 53743, max bwd 63367
Min long fwd: 37367, max long fwd 45094; min long bwd 64590, max long bwd 71935
Time taken by simulation: 605 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1230 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3248 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6679 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21555 microseconds

{1: 4.300398, 2: 4.128575, 3: 4.359024, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.128575
11 per stage
22 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 11
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21;
World size is 22
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20;1,3,5,7,9,11,13,15,17,19,21; --batch-size=93 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.21181368827819824
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
12 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:15:42.473366 resume step from  95
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:16:30.240598 - Finished loading checkpoint, takes 47.739 secs
2022-11-26 03:16:44.912613 Begin to exit
Process done with return code 0
Parent process ID: 13723 node: 172.31.30.99
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 2258812.98828125 0
End of simulation:  Mini-batch time (usec) = 4233458
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140872, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 57 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 656490.9057617188 248719.58977934244
End of simulation:  Mini-batch time (usec) = 3966187
Min send: 10000000, max send 0
Min long send: 249051, max long send 264414
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 93560, max long bwd 98662
Time taken by simulation: 209 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 458491.3635253906 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4073233
Min send: 10000000, max send 0
Min long send: 248907, max long send 271185
Min fwd: 34639, max fwd 68075; min bwd 55135, max bwd 63761
Min long fwd: 38640, max long fwd 45018; min long bwd 64291, max long bwd 71935
Time taken by simulation: 511 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 22 0 364029.9377441406 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4679330
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 21514, max fwd 60961; min bwd 37461, max bwd 50759
Min long fwd: 29330, max long fwd 37152; min long bwd 47928, max long bwd 55195
Time taken by simulation: 998 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 233226.318359375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6680183
Min send: 10000000, max send 0
Min long send: 248719, max long send 275881
Min fwd: 10965, max fwd 44004; min bwd 19992, max bwd 36467
Min long fwd: 22115, max long fwd 30954; min long bwd 32578, max long bwd 40795
Time taken by simulation: 2379 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 178155.74645996094 248719.58977934244
End of simulation:  Mini-batch time (usec) = 9019309
Min send: 10000000, max send 0
Min long send: 248735, max long send 276940
Min fwd: 5776, max fwd 41014; min bwd 16548, max bwd 29492
Min long fwd: 17309, max long fwd 25772; min long bwd 25282, max long bwd 32088
Time taken by simulation: 4418 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 117116.2109375 248719.58977934244
End of simulation:  Mini-batch time (usec) = 13687683
Min send: 10000000, max send 0
Min long send: 248720, max long send 280797
Min fwd: 141, max fwd 25024; min bwd 6433, max bwd 23373
Min long fwd: 7201, max long fwd 18161; min long bwd 16833, max long bwd 28389
Time taken by simulation: 10503 microseconds

{1: 4.233458, 2: 3.966187, 3: 4.073233, 4: 4.67933, 6: 6.680183, 8: 9.019309, 12: 13.687683}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 3.966187
12 per stage
24 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 12
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22;1,3,5,7,9,11,13,15,17,19,21,23; --batch-size=85 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 95
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.23897695541381836
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
11 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_95.pt
2022-11-26 03:17:10.863271 resume step from  95
2022-11-26 03:17:58.611796 - Finished loading checkpoint, takes 47.721 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:18:18.062171] Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 8905.214
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:18:21.546468] Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.016
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:18:25.006572] Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3460.069
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:18:28.454322] Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3447.752
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:18:31.918352] Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3463.919
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:18:34.425417] Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2507.025
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:18:36.888156] Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2462.744
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:18:39.361037] Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2472.812
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:18:41.879703] Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2518.646
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:18:44.364903] Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2485.161
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:18:46.834219] Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2469.277
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:18:49.364925] Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2530.680
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:18:51.853777] Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2488.816
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:18:54.324111] Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2470.353
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:18:56.821917] Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2497.737
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:18:59.260922] Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2438.975
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:19:01.767132] Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2506.179
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:19:04.224169] Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2457.038
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:19:06.663728] Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2439.491
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:09.140112] Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2476.421
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:11.602038] Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2461.837
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:14.097146] Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2495.096
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:16.591393] Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2494.194
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:19.021212] Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2429.788
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:21.466761] Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2445.514
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:23.933551] Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2466.761
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:26.392612] Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2459.031
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:28.912453] Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2519.830
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:31.347890] Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2435.391
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:33.785568] Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2437.647
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:36.277236] Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2491.643
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:19:38.696923] Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32), speed: 2419.661
2022-11-26 03:19:38.701456 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 7.3852198123931885
Process done with return code 0
Parent process ID: 16441 node: 172.31.22.229
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 3324582.03125 0
End of simulation:  Mini-batch time (usec) = 5626278
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 55 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 13 0 639521.4233398438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4315108
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58383, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 248 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 704 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 26 0 356986.63330078125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5178944
Min send: 10000000, max send 0
Min long send: 248907, max long send 273926
Min fwd: 22410, max fwd 60312; min bwd 37981, max bwd 50449
Min long fwd: 30741, max long fwd 37490; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1175 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3225 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6706 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21636 microseconds

{1: 5.626278, 2: 4.315108, 3: 4.729712, 4: 5.178944, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.315108
10 per stage
20 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 10
stage to rank map: 0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18;1,3,5,7,9,11,13,15,17,19; --batch-size=102 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.43928980827331543
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
13 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:20:44.009104 resume step from  127
2022-11-26 03:21:35.812034 - Finished loading checkpoint, takes 51.775 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:21:53.770948 Begin to exit
Process done with return code 0
Parent process ID: 17695 node: 172.31.22.229
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 5124879
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 59 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 299 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 817 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1453 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4948 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6698 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21555 microseconds

{1: 5.124879, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.23989129066467285
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:22:43.630493 resume step from  127
2022-11-26 03:23:33.015796 - Finished loading checkpoint, takes 49.356 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:23:58.041386 Begin to exit
Process done with return code 0
Parent process ID: 19035 node: 172.31.22.165
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 2234690.185546875 0
End of simulation:  Mini-batch time (usec) = 5528603
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 99 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 342 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 998 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1973 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4792 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13716 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21572 microseconds

{1: 5.528603, 2: 6.142701, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.142701
7 per stage
14 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.1925220489501953
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
19 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:24:48.524519 resume step from  127
2022-11-26 03:25:38.531505 - Finished loading checkpoint, takes 49.979 secs
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:25:52.804949 Begin to exit
Process done with return code 0
Parent process ID: 20232 node: 172.31.28.236
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 65 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 392 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1002 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1994 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4795 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13726 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21577 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.22245526313781738
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:26:43.328550 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:27:47.277497 - Finished loading checkpoint, takes 63.920 secs
2022-11-26 03:27:47.671789 Begin to exit
Process done with return code 0
Parent process ID: 21484 node: 172.31.28.236
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 12 0 1301385.1318359375 0
End of simulation:  Mini-batch time (usec) = 5252680
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145763; min long bwd 182761, max long bwd 191040
Time taken by simulation: 63 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 26 0 555338.134765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7296961
Min send: 10000000, max send 0
Min long send: 249051, max long send 272408
Min fwd: 78412, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 56472, max long fwd 64412; min long bwd 91514, max long bwd 98662
Time taken by simulation: 454 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 43 0 338854.248046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8832625
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 50125, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 63246, max long bwd 71935
Time taken by simulation: 1327 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 64 0 207089.01977539062 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10812652
Min send: 10000000, max send 0
Min long send: 248838, max long send 278723
Min fwd: 20508, max fwd 60312; min bwd 38425, max bwd 50813
Min long fwd: 28341, max long fwd 38392; min long bwd 47725, max long bwd 56957
Time taken by simulation: 2922 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 18586236
Min send: 10000000, max send 0
Min long send: 248758, max long send 288457
Min fwd: 9602, max fwd 45672; min bwd 22890, max bwd 38402
Min long fwd: 18236, max long fwd 30331; min long bwd 30481, max long bwd 41890
Time taken by simulation: 9747 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13796 microseconds

can't have 12 stages!
{1: 5.25268, 2: 7.296961, 3: 8.832625, 4: 10.812652, 6: 18.586236, 8: 19.658022}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8}
best config is: 2 8
expected time is 7.296961
5 per stage
10 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 5
stage to rank map: 0,2,4,6,8;1,3,5,7,9;
World size is 10
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8;1,3,5,7,9; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.21123027801513672
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:28:43.036113 resume step from  127
Parent process ID: 22141 node: 172.31.28.236
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 12 0 1301385.1318359375 0
End of simulation:  Mini-batch time (usec) = 5252680
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145763; min long bwd 182761, max long bwd 191040
Time taken by simulation: 66 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 26 0 555338.134765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7296961
Min send: 10000000, max send 0
Min long send: 249051, max long send 272408
Min fwd: 78412, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 56472, max long fwd 64412; min long bwd 91514, max long bwd 98662
Time taken by simulation: 457 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 43 0 338854.248046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8832625
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 50125, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 63246, max long bwd 71935
Time taken by simulation: 1370 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 64 0 207089.01977539062 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10812652
Min send: 10000000, max send 0
Min long send: 248838, max long send 278723
Min fwd: 20508, max fwd 60312; min bwd 38425, max bwd 50813
Min long fwd: 28341, max long fwd 38392; min long bwd 47725, max long bwd 56957
Time taken by simulation: 2907 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 18586236
Min send: 10000000, max send 0
Min long send: 248758, max long send 288457
Min fwd: 9602, max fwd 45672; min bwd 22890, max bwd 38402
Min long fwd: 18236, max long fwd 30331; min long bwd 30481, max long bwd 41890
Time taken by simulation: 9791 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13661 microseconds

can't have 12 stages!
{1: 5.25268, 2: 7.296961, 3: 8.832625, 4: 10.812652, 6: 18.586236, 8: 19.658022}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8}
best config is: 2 8
expected time is 7.296961
5 per stage
10 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 5
stage to rank map: 0,2,4,6,8;1,3,5,7,9;
World size is 10
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8;1,3,5,7,9; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.16370153427124023
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:29:34.729774 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:31:14.688609 - Finished loading checkpoint, takes 99.931 secs
2022-11-26 03:31:14.694422 Begin to exit
Process done with return code 0
Parent process ID: 23388 node: 172.31.28.236
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 66 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 393 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 993 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1988 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4802 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13700 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21627 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.3244638442993164
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:31:36.374203 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:32:40.557487 - Finished loading checkpoint, takes 64.155 secs
2022-11-26 03:32:40.718228 Begin to exit
Process done with return code 0
Parent process ID: 24632 node: 172.31.28.236
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1703856.3232421875 0
End of simulation:  Mini-batch time (usec) = 4997769
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 61 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 390 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 997 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1990 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4820 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13702 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21588 microseconds

{1: 4.997769, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.25012874603271484
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:33:06.224514 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:34:05.808418 - Finished loading checkpoint, takes 59.555 secs
2022-11-26 03:34:05.814334 Begin to exit
Process done with return code 0
Parent process ID: 25982 node: 172.31.21.45
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 11 0 1328472.0458984375 0
End of simulation:  Mini-batch time (usec) = 4948024
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 139789, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 61 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 389 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1077 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1940 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4863 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13677 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21498 microseconds

{1: 4.948024, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.16138744354248047
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:34:56.199233 resume step from  127
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
2022-11-26 03:35:46.786759 - Finished loading checkpoint, takes 50.558 secs
2022-11-26 03:36:01.137603 Begin to exit
Process done with return code 0
Parent process ID: 27226 node: 172.31.21.45
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1703856.3232421875 0
End of simulation:  Mini-batch time (usec) = 4997769
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 63 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 22 0 601662.59765625 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6661684
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 393 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1051 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1933 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4806 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13679 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21526 microseconds

{1: 4.997769, 2: 6.661684, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.661684
6 per stage
12 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 6
stage to rank map: 0,2,4,6,8,10;1,3,5,7,9,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10;1,3,5,7,9,11; --batch-size=170 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 127
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.11622476577758789
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
22 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_127.pt
2022-11-26 03:36:21.436259 resume step from  127
2022-11-26 03:37:11.623912 - Finished loading checkpoint, takes 50.158 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:37:24.127162] Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5799.957
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:37:29.042892] Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4915.590
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:37:32.943320] Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3900.425
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:37:36.790998] Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.546
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:37:40.658426] Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3867.403
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:37:44.499494] Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3841.040
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:37:48.349949] Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3850.425
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:37:52.217334] Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3867.354
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:37:56.034256] Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3816.895
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:37:59.935318] Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3901.030
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:38:03.796514] Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3861.147
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:38:07.685746] Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3889.213
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:38:11.556354] Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3870.577
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:38:15.414055] Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3857.670
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:38:19.329317] Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3915.236
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:38:23.201890] Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3872.548
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:38:27.096934] Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3895.023
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:38:30.944602] Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3847.607
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:38:34.820745] Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3876.111
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:38.698566] Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3877.808
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:42.629857] Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3931.245
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:46.542530] Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3912.647
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:50.419893] Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3877.325
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:54.259231] Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3839.310
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:38:58.092650] Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3833.384
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:01.966103] Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3873.437
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:05.877495] Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3911.374
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:39:09.776497] Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32), speed: 3898.980
2022-11-26 03:39:09.781034 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 9.470949649810791
Process done with return code 0
Parent process ID: 28595 node: 172.31.21.45
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 2234690.185546875 0
End of simulation:  Mini-batch time (usec) = 5528603
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 62 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 342 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 374479.98046875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6710343
Min send: 10000000, max send 0
Min long send: 248773, max long send 274819
Min fwd: 32300, max fwd 68600; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44860; min long bwd 64435, max long bwd 71935
Time taken by simulation: 1000 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1982 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4792 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13780 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21642 microseconds

{1: 5.528603, 2: 6.142701, 3: 6.710343, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 6.142701
7 per stage
14 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 155
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.20121407508850098
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
19 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_155.pt
2022-11-26 03:39:41.065435 resume step from  155
2022-11-26 03:40:17.893825 - Finished loading checkpoint, takes 36.799 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:40:31.900234] Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5573.192
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:40:35.449344] Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3548.871
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:40:39.016031] Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3566.752
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:40:42.535163] Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3519.011
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:40:46.063382] Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3528.184
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:40:49.561991] Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3498.580
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:40:53.135381] Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3573.361
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:40:56.711876] Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3576.485
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:41:00.251634] Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3539.705
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:41:03.736558] Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3484.905
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:41:07.349681] Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3613.089
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:41:10.947681] Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([4], dtype=torch.int32), speed: 3597.993
2022-11-26 03:41:10.952318 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 5.904502868652344
Process done with return code 0
Parent process ID: 29620 node: 172.31.21.45
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2427995.1171875 0
End of simulation:  Mini-batch time (usec) = 4729691
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 276 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 690 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1568 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3187 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6717 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21699 microseconds

{1: 4.729691, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 167
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.20166826248168945
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_167.pt
2022-11-26 03:41:43.061934 resume step from  167
2022-11-26 03:42:21.567264 - Finished loading checkpoint, takes 38.476 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:42:48.531742] Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6079.125
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:42:52.542284] Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4010.303
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:42:55.532970] Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2990.702
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:42:58.527368] Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2994.255
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:43:02.506720] Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3979.387
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:43:05.446759] Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2939.933
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:43:08.416017] Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.230
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:43:11.392755] Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.706
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:43:14.338079] Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2945.407
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:43:17.295589] Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2957.412
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:43:20.257420] Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2961.770
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:43:23.211265] Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2953.843
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:43:26.226771] Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.424
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:43:29.203208] Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2976.423
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:43:32.223627] Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3020.390
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:43:35.219494] Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2995.817
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:43:38.170959] Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2951.441
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:43:41.180502] Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3009.511
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:43:44.122790] Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2942.266
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:47.086225] Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2963.401
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:50.024991] Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2938.781
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:52.992893] Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2967.834
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:55.962145] Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2969.263
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:43:58.889035] Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2926.858
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:01.892241] Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3003.137
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:04.858636] Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.374
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:07.892491] Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.816
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:44:10.868655] Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2976.174
2022-11-26 03:44:10.873214 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 9.736279964447021
Parent process ID: 30743 node: 172.31.21.45
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 2490411.62109375 0
End of simulation:  Mini-batch time (usec) = 4792107
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 53 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 15 0 627061.3403320312 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4648858
Min send: 10000000, max send 0
Min long send: 249047, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 281 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 22 0 458857.6354980469 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4729712
Min send: 10000000, max send 0
Min long send: 248773, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 51930, max bwd 65605
Min long fwd: 35944, max long fwd 44034; min long bwd 63837, max long bwd 73078
Time taken by simulation: 704 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1495 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 43 0 218394.13452148438 248719.58977934244
End of simulation:  Mini-batch time (usec) = 8069195
Min send: 10000000, max send 0
Min long send: 248801, max long send 275881
Min fwd: 11106, max fwd 44737; min bwd 22890, max bwd 36101
Min long fwd: 21913, max long fwd 30446; min long bwd 33725, max long bwd 41238
Time taken by simulation: 3190 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6720 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21642 microseconds

{1: 4.792107, 2: 4.648858, 3: 4.729712, 4: 5.904441, 6: 8.069195, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.648858
9 per stage
18 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 195
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 1.1675236225128174
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
15 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_195.pt
2022-11-26 03:44:41.370553 resume step from  195
2022-11-26 03:45:24.424030 - Finished loading checkpoint, takes 43.026 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:45:54.566884] Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6094.389
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:45:57.513216] Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2946.122
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:46:00.547021] Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3033.678
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:46:03.555105] Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3008.051
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:46:06.544031] Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.928
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:46:09.510858] Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2966.770
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:46:12.498989] Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2988.105
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:46:15.505560] Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3006.550
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:46:18.495039] Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2989.437
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:46:22.313253] Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3818.189
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:46:25.328691] Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3015.410
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:46:29.084294] Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3755.577
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:46:32.098815] Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3014.488
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:46:35.091321] Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2992.471
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:46:38.075669] Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2984.318
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:46:41.066547] Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 2990.866
2022-11-26 03:46:41.071228 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 8.90947961807251
Process done with return code 0
Parent process ID: 32027 node: 172.31.21.109
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2493369.62890625 0
End of simulation:  Mini-batch time (usec) = 5124879
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 60 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 332 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 817 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1449 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4790 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6688 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21655 microseconds

{1: 5.124879, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 211
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.19718265533447266
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_211.pt
2022-11-26 03:47:40.914665 resume step from  211
2022-11-26 03:48:25.029402 - Finished loading checkpoint, takes 44.087 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:48:40.228596] Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32), speed: 5270.525
2022-11-26 03:48:40.233921 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 9.775769233703613
Process done with return code 0
Parent process ID: 33109 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 1937331.4208984375 0
End of simulation:  Mini-batch time (usec) = 4900183
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 145221; min long bwd 182761, max long bwd 191040
Time taken by simulation: 62 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 19 0 599249.4506835938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 6142701
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58530, max long fwd 64048; min long bwd 92081, max long bwd 98662
Time taken by simulation: 341 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 812 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 43 0 281100.89111328125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 7406611
Min send: 10000000, max send 0
Min long send: 248907, max long send 274819
Min fwd: 20994, max fwd 60813; min bwd 39133, max bwd 50497
Min long fwd: 29201, max long fwd 38392; min long bwd 47764, max long bwd 56568
Time taken by simulation: 1969 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4787 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 19658022
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 3881, max fwd 41293; min bwd 16227, max bwd 29457
Min long fwd: 18183, max long fwd 26718; min long bwd 21739, max long bwd 33732
Time taken by simulation: 13726 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21532 microseconds

{1: 4.900183, 2: 6.142701, 3: 5.992289, 4: 7.406611, 6: 10.628515, 8: 19.658022, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 3 8
expected time is 5.992289
5 per stage
15 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 3
chunk_size: 8
data depth: 5
stage to rank map: 0,3,6,9,12;1,4,7,10,13;2,5,8,11,14;
World size is 15
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,3,6,9,12;1,4,7,10,13;2,5,8,11,14; --batch-size=204 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 212
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.1744251251220703
SHARED WEIGHTS ARE
[(0, 2)]
this rank  5 is part of pipeline replica  1
26 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_212.pt
2022-11-26 03:49:40.551002 resume step from  212
2022-11-26 03:50:10.222614 - Finished loading checkpoint, takes 29.651 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:50:26.370188] Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 7490.300
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:50:30.771884] Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4401.464
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:50:35.180066] Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4408.085
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:50:38.582727] Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3402.570
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:50:42.956541] Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4373.874
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:50:46.307337] Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3350.677
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:50:49.770641] Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3463.262
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:50:53.311728] Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3541.042
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:50:56.749828] Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3438.076
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:51:00.208882] Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3459.025
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:51:03.688913] Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3479.999
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:51:07.136069] Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3447.125
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:51:10.574660] Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3438.569
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:51:13.984019] Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3409.339
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:51:17.362557] Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3378.508
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:51:20.705048] Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3342.482
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:51:24.132680] Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3427.616
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:51:27.577192] Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3444.438
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:51:30.990956] Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3413.771
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:34.379389] Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3388.368
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:37.778574] Finished iteration 232, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3399.155
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:41.161804] Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3383.214
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:44.572245] Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3410.443
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:48.024319] Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3451.995
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:51.407893] Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3383.543
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:54.864964] Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3457.039
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:51:58.295628] Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3430.673
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:01.709756] Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3414.073
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:05.103409] Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3393.632
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:08.531290] Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3427.840
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:52:11.959902] Finished iteration 242, CKPT_AND_STOP: False, flag: tensor([5], dtype=torch.int32), speed: 3428.596
2022-11-26 03:52:11.963278 Begin to save checkpont to s3://spot-checkpoints/bert and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 5 signal handler called with signal 10
Opt ckpt time 4.793114423751831
Process done with return code 0
Parent process ID: 34089 node: 172.31.21.254
24 cutpoints
Stages 1
Micro-bs 8 Max mem: 14622140211.200005
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 2595254.8828125 0
End of simulation:  Mini-batch time (usec) = 5226764
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 140628, max long fwd 144458; min long bwd 182761, max long bwd 191040
Time taken by simulation: 61 microseconds

Stages 2
Micro-bs 8 Max mem: 7660942131.200001
Predicted microbatch size for 2: 8
comm size 4194304
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 16 0 630155.517578125 248719.58977934244
End of simulation:  Mini-batch time (usec) = 4812054
Min send: 10000000, max send 0
Min long send: 249051, max long send 271185
Min fwd: 78473, max fwd 86627; min bwd 87174, max bwd 96382
Min long fwd: 58631, max long fwd 64048; min long bwd 92410, max long bwd 98662
Time taken by simulation: 299 microseconds

Stages 3
Micro-bs 8 Max mem: 5497876480.0
Predicted microbatch size for 3: 8
comm size 4194304
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 26 0 419870.6359863281 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5992289
Min send: 10000000, max send 0
Min long send: 248719, max long send 273213
Min fwd: 34027, max fwd 67951; min bwd 53788, max bwd 65605
Min long fwd: 36841, max long fwd 44034; min long bwd 64590, max long bwd 71935
Time taken by simulation: 815 microseconds

Stages 4
Micro-bs 8 Max mem: 4406266675.2
Predicted microbatch size for 4: 8
comm size 4194304
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 32 0 320241.69921875 248719.58977934244
End of simulation:  Mini-batch time (usec) = 5904441
Min send: 10000000, max send 0
Min long send: 248907, max long send 278723
Min fwd: 20622, max fwd 60312; min bwd 35656, max bwd 52205
Min long fwd: 30741, max long fwd 36945; min long bwd 47928, max long bwd 56568
Time taken by simulation: 1454 microseconds

Stages 6
Micro-bs 8 Max mem: 3314656870.4
Predicted microbatch size for 6: 8
comm size 4194304
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 64 0 159503.44848632812 248719.58977934244
End of simulation:  Mini-batch time (usec) = 10628515
Min send: 10000000, max send 0
Min long send: 248794, max long send 278723
Min fwd: 9813, max fwd 44004; min bwd 22822, max bwd 35461
Min long fwd: 21408, max long fwd 29794; min long bwd 32401, max long bwd 41612
Time taken by simulation: 4778 microseconds

Stages 8
Micro-bs 8 Max mem: 2758774988.8
Predicted microbatch size for 8: 8
comm size 4194304
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 144488.12866210938 248719.58977934244
End of simulation:  Mini-batch time (usec) = 11599775
Min send: 10000000, max send 0
Min long send: 248735, max long send 283577
Min fwd: 5572, max fwd 41293; min bwd 15648, max bwd 28947
Min long fwd: 17804, max long fwd 28406; min long bwd 25411, max long bwd 32670
Time taken by simulation: 6686 microseconds

Stages 12
Micro-bs 8 Max mem: 2202893107.2
Predicted microbatch size for 12: 8
comm size 4194304
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 248719.58977934244
End of simulation:  Mini-batch time (usec) = 21673100
Min send: 10000000, max send 0
Min long send: 248719, max long send 288457
Min fwd: 673, max fwd 27541; min bwd 8696, max bwd 22046
Min long fwd: 7851, max long fwd 17781; min long bwd 16226, max long bwd 27833
Time taken by simulation: 21534 microseconds

{1: 5.226764, 2: 4.812054, 3: 5.992289, 4: 5.904441, 6: 10.628515, 8: 11.599775, 12: 21.6731}
{1: 8, 2: 8, 3: 8, 4: 8, 6: 8, 8: 8, 12: 8}
best config is: 2 8
expected time is 4.812054
8 per stage
16 servers!
Config:
ranks: range(5, 6)
train batch size: 1024
partitions: 2
chunk_size: 8
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=5 --chunk_size=8 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 243
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
dry run time 0.2056269645690918
SHARED WEIGHTS ARE
[(0, 1)]
this rank  5 is part of pipeline replica  2
16 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_243.pt
2022-11-26 03:52:42.189487 resume step from  243
2022-11-26 03:53:21.269416 - Finished loading checkpoint, takes 39.051 secs
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-26 03:53:31.597411] Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5345.911
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-26 03:53:34.798842] Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3201.140
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-26 03:53:38.096020] Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3297.146
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-26 03:53:42.348932] Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4252.954
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-26 03:53:46.585255] Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4236.300
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-26 03:53:49.754902] Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3169.541
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-26 03:53:52.962284] Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3207.378
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-26 03:53:56.210190] Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3247.839
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-26 03:53:59.528003] Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3317.779
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-26 03:54:02.755994] Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3227.953
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-26 03:54:05.983598] Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3227.584
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-26 03:54:09.191254] Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3207.628
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-26 03:54:12.393724] Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3202.456
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-26 03:54:15.621779] Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3228.037
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-26 03:54:18.839295] Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3217.506
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-26 03:54:22.060623] Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3221.281
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-26 03:54:25.247357] Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3186.669
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-26 03:54:28.389212] Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3141.837
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-26 03:54:31.608956] Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3219.709
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:34.915035] Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3306.059
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:38.075592] Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3160.543
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:41.258910] Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3183.282
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:44.436715] Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3177.807
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:47.571308] Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3134.544
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:50.787889] Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3216.538
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:53.963724] Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3175.805
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:54:57.150160] Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3186.520
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:00.359329] Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3209.025
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:03.571983] Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3212.647
5 Overflow !!
5 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-26 03:55:06.777153] Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3205.129
