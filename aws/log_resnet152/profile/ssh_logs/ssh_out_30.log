Parent process ID: 5704 node: 172.31.25.155
16 per stage
32 servers!
Config:
ranks: range(30, 31)
train batch size: 2048
partitions: 2
chunk_size: 32
data depth: 16
stage to rank map: 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u main.py --rank=30 --chunk_size=32 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30;1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31; --batch-size=128 data -a resnet152 --varuna --lr 0.001 --epochs 100 --resume s3://spot-checkpoints/resnet-profile --profiling
=> creating model 'resnet152'
Files already downloaded and verified
Num cutpoints is 31
Stages to profile: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
factors [32, 16, 8, 4, 2, 1]
all reduce sizes [9536, 84544, 225344, 1444928, 13015104, 60192808]
STAGE 30
USED MODULES
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
30 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
1 7710.207939147949 6096.67181968689 167.54133502642313 171842662.4 200704
2 3213.6533260345454 10027.999877929688 207.81439840793607 190129664.0 401408
3 3215.0185902913413 8486.368179321289 249.30559694767 190819430.4 602112
4 3443.3706601460776 5112.936019897461 302.4959981441498 191230668.8 802816
5 3121.151924133301 6017.471949259441 356.82560205459595 192526643.2 1003520
6 3624.7039437294006 9122.143745422363 397.2399979829788 195048960.0 1204224
7 3327.7440071105957 4251.471996307373 407.82400220632553 195944345.6 1404928
8 4253.6959648132315 6309.1840744018555 521.8239948153496 198796800.0 1605632
9 3642.879962921143 4703.55208714803 523.6693223317465 198656716.8 1806336
10 3237.546682357788 5486.015796661377 514.7093335787456 201313382.4 2007040
11 3910.3146394093837 4866.631984710693 559.5519840717316 202028953.6 2207744
12 3997.440040111542 7442.70396232605 597.9327917098999 203805388.8 2408448
13 4061.1840486526485 6094.95464960734 575.791984796524 205050572.8 2609152
14 6492.159843444824 6509.5678965250645 702.1519988775253 208538726.4 2809856
15 3632.6399445533752 4939.712047576904 606.3519865274429 209312051.2 3010560
16 4079.957405726115 5937.991976737975 707.2559893131255 211858124.8 3211264
17 4507.13586807251 9346.07982635498 616.4693435033162 213322854.4 3411968
18 4122.623920440674 5647.317250569661 768.784001469612 214726963.2 3612672
19 3499.690612157186 4647.6694742838545 654.3253262837727 215250022.4 3813376
20 3950.080037117004 5617.226759592693 836.7893298467001 217825177.6 4014080
21 8855.55171966553 7844.480037689209 816.2479847669601 219773440.0 4214784
22 3924.6505896250405 5295.221328735352 842.0287847518921 222068224.0 4415488
23 4639.402548472087 7786.880016326904 879.1680037975311 222738944.0 4616192
24 3965.6107425689693 6579.368114471436 1099.071979522705 225325772.8 4816896
25 4677.119970321655 6744.951963424683 962.2720181941986 228577792.0 5017600
26 4170.069376627604 6065.240025520325 1044.2399829626083 228992102.4 5218304
27 4476.245244344075 6749.128103256226 1039.6160036325457 228328550.4 5419008
28 4564.480066299439 10736.736297607422 1206.9920003414156 230555545.6 5619712
29 4277.930736541748 6502.816041310629 1467.9360389709473 232068403.2 5820416
30 6654.975891113281 5515.455881754558 1148.0000019073489 234007654.4 6021120
31 4458.154837290445 7239.168167114258 1143.559992313385 234844057.6 6221824
32 4685.312032699585 5501.408100128173 1238.2879853248596 237627289.6 6422528
33 8815.6156539917 5664.079904556274 1194.4800019264221 240210022.4 6623232
34 4118.016064167023 8197.855949401855 1266.6639983654022 243560345.6 6823936
35 4108.799934387207 6161.984125773112 1422.688007354736 246081433.6 7024640
36 3973.8026460011806 5520.12002468109 1392.416000366211 248435814.4 7225344
37 4188.680052757263 6009.9040269851685 1360.2159917354581 250919833.6 7426048
38 4247.893253962198 5752.319971720377 1272.0959981282551 253669683.2 7626752
39 4616.192102432251 7423.664093017578 1213.3439779281616 255345766.4 7827456
40 9958.912372589111 6637.02392578125 2295.232057571411 259422515.2 8028160
41 4562.261422475179 5461.687922477721 1604.1173140207927 261555097.6 8228864
42 8625.151634216309 5796.021302541098 1443.4133370717368 265540300.8 8429568
43 4571.648001670838 6376.12803777059 3622.2879886627193 268149452.8 8630272
44 3900.0746409098306 5544.490655263264 1314.41068649292 273248972.8 8830976
45 4150.271972020467 5655.192017555237 1509.3226432800293 275976908.8 9031680
46 4061.1840089162183 5190.47212600708 1641.0346825917563 276933734.4 9232384
47 4062.549352645874 6216.351985931396 1752.8853416442873 281513062.4 9433088
pre-alr mem 57250304
All reduce times
32 [0.0, 137664.09301757812, 143959.7625732422]
16 [0.0, 6535.486698150635, 5174.301624298096]
8 [0.0, 14485.084533691406, 15014.791488647461]
4 [0.0, 25247.96485900879, 25975.866317749023]
2 [0.0, 80494.86541748047, 85857.61260986328]
1 [0.0, 355241.7907714844, 385381.5002441406]
Process done with return code 0
