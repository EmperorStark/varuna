Parent process ID: 10643 node: 172.31.28.108
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 5880609.86328125 0
End of simulation:  Mini-batch time (usec) = 10058641
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164303, max long fwd 168896; min long bwd 294106, max long bwd 302385
Time taken by simulation: 77 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 18 0 2725911.1328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8250798
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82380, max long fwd 89657; min long bwd 149642, max long bwd 157738
Time taken by simulation: 295 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 1527054.19921875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8830859
Min send: 10000000, max send 0
Min long send: 38055, max long send 61744
Min fwd: 45773, max fwd 61214; min bwd 93190, max bwd 104086
Min long fwd: 56519, max long fwd 64913; min long bwd 96534, max long bwd 106927
Time taken by simulation: 909 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1050667.96875 80871.45884831746
End of simulation:  Mini-batch time (usec) = 6512804
Min send: 10000000, max send 0
Min long send: 80871, max long send 103337
Min fwd: 47099, max fwd 59401; min bwd 106731, max bwd 128252
Min long fwd: 54031, max long fwd 62603; min long bwd 118588, max long bwd 125970
Time taken by simulation: 975 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 16 0 548680.6640625 159635.3450129109
End of simulation:  Mini-batch time (usec) = 7062870
Min send: 10000000, max send 0
Min long send: 159689, max long send 182101
Min fwd: 52659, max fwd 63293; min bwd 123973, max bwd 139940
Min long fwd: 73341, max long fwd 81486; min long bwd 146196, max long bwd 154690
Time taken by simulation: 1177 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 0 230166.85465330718
End of simulation:  Mini-batch time (usec) = 10536264
Min send: 10000000, max send 0
Min long send: 230303, max long send 257328
Min fwd: 54967, max fwd 67591; min bwd 130849, max bwd 143444
Min long fwd: 78331, max long fwd 89630; min long bwd 156920, max long bwd 164176
Time taken by simulation: 2196 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1913 microseconds

can't have 16 stages!
can't have 24 stages!
{1: 10.058641, 2: 8.250798, 3: 8.830859, 4: 6.512804, 6: 7.06287, 8: 10.536264, 12: 15.020502}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10}
best config is: 4 2
expected time is 6.512804
3 per stage
12 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 3
stage to rank map: 0,4,8;1,5,9;2,6,10;3,7,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8;1,5,9;2,6,10;3,7,11; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 25
using world size: 12 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 25
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8;1,5,9;2,6,10;3,7,11;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 12
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
