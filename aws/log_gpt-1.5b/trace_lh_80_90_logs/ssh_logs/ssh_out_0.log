Parent process ID: 14759 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 5984009.765625 0
End of simulation:  Mini-batch time (usec) = 9230845
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164303, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 57 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 14 0 2816083.251953125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7191944
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82709, max long fwd 89657; min long bwd 149926, max long bwd 157738
Time taken by simulation: 239 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 21 0 1916562.744140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6803052
Min send: 10000000, max send 0
Min long send: 38109, max long send 62549
Min fwd: 45773, max fwd 59456; min bwd 92874, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 96930, max long bwd 106927
Time taken by simulation: 607 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 1199719.8486328125 80871.45884831746
End of simulation:  Mini-batch time (usec) = 5670469
Min send: 10000000, max send 0
Min long send: 80925, max long send 104560
Min fwd: 47855, max fwd 59401; min bwd 106211, max bwd 126925
Min long fwd: 52606, max long fwd 62603; min long bwd 118442, max long bwd 125606
Time taken by simulation: 700 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 11 0 770127.8076171875 159635.3450129109
End of simulation:  Mini-batch time (usec) = 6157105
Min send: 10000000, max send 0
Min long send: 159689, max long send 184340
Min fwd: 52659, max fwd 63832; min bwd 123243, max bwd 140240
Min long fwd: 72998, max long fwd 78841; min long bwd 148211, max long bwd 154022
Time taken by simulation: 755 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 11 0 442992.24853515625 230166.85465330718
End of simulation:  Mini-batch time (usec) = 8313990
Min send: 10000000, max send 0
Min long send: 230166, max long send 255370
Min fwd: 54051, max fwd 66033; min bwd 132265, max bwd 140978
Min long fwd: 81808, max long fwd 89347; min long bwd 157650, max long bwd 164467
Time taken by simulation: 1045 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1930 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1919 microseconds

can't have 24 stages!
{1: 9.230845, 2: 7.191944, 3: 6.803052, 4: 5.670469, 6: 6.157105, 8: 8.31399, 12: 15.020502, 16: 20.148929}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13}
best config is: 4 2
expected time is 5.670469
4 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 4
stage to rank map: 0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15; --batch-size=32 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... None
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2400000
    validation: 2560
    test:       1280
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
device is' 0
dry run time 0.5775749683380127
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
16 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
WARNING: could not find the metadata file s3://spot-checkpoints/gpt/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
 > finished loading checkpoint in 0.210 seconds
setting training data start iteration to 0
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 17177.04 | train/valid/test data iterators: 173.10
training ...
START iteration 0, CKPT_AND_STOP: False
Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:04.737487] iteration        1/   18750 | elapsed time per iteration (ms): 8726.2 | learning rate: 8.000E-07 | lm loss: 1.114267E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 1 iterations memory (MB) | allocated: 6072.32275390625 | max allocated: 9552.16162109375 | reserved: 10576.0 | max reserved: 10576.0
time (ms) | optimizer: 28.10 | batch generator: 5.89
START iteration 1, CKPT_AND_STOP: False
Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:10.074136] iteration        2/   18750 | elapsed time per iteration (ms): 5336.7 | learning rate: 1.600E-06 | lm loss: 1.113987E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.80 | batch generator: 2.89
START iteration 2, CKPT_AND_STOP: False
Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:15.430190] iteration        3/   18750 | elapsed time per iteration (ms): 5356.0 | learning rate: 2.400E-06 | lm loss: 1.112956E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 2.55
START iteration 3, CKPT_AND_STOP: False
Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:20.747736] iteration        4/   18750 | elapsed time per iteration (ms): 5317.5 | learning rate: 3.200E-06 | lm loss: 1.111723E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.66
START iteration 4, CKPT_AND_STOP: False
Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:26.117441] iteration        5/   18750 | elapsed time per iteration (ms): 5369.7 | learning rate: 4.000E-06 | lm loss: 1.109938E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.70
START iteration 5, CKPT_AND_STOP: False
Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:31.431726] iteration        6/   18750 | elapsed time per iteration (ms): 5314.3 | learning rate: 4.800E-06 | lm loss: 1.109044E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.57
START iteration 6, CKPT_AND_STOP: False
Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:36.732034] iteration        7/   18750 | elapsed time per iteration (ms): 5300.3 | learning rate: 5.600E-06 | lm loss: 1.107985E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.89
START iteration 7, CKPT_AND_STOP: False
Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:42.069475] iteration        8/   18750 | elapsed time per iteration (ms): 5337.4 | learning rate: 6.400E-06 | lm loss: 1.106226E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.59
START iteration 8, CKPT_AND_STOP: False
Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:47.333411] iteration        9/   18750 | elapsed time per iteration (ms): 5263.9 | learning rate: 7.200E-06 | lm loss: 1.105281E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 3.75
START iteration 9, CKPT_AND_STOP: False
Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:52.665053] iteration       10/   18750 | elapsed time per iteration (ms): 5331.6 | learning rate: 8.000E-06 | lm loss: 1.104105E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.45
START iteration 10, CKPT_AND_STOP: False
Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:45:57.990197] iteration       11/   18750 | elapsed time per iteration (ms): 5325.1 | learning rate: 8.800E-06 | lm loss: 1.103559E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.49
START iteration 11, CKPT_AND_STOP: False
Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:03.309175] iteration       12/   18750 | elapsed time per iteration (ms): 5319.0 | learning rate: 9.600E-06 | lm loss: 1.102836E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.64
START iteration 12, CKPT_AND_STOP: False
Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:08.647237] iteration       13/   18750 | elapsed time per iteration (ms): 5338.0 | learning rate: 1.040E-05 | lm loss: 1.102324E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.66
START iteration 13, CKPT_AND_STOP: False
Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:14.085238] iteration       14/   18750 | elapsed time per iteration (ms): 5438.0 | learning rate: 1.120E-05 | lm loss: 1.101298E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.80
START iteration 14, CKPT_AND_STOP: False
Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:19.415859] iteration       15/   18750 | elapsed time per iteration (ms): 5330.6 | learning rate: 1.200E-05 | lm loss: 1.101175E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.55
START iteration 15, CKPT_AND_STOP: False
Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:24.868796] iteration       16/   18750 | elapsed time per iteration (ms): 5452.9 | learning rate: 1.280E-05 | lm loss: 1.099901E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.82
START iteration 16, CKPT_AND_STOP: False
Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:30.307704] iteration       17/   18750 | elapsed time per iteration (ms): 5438.9 | learning rate: 1.360E-05 | lm loss: 1.098725E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.66
START iteration 17, CKPT_AND_STOP: False
Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:35.617253] iteration       18/   18750 | elapsed time per iteration (ms): 5309.5 | learning rate: 1.440E-05 | lm loss: 1.099634E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.82
START iteration 18, CKPT_AND_STOP: False
Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:40.940087] iteration       19/   18750 | elapsed time per iteration (ms): 5322.8 | learning rate: 1.520E-05 | lm loss: 1.098191E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.85
START iteration 19, CKPT_AND_STOP: False
Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:46.322825] iteration       20/   18750 | elapsed time per iteration (ms): 5382.7 | learning rate: 1.600E-05 | lm loss: 1.096201E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.56
START iteration 20, CKPT_AND_STOP: False
Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:51.718085] iteration       21/   18750 | elapsed time per iteration (ms): 5395.2 | learning rate: 1.680E-05 | lm loss: 1.095749E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.51
START iteration 21, CKPT_AND_STOP: False
Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:46:57.075605] iteration       22/   18750 | elapsed time per iteration (ms): 5357.5 | learning rate: 1.760E-05 | lm loss: 1.094839E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.74 | batch generator: 1.46
START iteration 22, CKPT_AND_STOP: False
Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:02.555028] iteration       23/   18750 | elapsed time per iteration (ms): 5479.4 | learning rate: 1.840E-05 | lm loss: 1.093868E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.55
START iteration 23, CKPT_AND_STOP: False
Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:07.826565] iteration       24/   18750 | elapsed time per iteration (ms): 5271.5 | learning rate: 1.920E-05 | lm loss: 1.093295E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.49
START iteration 24, CKPT_AND_STOP: False
Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:13.159791] iteration       25/   18750 | elapsed time per iteration (ms): 5333.2 | learning rate: 2.000E-05 | lm loss: 1.092191E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.76 | batch generator: 1.70
START iteration 25, CKPT_AND_STOP: False
Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:18.486297] iteration       26/   18750 | elapsed time per iteration (ms): 5326.5 | learning rate: 2.080E-05 | lm loss: 1.090971E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.96
START iteration 26, CKPT_AND_STOP: False
Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:23.779373] iteration       27/   18750 | elapsed time per iteration (ms): 5293.1 | learning rate: 2.160E-05 | lm loss: 1.089332E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.95
START iteration 27, CKPT_AND_STOP: False
Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:29.120328] iteration       28/   18750 | elapsed time per iteration (ms): 5340.9 | learning rate: 2.240E-05 | lm loss: 1.088679E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.60
START iteration 28, CKPT_AND_STOP: False
Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:34.405607] iteration       29/   18750 | elapsed time per iteration (ms): 5285.3 | learning rate: 2.320E-05 | lm loss: 1.088251E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.56
START iteration 29, CKPT_AND_STOP: False
Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:39.732215] iteration       30/   18750 | elapsed time per iteration (ms): 5326.6 | learning rate: 2.400E-05 | lm loss: 1.087012E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.67
START iteration 30, CKPT_AND_STOP: False
Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:45.044828] iteration       31/   18750 | elapsed time per iteration (ms): 5312.6 | learning rate: 2.480E-05 | lm loss: 1.085855E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.74 | batch generator: 1.62
START iteration 31, CKPT_AND_STOP: False
Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:50.492836] iteration       32/   18750 | elapsed time per iteration (ms): 5448.0 | learning rate: 2.560E-05 | lm loss: 1.084662E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.76 | batch generator: 1.53
START iteration 32, CKPT_AND_STOP: False
Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:47:55.801028] iteration       33/   18750 | elapsed time per iteration (ms): 5308.2 | learning rate: 2.640E-05 | lm loss: 1.083518E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.74
START iteration 33, CKPT_AND_STOP: False
Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:48:01.091975] iteration       34/   18750 | elapsed time per iteration (ms): 5290.9 | learning rate: 2.720E-05 | lm loss: 1.082205E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.62
START iteration 34, CKPT_AND_STOP: False
Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:48:06.553823] iteration       35/   18750 | elapsed time per iteration (ms): 5461.8 | learning rate: 2.800E-05 | lm loss: 1.081104E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.74 | batch generator: 1.60
START iteration 35, CKPT_AND_STOP: False
Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:48:11.983778] iteration       36/   18750 | elapsed time per iteration (ms): 5429.9 | learning rate: 2.880E-05 | lm loss: 1.080179E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.87
START iteration 36, CKPT_AND_STOP: False
Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:48:17.244263] iteration       37/   18750 | elapsed time per iteration (ms): 5260.5 | learning rate: 2.960E-05 | lm loss: 1.079589E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.63
START iteration 37, CKPT_AND_STOP: False
Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:48:22.558570] iteration       38/   18750 | elapsed time per iteration (ms): 5314.3 | learning rate: 3.040E-05 | lm loss: 1.078142E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.74 | batch generator: 1.65
START iteration 38, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 39, CKPT_AND_STOP: True, flag: tensor([7], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      39 to s3://spot-checkpoints/gpt/iter_0000039/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000039/mp_rank_00/model_optim_rng.pt
Opt ckpt time 30.27484655380249
Process done with return code 0
Parent process ID: 15304 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 5940361.81640625 0
End of simulation:  Mini-batch time (usec) = 8725127
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164547, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 52 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 2836733.88671875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6512857
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82709, max long fwd 89657; min long bwd 149926, max long bwd 157738
Time taken by simulation: 211 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 21 0 1916562.744140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6803052
Min send: 10000000, max send 0
Min long send: 38109, max long send 62549
Min fwd: 45773, max fwd 59456; min bwd 92874, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 96930, max long bwd 106927
Time taken by simulation: 609 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 13 0 1338646.484375 80871.45884831746
End of simulation:  Mini-batch time (usec) = 5132496
Min send: 10000000, max send 0
Min long send: 80953, max long send 103337
Min fwd: 48269, max fwd 59401; min bwd 107938, max bwd 126574
Min long fwd: 53770, max long fwd 60383; min long bwd 118672, max long bwd 125606
Time taken by simulation: 571 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 11 0 770127.8076171875 159635.3450129109
End of simulation:  Mini-batch time (usec) = 6157105
Min send: 10000000, max send 0
Min long send: 159689, max long send 184340
Min fwd: 52659, max fwd 63832; min bwd 123243, max bwd 140240
Min long fwd: 72998, max long fwd 78841; min long bwd 148211, max long bwd 154022
Time taken by simulation: 757 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 11 0 442992.24853515625 230166.85465330718
End of simulation:  Mini-batch time (usec) = 8313990
Min send: 10000000, max send 0
Min long send: 230166, max long send 255370
Min fwd: 54051, max fwd 66033; min bwd 132265, max bwd 140978
Min long fwd: 81808, max long fwd 89347; min long bwd 157650, max long bwd 164467
Time taken by simulation: 1038 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1947 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1951 microseconds

can't have 24 stages!
{1: 8.725127, 2: 6.512857, 3: 6.803052, 4: 5.132496, 6: 6.157105, 8: 8.31399, 12: 15.020502, 16: 20.148929}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13}
best config is: 4 2
expected time is 5.132496
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 39
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 39
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
device is' 0
dry run time 1.4022815227508545
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
13 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000039/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000039/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 77.740 seconds
setting training data start iteration to 39
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 95560.02 | train/valid/test data iterators: 280.28
training ...
START iteration 39, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 40, CKPT_AND_STOP: True, flag: tensor([20], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      40 to s3://spot-checkpoints/gpt/iter_0000040/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000040/mp_rank_00/model_optim_rng.pt
Opt ckpt time 21.33778429031372
Process done with return code 0
Parent process ID: 16501 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 6125747.0703125 0
End of simulation:  Mini-batch time (usec) = 8451860
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164718, max long fwd 168133; min long bwd 296265, max long bwd 302385
Time taken by simulation: 51 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 11 0 2767979.736328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6203240
Min send: 10000000, max send 0
Min long send: 38387, max long send 62760
Min fwd: 76538, max fwd 83279; min bwd 142710, max bwd 147733
Min long fwd: 81915, max long fwd 89657; min long bwd 148530, max long bwd 156110
Time taken by simulation: 203 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 18 0 1846814.0869140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5987018
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 59834; min bwd 93871, max bwd 104244
Min long fwd: 57033, max long fwd 64913; min long bwd 98140, max long bwd 103204
Time taken by simulation: 529 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 13 0 1338646.484375 80871.45884831746
End of simulation:  Mini-batch time (usec) = 5132496
Min send: 10000000, max send 0
Min long send: 80953, max long send 103337
Min fwd: 48269, max fwd 59401; min bwd 107938, max bwd 126574
Min long fwd: 53770, max long fwd 60383; min long bwd 118672, max long bwd 125606
Time taken by simulation: 573 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 11 0 770127.8076171875 159635.3450129109
End of simulation:  Mini-batch time (usec) = 6157105
Min send: 10000000, max send 0
Min long send: 159689, max long send 184340
Min fwd: 52659, max fwd 63832; min bwd 123243, max bwd 140240
Min long fwd: 72998, max long fwd 78841; min long bwd 148211, max long bwd 154022
Time taken by simulation: 766 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 11 0 442992.24853515625 230166.85465330718
End of simulation:  Mini-batch time (usec) = 8313990
Min send: 10000000, max send 0
Min long send: 230166, max long send 255370
Min fwd: 54051, max fwd 66033; min bwd 132265, max bwd 140978
Min long fwd: 81808, max long fwd 89347; min long bwd 157650, max long bwd 164467
Time taken by simulation: 1038 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1900 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1922 microseconds

can't have 24 stages!
{1: 8.45186, 2: 6.20324, 3: 5.987018, 4: 5.132496, 6: 6.157105, 8: 8.31399, 12: 15.020502, 16: 20.148929}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13}
best config is: 4 2
expected time is 5.132496
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 40
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 40
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
device is' 0
dry run time 1.24013090133667
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
13 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000040/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000040/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 73.993 seconds
setting training data start iteration to 40
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 91651.12 | train/valid/test data iterators: 288.63
training ...
START iteration 40, CKPT_AND_STOP: False
Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:55:03.737765] iteration       41/   18750 | elapsed time per iteration (ms): 8100.5 | learning rate: 3.280E-05 | lm loss: 1.075365E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 41 iterations memory (MB) | allocated: 6137.60009765625 | max allocated: 13061.97802734375 | reserved: 14082.0 | max reserved: 14082.0
time (ms) | optimizer: 24.01 | batch generator: 5.06
START iteration 41, CKPT_AND_STOP: False
Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:55:08.453438] iteration       42/   18750 | elapsed time per iteration (ms): 4715.7 | learning rate: 3.360E-05 | lm loss: 1.074452E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.60
START iteration 42, CKPT_AND_STOP: False
Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:55:13.127335] iteration       43/   18750 | elapsed time per iteration (ms): 4673.9 | learning rate: 3.440E-05 | lm loss: 1.074538E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.82
START iteration 43, CKPT_AND_STOP: False
Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:55:17.799487] iteration       44/   18750 | elapsed time per iteration (ms): 4672.1 | learning rate: 3.520E-05 | lm loss: 1.073085E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.51
START iteration 44, CKPT_AND_STOP: False
Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:55:22.529716] iteration       45/   18750 | elapsed time per iteration (ms): 4730.2 | learning rate: 3.600E-05 | lm loss: 1.072456E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.42
START iteration 45, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 46, CKPT_AND_STOP: True, flag: tensor([6], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      46 to s3://spot-checkpoints/gpt/iter_0000046/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000046/mp_rank_00/model_optim_rng.pt
Opt ckpt time 22.357218980789185
Process done with return code 0
Parent process ID: 17759 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 5 0 5671273.4375 0
End of simulation:  Mini-batch time (usec) = 7997386
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164718, max long fwd 168133; min long bwd 296265, max long bwd 302385
Time taken by simulation: 49 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 10 0 2977483.88671875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6172069
Min send: 10000000, max send 0
Min long send: 38387, max long send 59301
Min fwd: 77133, max fwd 85384; min bwd 141071, max bwd 148651
Min long fwd: 83859, max long fwd 89657; min long bwd 148892, max long bwd 154838
Time taken by simulation: 187 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 2198285.64453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5986016
Min send: 10000000, max send 0
Min long send: 38293, max long send 60521
Min fwd: 45773, max fwd 59456; min bwd 93556, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 97741, max long bwd 104518
Time taken by simulation: 472 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 11 0 1478731.4453125 80871.45884831746
End of simulation:  Mini-batch time (usec) = 4902207
Min send: 10000000, max send 0
Min long send: 81109, max long send 105576
Min fwd: 49725, max fwd 58709; min bwd 110370, max bwd 129008
Min long fwd: 53770, max long fwd 59495; min long bwd 117692, max long bwd 125606
Time taken by simulation: 517 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 8 0 876895.5078125 159635.3450129109
End of simulation:  Mini-batch time (usec) = 5601136
Min send: 10000000, max send 0
Min long send: 159772, max long send 184340
Min fwd: 53241, max fwd 62601; min bwd 123697, max bwd 141328
Min long fwd: 72998, max long fwd 79077; min long bwd 148883, max long bwd 151161
Time taken by simulation: 564 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 7 0 610855.46875 230166.85465330718
End of simulation:  Mini-batch time (usec) = 7505355
Min send: 10000000, max send 0
Min long send: 230248, max long send 254871
Min fwd: 56011, max fwd 66812; min bwd 129842, max bwd 141350
Min long fwd: 83341, max long fwd 88072; min long bwd 159951, max long bwd 163059
Time taken by simulation: 658 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 7 0 343797.36328125 361377.8571928701
End of simulation:  Mini-batch time (usec) = 13548777
Min send: 10000000, max send 0
Min long send: 361459, max long send 385871
Min fwd: 61802, max fwd 72928; min bwd 141491, max bwd 152040
Min long fwd: 108389, max long fwd 112365; min long bwd 179662, max long bwd 186130
Time taken by simulation: 1029 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1934 microseconds

Stages 24
13 11546353561.599998
19 16037201817.599998
16 13791433625.599998
17 14539214745.599998
18 15288667033.599998
Predicted microbatch size for 24: 17
comm size 27852800
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 8 0 0 548143.2004641461
End of simulation:  Mini-batch time (usec) = 33444830
Min send: 10000000, max send 0
Min long send: 548262, max long send 578147
Min fwd: 49763, max fwd 61192; min bwd 113823, max bwd 127856
Min long fwd: 124104, max long fwd 128023; min long bwd 187948, max long bwd 195033
Time taken by simulation: 2269 microseconds

{1: 7.997386, 2: 6.172069, 3: 5.986016, 4: 4.902207, 6: 5.601136, 8: 7.505355, 12: 13.548777, 16: 20.148929, 24: 33.44483}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13, 24: 17}
best config is: 4 2
expected time is 4.902207
6 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 6
stage to rank map: 0,4,8,12,16,20;1,5,9,13,17,21;2,6,10,14,18,22;3,7,11,15,19,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12,16,20;1,5,9,13,17,21;2,6,10,14,18,22;3,7,11,15,19,23; --batch-size=21 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 46
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 21
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 46
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16,20;1,5,9,13,17,21;2,6,10,14,18,22;3,7,11,15,19,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
device is' 0
dry run time 1.3908448219299316
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
11 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000046/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000046/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 73.639 seconds
setting training data start iteration to 46
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 91522.37 | train/valid/test data iterators: 569.22
training ...
START iteration 46, CKPT_AND_STOP: False
Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:23.879739] iteration       47/   18750 | elapsed time per iteration (ms): 44012.9 | learning rate: 3.760E-05 | lm loss: 1.071502E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 47 iterations memory (MB) | allocated: 6137.52197265625 | max allocated: 13060.94677734375 | reserved: 14082.0 | max reserved: 14082.0
time (ms) | optimizer: 23.82 | batch generator: 13.66
START iteration 47, CKPT_AND_STOP: False
Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:29.383310] iteration       48/   18750 | elapsed time per iteration (ms): 5503.6 | learning rate: 3.840E-05 | lm loss: 1.070366E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.72
START iteration 48, CKPT_AND_STOP: False
Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:34.929373] iteration       49/   18750 | elapsed time per iteration (ms): 5546.0 | learning rate: 3.920E-05 | lm loss: 1.069825E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.63
START iteration 49, CKPT_AND_STOP: False
Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:40.450662] iteration       50/   18750 | elapsed time per iteration (ms): 5521.3 | learning rate: 4.000E-05 | lm loss: 1.069410E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.56
START iteration 50, CKPT_AND_STOP: False
Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:44.877964] iteration       51/   18750 | elapsed time per iteration (ms): 4427.3 | learning rate: 4.080E-05 | lm loss: 1.069101E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.50
START iteration 51, CKPT_AND_STOP: False
Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:49.280875] iteration       52/   18750 | elapsed time per iteration (ms): 4402.9 | learning rate: 4.160E-05 | lm loss: 1.067913E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.84
START iteration 52, CKPT_AND_STOP: False
Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:53.785372] iteration       53/   18750 | elapsed time per iteration (ms): 4504.5 | learning rate: 4.240E-05 | lm loss: 1.067722E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.41
START iteration 53, CKPT_AND_STOP: False
Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:58:58.229862] iteration       54/   18750 | elapsed time per iteration (ms): 4444.5 | learning rate: 4.320E-05 | lm loss: 1.067698E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.84
START iteration 54, CKPT_AND_STOP: False
Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:59:02.681228] iteration       55/   18750 | elapsed time per iteration (ms): 4451.3 | learning rate: 4.400E-05 | lm loss: 1.066720E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.35
START iteration 55, CKPT_AND_STOP: False
Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:59:07.131315] iteration       56/   18750 | elapsed time per iteration (ms): 4450.1 | learning rate: 4.480E-05 | lm loss: 1.065889E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.41
START iteration 56, CKPT_AND_STOP: False
Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:59:11.644534] iteration       57/   18750 | elapsed time per iteration (ms): 4513.2 | learning rate: 4.560E-05 | lm loss: 1.066347E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.41
START iteration 57, CKPT_AND_STOP: False
Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:59:16.029620] iteration       58/   18750 | elapsed time per iteration (ms): 4385.1 | learning rate: 4.640E-05 | lm loss: 1.066126E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.36
START iteration 58, CKPT_AND_STOP: False
Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 13:59:20.478583] iteration       59/   18750 | elapsed time per iteration (ms): 4448.9 | learning rate: 4.720E-05 | lm loss: 1.065611E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.38
START iteration 59, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 60, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      60 to s3://spot-checkpoints/gpt/iter_0000060/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000060/mp_rank_00/model_optim_rng.pt
Opt ckpt time 23.74285650253296
Process done with return code 0
Parent process ID: 19033 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 5940361.81640625 0
End of simulation:  Mini-batch time (usec) = 8725127
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164547, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 52 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 12 0 2836733.88671875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6512857
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82709, max long fwd 89657; min long bwd 149926, max long bwd 157738
Time taken by simulation: 216 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 21 0 1916562.744140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6803052
Min send: 10000000, max send 0
Min long send: 38109, max long send 62549
Min fwd: 45773, max fwd 59456; min bwd 92874, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 96930, max long bwd 106927
Time taken by simulation: 612 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 13 0 1338646.484375 80871.45884831746
End of simulation:  Mini-batch time (usec) = 5132496
Min send: 10000000, max send 0
Min long send: 80953, max long send 103337
Min fwd: 48269, max fwd 59401; min bwd 107938, max bwd 126574
Min long fwd: 53770, max long fwd 60383; min long bwd 118672, max long bwd 125606
Time taken by simulation: 568 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 11 0 770127.8076171875 159635.3450129109
End of simulation:  Mini-batch time (usec) = 6157105
Min send: 10000000, max send 0
Min long send: 159689, max long send 184340
Min fwd: 52659, max fwd 63832; min bwd 123243, max bwd 140240
Min long fwd: 72998, max long fwd 78841; min long bwd 148211, max long bwd 154022
Time taken by simulation: 758 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 11 0 442992.24853515625 230166.85465330718
End of simulation:  Mini-batch time (usec) = 8313990
Min send: 10000000, max send 0
Min long send: 230166, max long send 255370
Min fwd: 54051, max fwd 66033; min bwd 132265, max bwd 140978
Min long fwd: 81808, max long fwd 89347; min long bwd 157650, max long bwd 164467
Time taken by simulation: 1044 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1935 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1924 microseconds

can't have 24 stages!
{1: 8.725127, 2: 6.512857, 3: 6.803052, 4: 5.132496, 6: 6.157105, 8: 8.31399, 12: 15.020502, 16: 20.148929}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13}
best config is: 4 2
expected time is 5.132496
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 60
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 60
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
device is' 0
dry run time 1.9485690593719482
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
13 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000060/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000060/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 83.077 seconds
setting training data start iteration to 60
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 101496.14 | train/valid/test data iterators: 282.77
training ...
START iteration 60, CKPT_AND_STOP: False
Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:01:54.209399] iteration       61/   18750 | elapsed time per iteration (ms): 8107.7 | learning rate: 4.880E-05 | lm loss: 1.065095E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 61 iterations memory (MB) | allocated: 6137.60009765625 | max allocated: 13061.97802734375 | reserved: 14082.0 | max reserved: 14082.0
time (ms) | optimizer: 23.89 | batch generator: 4.70
START iteration 61, CKPT_AND_STOP: False
Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:01:58.855159] iteration       62/   18750 | elapsed time per iteration (ms): 4645.8 | learning rate: 4.960E-05 | lm loss: 1.065095E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.52
START iteration 62, CKPT_AND_STOP: False
Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:03.531923] iteration       63/   18750 | elapsed time per iteration (ms): 4676.7 | learning rate: 5.040E-05 | lm loss: 1.066283E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.51
START iteration 63, CKPT_AND_STOP: False
Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:09.168240] iteration       64/   18750 | elapsed time per iteration (ms): 5636.3 | learning rate: 5.120E-05 | lm loss: 1.064942E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.55
START iteration 64, CKPT_AND_STOP: False
Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:13.796799] iteration       65/   18750 | elapsed time per iteration (ms): 4628.5 | learning rate: 5.200E-05 | lm loss: 1.064677E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.50
START iteration 65, CKPT_AND_STOP: False
Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:18.404398] iteration       66/   18750 | elapsed time per iteration (ms): 4607.6 | learning rate: 5.280E-05 | lm loss: 1.064531E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.62
START iteration 66, CKPT_AND_STOP: False
Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:23.062172] iteration       67/   18750 | elapsed time per iteration (ms): 4657.8 | learning rate: 5.360E-05 | lm loss: 1.064182E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.86
START iteration 67, CKPT_AND_STOP: False
Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:27.696280] iteration       68/   18750 | elapsed time per iteration (ms): 4634.1 | learning rate: 5.440E-05 | lm loss: 1.064251E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.48
START iteration 68, CKPT_AND_STOP: False
Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:32.337420] iteration       69/   18750 | elapsed time per iteration (ms): 4641.1 | learning rate: 5.520E-05 | lm loss: 1.064095E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.46
START iteration 69, CKPT_AND_STOP: False
Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:36.965978] iteration       70/   18750 | elapsed time per iteration (ms): 4628.5 | learning rate: 5.600E-05 | lm loss: 1.064181E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.48
START iteration 70, CKPT_AND_STOP: False
Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:41.614139] iteration       71/   18750 | elapsed time per iteration (ms): 4648.1 | learning rate: 5.680E-05 | lm loss: 1.063777E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.83
START iteration 71, CKPT_AND_STOP: False
Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:46.260923] iteration       72/   18750 | elapsed time per iteration (ms): 4646.8 | learning rate: 5.760E-05 | lm loss: 1.063727E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.66
START iteration 72, CKPT_AND_STOP: False
Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:50.924158] iteration       73/   18750 | elapsed time per iteration (ms): 4663.2 | learning rate: 5.840E-05 | lm loss: 1.063502E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.68
START iteration 73, CKPT_AND_STOP: False
Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:02:55.589786] iteration       74/   18750 | elapsed time per iteration (ms): 4665.6 | learning rate: 5.920E-05 | lm loss: 1.063350E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.87
START iteration 74, CKPT_AND_STOP: False
Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:00.287420] iteration       75/   18750 | elapsed time per iteration (ms): 4697.6 | learning rate: 6.000E-05 | lm loss: 1.063637E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.51
START iteration 75, CKPT_AND_STOP: False
Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:04.955664] iteration       76/   18750 | elapsed time per iteration (ms): 4668.2 | learning rate: 6.080E-05 | lm loss: 1.063325E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.44
START iteration 76, CKPT_AND_STOP: False
Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:09.653903] iteration       77/   18750 | elapsed time per iteration (ms): 4698.2 | learning rate: 6.160E-05 | lm loss: 1.061847E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.52
START iteration 77, CKPT_AND_STOP: False
Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:14.294348] iteration       78/   18750 | elapsed time per iteration (ms): 4640.4 | learning rate: 6.240E-05 | lm loss: 1.062969E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.43
START iteration 78, CKPT_AND_STOP: False
Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:18.953683] iteration       79/   18750 | elapsed time per iteration (ms): 4659.3 | learning rate: 6.320E-05 | lm loss: 1.062974E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.74 | batch generator: 1.50
START iteration 79, CKPT_AND_STOP: False
Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:23.616583] iteration       80/   18750 | elapsed time per iteration (ms): 4662.9 | learning rate: 6.400E-05 | lm loss: 1.063065E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.44
START iteration 80, CKPT_AND_STOP: False
Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:28.286088] iteration       81/   18750 | elapsed time per iteration (ms): 4669.5 | learning rate: 6.480E-05 | lm loss: 1.062859E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.46
START iteration 81, CKPT_AND_STOP: False
Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:32.961147] iteration       82/   18750 | elapsed time per iteration (ms): 4675.0 | learning rate: 6.560E-05 | lm loss: 1.062739E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.49
START iteration 82, CKPT_AND_STOP: False
Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:37.650399] iteration       83/   18750 | elapsed time per iteration (ms): 4689.2 | learning rate: 6.640E-05 | lm loss: 1.062596E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.50
START iteration 83, CKPT_AND_STOP: False
Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:42.471253] iteration       84/   18750 | elapsed time per iteration (ms): 4820.8 | learning rate: 6.720E-05 | lm loss: 1.062864E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.56
START iteration 84, CKPT_AND_STOP: False
Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:47.102249] iteration       85/   18750 | elapsed time per iteration (ms): 4631.0 | learning rate: 6.800E-05 | lm loss: 1.062641E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.52
START iteration 85, CKPT_AND_STOP: False
Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:51.757195] iteration       86/   18750 | elapsed time per iteration (ms): 4654.9 | learning rate: 6.880E-05 | lm loss: 1.062729E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.40
START iteration 86, CKPT_AND_STOP: False
Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:03:57.135637] iteration       87/   18750 | elapsed time per iteration (ms): 5378.4 | learning rate: 6.960E-05 | lm loss: 1.062575E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.57
START iteration 87, CKPT_AND_STOP: False
Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:01.827398] iteration       88/   18750 | elapsed time per iteration (ms): 4691.7 | learning rate: 7.040E-05 | lm loss: 1.062767E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.43
START iteration 88, CKPT_AND_STOP: False
Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:06.458969] iteration       89/   18750 | elapsed time per iteration (ms): 4631.6 | learning rate: 7.120E-05 | lm loss: 1.062393E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.50
START iteration 89, CKPT_AND_STOP: False
Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:11.146635] iteration       90/   18750 | elapsed time per iteration (ms): 4687.6 | learning rate: 7.200E-05 | lm loss: 1.062297E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.37
START iteration 90, CKPT_AND_STOP: False
Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:15.795728] iteration       91/   18750 | elapsed time per iteration (ms): 4649.1 | learning rate: 7.280E-05 | lm loss: 1.062652E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.64
START iteration 91, CKPT_AND_STOP: False
Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:20.494327] iteration       92/   18750 | elapsed time per iteration (ms): 4698.6 | learning rate: 7.360E-05 | lm loss: 1.062324E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.36
START iteration 92, CKPT_AND_STOP: False
Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:25.171967] iteration       93/   18750 | elapsed time per iteration (ms): 4677.6 | learning rate: 7.440E-05 | lm loss: 1.062839E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.32
START iteration 93, CKPT_AND_STOP: False
Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:29.821770] iteration       94/   18750 | elapsed time per iteration (ms): 4649.8 | learning rate: 7.520E-05 | lm loss: 1.062257E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.34
START iteration 94, CKPT_AND_STOP: False
Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:34.487651] iteration       95/   18750 | elapsed time per iteration (ms): 4665.9 | learning rate: 7.600E-05 | lm loss: 1.061996E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.47
START iteration 95, CKPT_AND_STOP: False
Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:39.096596] iteration       96/   18750 | elapsed time per iteration (ms): 4608.9 | learning rate: 7.680E-05 | lm loss: 1.062292E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.68
START iteration 96, CKPT_AND_STOP: False
Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:43.768774] iteration       97/   18750 | elapsed time per iteration (ms): 4672.2 | learning rate: 7.760E-05 | lm loss: 1.062128E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.43
START iteration 97, CKPT_AND_STOP: False
Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:48.391932] iteration       98/   18750 | elapsed time per iteration (ms): 4623.1 | learning rate: 7.840E-05 | lm loss: 1.061916E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.61
START iteration 98, CKPT_AND_STOP: False
Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:53.036931] iteration       99/   18750 | elapsed time per iteration (ms): 4645.0 | learning rate: 7.920E-05 | lm loss: 1.061803E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.63
START iteration 99, CKPT_AND_STOP: False
Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:04:57.645128] iteration      100/   18750 | elapsed time per iteration (ms): 4608.2 | learning rate: 8.000E-05 | lm loss: 1.062086E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.92
START iteration 100, CKPT_AND_STOP: False
Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:02.245908] iteration      101/   18750 | elapsed time per iteration (ms): 4600.8 | learning rate: 8.080E-05 | lm loss: 1.062015E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.76 | batch generator: 1.44
START iteration 101, CKPT_AND_STOP: False
Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:06.903055] iteration      102/   18750 | elapsed time per iteration (ms): 4657.1 | learning rate: 8.160E-05 | lm loss: 1.062140E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.49
START iteration 102, CKPT_AND_STOP: False
Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:11.590876] iteration      103/   18750 | elapsed time per iteration (ms): 4687.8 | learning rate: 8.240E-05 | lm loss: 1.061987E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.36
START iteration 103, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:16.195709] iteration      104/   18750 | elapsed time per iteration (ms): 4604.8 | learning rate: 8.240E-05 | loss scale: 65536.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | optimizer: 4.29 | batch generator: 1.43
START iteration 104, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:20.871377] iteration      105/   18750 | elapsed time per iteration (ms): 4675.7 | learning rate: 8.240E-05 | loss scale: 32768.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | optimizer: 4.29 | batch generator: 1.43
START iteration 105, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:25.510868] iteration      106/   18750 | elapsed time per iteration (ms): 4639.5 | learning rate: 8.240E-05 | loss scale: 16384.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | optimizer: 4.34 | batch generator: 1.74
START iteration 106, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:30.184803] iteration      107/   18750 | elapsed time per iteration (ms): 4673.9 | learning rate: 8.240E-05 | loss scale: 8192.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | optimizer: 4.30 | batch generator: 1.47
START iteration 107, CKPT_AND_STOP: False
Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:34.919949] iteration      108/   18750 | elapsed time per iteration (ms): 4735.1 | learning rate: 8.320E-05 | lm loss: 1.210924E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.39
START iteration 108, CKPT_AND_STOP: False
Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:39.610856] iteration      109/   18750 | elapsed time per iteration (ms): 4690.9 | learning rate: 8.400E-05 | lm loss: 1.062008E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.42
START iteration 109, CKPT_AND_STOP: False
Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:44.285936] iteration      110/   18750 | elapsed time per iteration (ms): 4675.1 | learning rate: 8.480E-05 | lm loss: 1.062170E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.34
START iteration 110, CKPT_AND_STOP: False
Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:48.945871] iteration      111/   18750 | elapsed time per iteration (ms): 4659.9 | learning rate: 8.560E-05 | lm loss: 1.062895E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.84
START iteration 111, CKPT_AND_STOP: False
Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:53.790118] iteration      112/   18750 | elapsed time per iteration (ms): 4844.2 | learning rate: 8.640E-05 | lm loss: 1.062629E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.50
START iteration 112, CKPT_AND_STOP: False
Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:05:58.499140] iteration      113/   18750 | elapsed time per iteration (ms): 4709.0 | learning rate: 8.720E-05 | lm loss: 1.062485E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.57
START iteration 113, CKPT_AND_STOP: False
Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:03.140509] iteration      114/   18750 | elapsed time per iteration (ms): 4641.4 | learning rate: 8.800E-05 | lm loss: 1.061973E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.44
START iteration 114, CKPT_AND_STOP: False
Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:07.783765] iteration      115/   18750 | elapsed time per iteration (ms): 4643.2 | learning rate: 8.880E-05 | lm loss: 1.061810E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.56
START iteration 115, CKPT_AND_STOP: False
Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:12.457246] iteration      116/   18750 | elapsed time per iteration (ms): 4673.5 | learning rate: 8.960E-05 | lm loss: 1.062059E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.41
START iteration 116, CKPT_AND_STOP: False
Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:17.092338] iteration      117/   18750 | elapsed time per iteration (ms): 4635.1 | learning rate: 9.040E-05 | lm loss: 1.062041E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.64
START iteration 117, CKPT_AND_STOP: False
Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:21.751453] iteration      118/   18750 | elapsed time per iteration (ms): 4659.1 | learning rate: 9.120E-05 | lm loss: 1.062415E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.39
START iteration 118, CKPT_AND_STOP: False
Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:26.425808] iteration      119/   18750 | elapsed time per iteration (ms): 4674.3 | learning rate: 9.200E-05 | lm loss: 1.062564E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.79
START iteration 119, CKPT_AND_STOP: False
Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:31.099277] iteration      120/   18750 | elapsed time per iteration (ms): 4673.5 | learning rate: 9.280E-05 | lm loss: 1.065083E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.52
START iteration 120, CKPT_AND_STOP: False
Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:35.782004] iteration      121/   18750 | elapsed time per iteration (ms): 4682.7 | learning rate: 9.360E-05 | lm loss: 1.064898E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.82
START iteration 121, CKPT_AND_STOP: False
Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:40.411207] iteration      122/   18750 | elapsed time per iteration (ms): 4629.2 | learning rate: 9.440E-05 | lm loss: 1.064640E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.48
START iteration 122, CKPT_AND_STOP: False
Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:45.055783] iteration      123/   18750 | elapsed time per iteration (ms): 4644.5 | learning rate: 9.520E-05 | lm loss: 1.064839E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.83
START iteration 123, CKPT_AND_STOP: False
Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:49.672615] iteration      124/   18750 | elapsed time per iteration (ms): 4616.8 | learning rate: 9.600E-05 | lm loss: 1.064765E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.51
START iteration 124, CKPT_AND_STOP: False
Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:54.333773] iteration      125/   18750 | elapsed time per iteration (ms): 4661.1 | learning rate: 9.680E-05 | lm loss: 1.062687E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.48
START iteration 125, CKPT_AND_STOP: False
Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:06:59.008325] iteration      126/   18750 | elapsed time per iteration (ms): 4674.5 | learning rate: 9.760E-05 | lm loss: 1.062497E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.50
START iteration 126, CKPT_AND_STOP: False
Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:03.716296] iteration      127/   18750 | elapsed time per iteration (ms): 4708.0 | learning rate: 9.840E-05 | lm loss: 1.062477E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.44
START iteration 127, CKPT_AND_STOP: False
Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:08.328816] iteration      128/   18750 | elapsed time per iteration (ms): 4612.5 | learning rate: 9.920E-05 | lm loss: 1.062601E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.50
START iteration 128, CKPT_AND_STOP: False
Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:13.010398] iteration      129/   18750 | elapsed time per iteration (ms): 4681.6 | learning rate: 1.000E-04 | lm loss: 1.062527E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.59
START iteration 129, CKPT_AND_STOP: False
Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:17.712007] iteration      130/   18750 | elapsed time per iteration (ms): 4701.6 | learning rate: 1.008E-04 | lm loss: 1.062526E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.49
START iteration 130, CKPT_AND_STOP: False
Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:22.340431] iteration      131/   18750 | elapsed time per iteration (ms): 4628.4 | learning rate: 1.016E-04 | lm loss: 1.062466E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.34
START iteration 131, CKPT_AND_STOP: False
Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:26.983377] iteration      132/   18750 | elapsed time per iteration (ms): 4642.9 | learning rate: 1.024E-04 | lm loss: 1.062694E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.35
START iteration 132, CKPT_AND_STOP: False
Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:31.633688] iteration      133/   18750 | elapsed time per iteration (ms): 4650.3 | learning rate: 1.032E-04 | lm loss: 1.062503E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.38
START iteration 133, CKPT_AND_STOP: False
Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:36.230868] iteration      134/   18750 | elapsed time per iteration (ms): 4597.2 | learning rate: 1.040E-04 | lm loss: 1.062684E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.53
START iteration 134, CKPT_AND_STOP: False
Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:40.882196] iteration      135/   18750 | elapsed time per iteration (ms): 4651.3 | learning rate: 1.048E-04 | lm loss: 1.062416E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.80
START iteration 135, CKPT_AND_STOP: False
Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:45.579427] iteration      136/   18750 | elapsed time per iteration (ms): 4697.2 | learning rate: 1.056E-04 | lm loss: 1.062056E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.43
START iteration 136, CKPT_AND_STOP: False
Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:50.233417] iteration      137/   18750 | elapsed time per iteration (ms): 4654.0 | learning rate: 1.064E-04 | lm loss: 1.061847E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.52
START iteration 137, CKPT_AND_STOP: False
Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:54.837718] iteration      138/   18750 | elapsed time per iteration (ms): 4604.3 | learning rate: 1.072E-04 | lm loss: 1.061828E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.50
START iteration 138, CKPT_AND_STOP: False
Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:07:59.501385] iteration      139/   18750 | elapsed time per iteration (ms): 4663.7 | learning rate: 1.080E-04 | lm loss: 1.062086E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.51
START iteration 139, CKPT_AND_STOP: False
Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:04.199149] iteration      140/   18750 | elapsed time per iteration (ms): 4697.7 | learning rate: 1.088E-04 | lm loss: 1.061951E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.46
START iteration 140, CKPT_AND_STOP: False
Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:08.854668] iteration      141/   18750 | elapsed time per iteration (ms): 4655.5 | learning rate: 1.096E-04 | lm loss: 1.062769E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.54
START iteration 141, CKPT_AND_STOP: False
Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:13.494115] iteration      142/   18750 | elapsed time per iteration (ms): 4639.4 | learning rate: 1.104E-04 | lm loss: 1.062493E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.47
START iteration 142, CKPT_AND_STOP: False
Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:18.334614] iteration      143/   18750 | elapsed time per iteration (ms): 4840.5 | learning rate: 1.112E-04 | lm loss: 1.062683E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.57
START iteration 143, CKPT_AND_STOP: False
Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:22.994595] iteration      144/   18750 | elapsed time per iteration (ms): 4660.0 | learning rate: 1.120E-04 | lm loss: 1.062828E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.41
START iteration 144, CKPT_AND_STOP: False
Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:27.650632] iteration      145/   18750 | elapsed time per iteration (ms): 4656.0 | learning rate: 1.128E-04 | lm loss: 1.062653E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.45
START iteration 145, CKPT_AND_STOP: False
Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:32.267562] iteration      146/   18750 | elapsed time per iteration (ms): 4616.9 | learning rate: 1.136E-04 | lm loss: 1.062549E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.40
START iteration 146, CKPT_AND_STOP: False
Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:36.986805] iteration      147/   18750 | elapsed time per iteration (ms): 4719.2 | learning rate: 1.144E-04 | lm loss: 1.062441E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.54
START iteration 147, CKPT_AND_STOP: False
Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:41.702305] iteration      148/   18750 | elapsed time per iteration (ms): 4715.5 | learning rate: 1.152E-04 | lm loss: 1.062689E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.47
START iteration 148, CKPT_AND_STOP: False
Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:46.384036] iteration      149/   18750 | elapsed time per iteration (ms): 4681.7 | learning rate: 1.160E-04 | lm loss: 1.062500E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.79 | batch generator: 1.38
START iteration 149, CKPT_AND_STOP: False
Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:51.037818] iteration      150/   18750 | elapsed time per iteration (ms): 4653.8 | learning rate: 1.168E-04 | lm loss: 1.062661E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.39
START iteration 150, CKPT_AND_STOP: False
Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:08:55.658148] iteration      151/   18750 | elapsed time per iteration (ms): 4620.3 | learning rate: 1.176E-04 | lm loss: 1.062481E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.46
START iteration 151, CKPT_AND_STOP: False
Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:00.321830] iteration      152/   18750 | elapsed time per iteration (ms): 4663.7 | learning rate: 1.184E-04 | lm loss: 1.062442E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.50
START iteration 152, CKPT_AND_STOP: False
Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:04.990846] iteration      153/   18750 | elapsed time per iteration (ms): 4669.0 | learning rate: 1.192E-04 | lm loss: 1.062138E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.56
START iteration 153, CKPT_AND_STOP: False
Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:09.644834] iteration      154/   18750 | elapsed time per iteration (ms): 4654.0 | learning rate: 1.200E-04 | lm loss: 1.062278E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.57
START iteration 154, CKPT_AND_STOP: False
Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:14.318335] iteration      155/   18750 | elapsed time per iteration (ms): 4673.5 | learning rate: 1.208E-04 | lm loss: 1.062536E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.50
START iteration 155, CKPT_AND_STOP: False
Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:19.003345] iteration      156/   18750 | elapsed time per iteration (ms): 4685.0 | learning rate: 1.216E-04 | lm loss: 1.062632E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.45
START iteration 156, CKPT_AND_STOP: False
Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:23.645393] iteration      157/   18750 | elapsed time per iteration (ms): 4642.0 | learning rate: 1.224E-04 | lm loss: 1.062232E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.50
START iteration 157, CKPT_AND_STOP: False
Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:28.308514] iteration      158/   18750 | elapsed time per iteration (ms): 4663.1 | learning rate: 1.232E-04 | lm loss: 1.061896E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.47
START iteration 158, CKPT_AND_STOP: False
Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:32.960935] iteration      159/   18750 | elapsed time per iteration (ms): 4652.4 | learning rate: 1.240E-04 | lm loss: 1.062005E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.44
START iteration 159, CKPT_AND_STOP: False
Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:37.575512] iteration      160/   18750 | elapsed time per iteration (ms): 4614.6 | learning rate: 1.248E-04 | lm loss: 1.062100E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.52
START iteration 160, CKPT_AND_STOP: False
Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:42.257965] iteration      161/   18750 | elapsed time per iteration (ms): 4682.4 | learning rate: 1.256E-04 | lm loss: 1.061981E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.39
START iteration 161, CKPT_AND_STOP: False
Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:46.889312] iteration      162/   18750 | elapsed time per iteration (ms): 4631.3 | learning rate: 1.264E-04 | lm loss: 1.061960E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.49
START iteration 162, CKPT_AND_STOP: False
Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:51.569054] iteration      163/   18750 | elapsed time per iteration (ms): 4679.7 | learning rate: 1.272E-04 | lm loss: 1.061938E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.80
START iteration 163, CKPT_AND_STOP: False
Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:09:56.214360] iteration      164/   18750 | elapsed time per iteration (ms): 4645.3 | learning rate: 1.280E-04 | lm loss: 1.062189E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.55
START iteration 164, CKPT_AND_STOP: False
Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:00.821459] iteration      165/   18750 | elapsed time per iteration (ms): 4607.1 | learning rate: 1.288E-04 | lm loss: 1.062010E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.42
START iteration 165, CKPT_AND_STOP: False
Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:05.469102] iteration      166/   18750 | elapsed time per iteration (ms): 4647.6 | learning rate: 1.296E-04 | lm loss: 1.062166E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.52
START iteration 166, CKPT_AND_STOP: False
Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:10.142210] iteration      167/   18750 | elapsed time per iteration (ms): 4673.1 | learning rate: 1.304E-04 | lm loss: 1.062118E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.49
START iteration 167, CKPT_AND_STOP: False
Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:14.792408] iteration      168/   18750 | elapsed time per iteration (ms): 4650.2 | learning rate: 1.312E-04 | lm loss: 1.062276E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.50
START iteration 168, CKPT_AND_STOP: False
Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:19.441006] iteration      169/   18750 | elapsed time per iteration (ms): 4648.6 | learning rate: 1.320E-04 | lm loss: 1.062000E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.46
START iteration 169, CKPT_AND_STOP: False
Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:24.045927] iteration      170/   18750 | elapsed time per iteration (ms): 4604.9 | learning rate: 1.328E-04 | lm loss: 1.061984E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.52
START iteration 170, CKPT_AND_STOP: False
Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:28.737960] iteration      171/   18750 | elapsed time per iteration (ms): 4692.0 | learning rate: 1.336E-04 | lm loss: 1.062346E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.51
START iteration 171, CKPT_AND_STOP: False
Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:33.392211] iteration      172/   18750 | elapsed time per iteration (ms): 4654.2 | learning rate: 1.344E-04 | lm loss: 1.062462E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.52
START iteration 172, CKPT_AND_STOP: False
Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:38.057911] iteration      173/   18750 | elapsed time per iteration (ms): 4665.7 | learning rate: 1.352E-04 | lm loss: 1.062304E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 1.45
START iteration 173, CKPT_AND_STOP: False
Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:42.667313] iteration      174/   18750 | elapsed time per iteration (ms): 4609.4 | learning rate: 1.360E-04 | lm loss: 1.062040E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.49
START iteration 174, CKPT_AND_STOP: False
Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:47.318739] iteration      175/   18750 | elapsed time per iteration (ms): 4651.4 | learning rate: 1.368E-04 | lm loss: 1.062018E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.73 | batch generator: 1.49
START iteration 175, CKPT_AND_STOP: False
Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:51.986303] iteration      176/   18750 | elapsed time per iteration (ms): 4667.5 | learning rate: 1.376E-04 | lm loss: 1.062214E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.48
START iteration 176, CKPT_AND_STOP: False
Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:10:56.620083] iteration      177/   18750 | elapsed time per iteration (ms): 4633.8 | learning rate: 1.384E-04 | lm loss: 1.062195E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.42
START iteration 177, CKPT_AND_STOP: False
Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:11:01.249648] iteration      178/   18750 | elapsed time per iteration (ms): 4629.5 | learning rate: 1.392E-04 | lm loss: 1.062047E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.48
START iteration 178, CKPT_AND_STOP: False
Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:11:05.867801] iteration      179/   18750 | elapsed time per iteration (ms): 4618.1 | learning rate: 1.400E-04 | lm loss: 1.062030E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 1.66
START iteration 179, CKPT_AND_STOP: False
Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:11:10.487929] iteration      180/   18750 | elapsed time per iteration (ms): 4620.1 | learning rate: 1.408E-04 | lm loss: 1.062244E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.74 | batch generator: 1.42
START iteration 180, CKPT_AND_STOP: False
Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:11:15.099278] iteration      181/   18750 | elapsed time per iteration (ms): 4611.3 | learning rate: 1.416E-04 | lm loss: 1.062129E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.69 | batch generator: 1.40
START iteration 181, CKPT_AND_STOP: False
Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:11:19.759588] iteration      182/   18750 | elapsed time per iteration (ms): 4660.3 | learning rate: 1.424E-04 | lm loss: 1.062225E+01 | loss scale: 8192.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.74
START iteration 182, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 183, CKPT_AND_STOP: True, flag: tensor([2], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     183 to s3://spot-checkpoints/gpt/iter_0000183/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000183/mp_rank_00/model_optim_rng.pt
Opt ckpt time 19.753519773483276
Process done with return code 0
Parent process ID: 20598 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 5592122.55859375 0
End of simulation:  Mini-batch time (usec) = 9770154
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164303, max long fwd 168896; min long bwd 294106, max long bwd 302385
Time taken by simulation: 62 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 21 0 2732766.6015625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 9192802
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82380, max long fwd 89657; min long bwd 149642, max long bwd 157738
Time taken by simulation: 380 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 1527054.19921875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8830859
Min send: 10000000, max send 0
Min long send: 38055, max long send 61744
Min fwd: 45773, max fwd 61214; min bwd 93190, max bwd 104086
Min long fwd: 56519, max long fwd 64913; min long bwd 96534, max long bwd 106927
Time taken by simulation: 908 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1050667.96875 80871.45884831746
End of simulation:  Mini-batch time (usec) = 6512804
Min send: 10000000, max send 0
Min long send: 80871, max long send 103337
Min fwd: 47099, max fwd 59401; min bwd 106731, max bwd 128252
Min long fwd: 54031, max long fwd 62603; min long bwd 118588, max long bwd 125970
Time taken by simulation: 919 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 16 0 548680.6640625 159635.3450129109
End of simulation:  Mini-batch time (usec) = 7062870
Min send: 10000000, max send 0
Min long send: 159689, max long send 182101
Min fwd: 52659, max fwd 63293; min bwd 123973, max bwd 139940
Min long fwd: 73341, max long fwd 81486; min long bwd 146196, max long bwd 154690
Time taken by simulation: 1126 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 0 230166.85465330718
End of simulation:  Mini-batch time (usec) = 10536264
Min send: 10000000, max send 0
Min long send: 230303, max long send 257328
Min fwd: 54967, max fwd 67591; min bwd 130849, max bwd 143444
Min long fwd: 78331, max long fwd 89630; min long bwd 156920, max long bwd 164176
Time taken by simulation: 2304 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1894 microseconds

can't have 16 stages!
can't have 24 stages!
{1: 9.770154, 2: 9.192802, 3: 8.830859, 4: 6.512804, 6: 7.06287, 8: 10.536264, 12: 15.020502}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10}
best config is: 4 2
expected time is 6.512804
3 per stage
12 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 3
stage to rank map: 0,4,8;1,5,9;2,6,10;3,7,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8;1,5,9;2,6,10;3,7,11; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 183
using world size: 12 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 183
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8;1,5,9;2,6,10;3,7,11;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 12
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
device is' 0
dry run time 1.4958043098449707
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
21 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000183/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000183/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 86.730 seconds
setting training data start iteration to 183
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 104426.97 | train/valid/test data iterators: 280.86
training ...
START iteration 183, CKPT_AND_STOP: False
Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:13:56.010080] iteration      184/   18750 | elapsed time per iteration (ms): 9382.2 | learning rate: 1.440E-04 | lm loss: 1.061910E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 184 iterations memory (MB) | allocated: 6137.93212890625 | max allocated: 13062.31005859375 | reserved: 14082.0 | max reserved: 14082.0
time (ms) | optimizer: 23.76 | batch generator: 8.52
START iteration 184, CKPT_AND_STOP: False
Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:02.009240] iteration      185/   18750 | elapsed time per iteration (ms): 5999.2 | learning rate: 1.448E-04 | lm loss: 1.062044E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 4.31
START iteration 185, CKPT_AND_STOP: False
Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:07.942538] iteration      186/   18750 | elapsed time per iteration (ms): 5933.3 | learning rate: 1.456E-04 | lm loss: 1.061961E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.68
START iteration 186, CKPT_AND_STOP: False
Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:13.983720] iteration      187/   18750 | elapsed time per iteration (ms): 6041.2 | learning rate: 1.464E-04 | lm loss: 1.062142E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.86
START iteration 187, CKPT_AND_STOP: False
Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:19.965146] iteration      188/   18750 | elapsed time per iteration (ms): 5981.4 | learning rate: 1.472E-04 | lm loss: 1.061971E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.55
START iteration 188, CKPT_AND_STOP: False
Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:25.952778] iteration      189/   18750 | elapsed time per iteration (ms): 5987.6 | learning rate: 1.480E-04 | lm loss: 1.061967E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.71
START iteration 189, CKPT_AND_STOP: False
Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:31.951158] iteration      190/   18750 | elapsed time per iteration (ms): 5998.4 | learning rate: 1.488E-04 | lm loss: 1.061961E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.66
START iteration 190, CKPT_AND_STOP: False
Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:37.937293] iteration      191/   18750 | elapsed time per iteration (ms): 5986.1 | learning rate: 1.496E-04 | lm loss: 1.062022E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.74
START iteration 191, CKPT_AND_STOP: False
Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:43.981593] iteration      192/   18750 | elapsed time per iteration (ms): 6044.3 | learning rate: 1.500E-04 | lm loss: 1.061978E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.50
START iteration 192, CKPT_AND_STOP: False
Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:49.992488] iteration      193/   18750 | elapsed time per iteration (ms): 6010.9 | learning rate: 1.500E-04 | lm loss: 1.062020E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.69
START iteration 193, CKPT_AND_STOP: False
Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:14:56.244222] iteration      194/   18750 | elapsed time per iteration (ms): 6251.7 | learning rate: 1.500E-04 | lm loss: 1.062022E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 2.07
START iteration 194, CKPT_AND_STOP: False
Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:02.301216] iteration      195/   18750 | elapsed time per iteration (ms): 6057.0 | learning rate: 1.500E-04 | lm loss: 1.062073E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.52
START iteration 195, CKPT_AND_STOP: False
Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:08.378871] iteration      196/   18750 | elapsed time per iteration (ms): 6077.6 | learning rate: 1.500E-04 | lm loss: 1.061992E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.57
START iteration 196, CKPT_AND_STOP: False
Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:14.461377] iteration      197/   18750 | elapsed time per iteration (ms): 6082.5 | learning rate: 1.500E-04 | lm loss: 1.061987E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 3.70
START iteration 197, CKPT_AND_STOP: False
Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:20.513395] iteration      198/   18750 | elapsed time per iteration (ms): 6052.0 | learning rate: 1.500E-04 | lm loss: 1.061876E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.58
START iteration 198, CKPT_AND_STOP: False
Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:26.545892] iteration      199/   18750 | elapsed time per iteration (ms): 6032.5 | learning rate: 1.500E-04 | lm loss: 1.063220E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 3.90
START iteration 199, CKPT_AND_STOP: False
Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:32.554561] iteration      200/   18750 | elapsed time per iteration (ms): 6008.7 | learning rate: 1.500E-04 | lm loss: 1.062458E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.55
START iteration 200, CKPT_AND_STOP: False
Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:38.554534] iteration      201/   18750 | elapsed time per iteration (ms): 6000.0 | learning rate: 1.500E-04 | lm loss: 1.062425E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 4.01
START iteration 201, CKPT_AND_STOP: False
Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:44.510191] iteration      202/   18750 | elapsed time per iteration (ms): 5955.6 | learning rate: 1.500E-04 | lm loss: 1.062498E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.55
START iteration 202, CKPT_AND_STOP: False
Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:50.478656] iteration      203/   18750 | elapsed time per iteration (ms): 5968.5 | learning rate: 1.500E-04 | lm loss: 1.062518E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.61
START iteration 203, CKPT_AND_STOP: False
Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:15:56.507248] iteration      204/   18750 | elapsed time per iteration (ms): 6028.6 | learning rate: 1.500E-04 | lm loss: 1.062299E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.56
START iteration 204, CKPT_AND_STOP: False
Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:02.529135] iteration      205/   18750 | elapsed time per iteration (ms): 6021.9 | learning rate: 1.500E-04 | lm loss: 1.062350E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.71
START iteration 205, CKPT_AND_STOP: False
Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:08.531210] iteration      206/   18750 | elapsed time per iteration (ms): 6002.1 | learning rate: 1.500E-04 | lm loss: 1.062308E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.62 | batch generator: 1.53
START iteration 206, CKPT_AND_STOP: False
Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:14.445894] iteration      207/   18750 | elapsed time per iteration (ms): 5914.7 | learning rate: 1.500E-04 | lm loss: 1.062398E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.53
START iteration 207, CKPT_AND_STOP: False
Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:20.493041] iteration      208/   18750 | elapsed time per iteration (ms): 6047.1 | learning rate: 1.500E-04 | lm loss: 1.062430E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.51
START iteration 208, CKPT_AND_STOP: False
Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:26.518198] iteration      209/   18750 | elapsed time per iteration (ms): 6025.1 | learning rate: 1.500E-04 | lm loss: 1.062314E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.58
START iteration 209, CKPT_AND_STOP: False
Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:32.535036] iteration      210/   18750 | elapsed time per iteration (ms): 6016.8 | learning rate: 1.500E-04 | lm loss: 1.062178E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.54
START iteration 210, CKPT_AND_STOP: False
Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:38.434169] iteration      211/   18750 | elapsed time per iteration (ms): 5899.1 | learning rate: 1.500E-04 | lm loss: 1.062348E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 3.88
START iteration 211, CKPT_AND_STOP: False
Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:44.412687] iteration      212/   18750 | elapsed time per iteration (ms): 5978.5 | learning rate: 1.500E-04 | lm loss: 1.062267E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 1.54
START iteration 212, CKPT_AND_STOP: False
Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:50.431261] iteration      213/   18750 | elapsed time per iteration (ms): 6018.6 | learning rate: 1.500E-04 | lm loss: 1.062141E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.49
START iteration 213, CKPT_AND_STOP: False
Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:16:56.445808] iteration      214/   18750 | elapsed time per iteration (ms): 6014.5 | learning rate: 1.500E-04 | lm loss: 1.061950E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 1.57
START iteration 214, CKPT_AND_STOP: False
Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:02.394727] iteration      215/   18750 | elapsed time per iteration (ms): 5948.9 | learning rate: 1.500E-04 | lm loss: 1.061522E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.68
START iteration 215, CKPT_AND_STOP: False
Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:08.351901] iteration      216/   18750 | elapsed time per iteration (ms): 5957.2 | learning rate: 1.500E-04 | lm loss: 1.061953E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.96
START iteration 216, CKPT_AND_STOP: False
Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:14.349897] iteration      217/   18750 | elapsed time per iteration (ms): 5998.0 | learning rate: 1.500E-04 | lm loss: 1.061886E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 3.62
START iteration 217, CKPT_AND_STOP: False
Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:20.324584] iteration      218/   18750 | elapsed time per iteration (ms): 5974.7 | learning rate: 1.500E-04 | lm loss: 1.061992E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 3.91
START iteration 218, CKPT_AND_STOP: False
Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:26.337556] iteration      219/   18750 | elapsed time per iteration (ms): 6013.0 | learning rate: 1.500E-04 | lm loss: 1.062142E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.92
START iteration 219, CKPT_AND_STOP: False
Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:32.324438] iteration      220/   18750 | elapsed time per iteration (ms): 5986.9 | learning rate: 1.500E-04 | lm loss: 1.062031E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.62 | batch generator: 2.04
START iteration 220, CKPT_AND_STOP: False
Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:38.323024] iteration      221/   18750 | elapsed time per iteration (ms): 5998.6 | learning rate: 1.500E-04 | lm loss: 1.062127E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.55
START iteration 221, CKPT_AND_STOP: False
Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:44.299084] iteration      222/   18750 | elapsed time per iteration (ms): 5976.0 | learning rate: 1.500E-04 | lm loss: 1.061934E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 3.48
START iteration 222, CKPT_AND_STOP: False
Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:50.324265] iteration      223/   18750 | elapsed time per iteration (ms): 6025.2 | learning rate: 1.500E-04 | lm loss: 1.061998E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.87
START iteration 223, CKPT_AND_STOP: False
Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:17:56.320385] iteration      224/   18750 | elapsed time per iteration (ms): 5996.1 | learning rate: 1.500E-04 | lm loss: 1.062135E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.50
START iteration 224, CKPT_AND_STOP: False
Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:18:02.311626] iteration      225/   18750 | elapsed time per iteration (ms): 5991.2 | learning rate: 1.500E-04 | lm loss: 1.061869E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.75
START iteration 225, CKPT_AND_STOP: False
Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:18:08.402509] iteration      226/   18750 | elapsed time per iteration (ms): 6090.9 | learning rate: 1.500E-04 | lm loss: 1.061901E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 3.40
START iteration 226, CKPT_AND_STOP: False
Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:18:14.763581] iteration      227/   18750 | elapsed time per iteration (ms): 6361.1 | learning rate: 1.500E-04 | lm loss: 1.062030E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.84
START iteration 227, CKPT_AND_STOP: False
Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-21 14:18:20.735451] iteration      228/   18750 | elapsed time per iteration (ms): 5971.9 | learning rate: 1.500E-04 | lm loss: 1.062009E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 2.05
START iteration 228, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 229, CKPT_AND_STOP: True, flag: tensor([4], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     229 to s3://spot-checkpoints/gpt/iter_0000229/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000229/mp_rank_00/model_optim_rng.pt
Opt ckpt time 28.49976897239685
Process done with return code 0
Parent process ID: 21942 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 5880609.86328125 0
End of simulation:  Mini-batch time (usec) = 10058641
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164303, max long fwd 168896; min long bwd 294106, max long bwd 302385
Time taken by simulation: 61 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 18 0 2725911.1328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8250798
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82380, max long fwd 89657; min long bwd 149642, max long bwd 157738
Time taken by simulation: 303 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 32 0 1527054.19921875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8830859
Min send: 10000000, max send 0
Min long send: 38055, max long send 61744
Min fwd: 45773, max fwd 61214; min bwd 93190, max bwd 104086
Min long fwd: 56519, max long fwd 64913; min long bwd 96534, max long bwd 106927
Time taken by simulation: 906 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1050667.96875 80871.45884831746
End of simulation:  Mini-batch time (usec) = 6512804
Min send: 10000000, max send 0
Min long send: 80871, max long send 103337
Min fwd: 47099, max fwd 59401; min bwd 106731, max bwd 128252
Min long fwd: 54031, max long fwd 62603; min long bwd 118588, max long bwd 125970
Time taken by simulation: 956 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 16 0 548680.6640625 159635.3450129109
End of simulation:  Mini-batch time (usec) = 7062870
Min send: 10000000, max send 0
Min long send: 159689, max long send 182101
Min fwd: 52659, max fwd 63293; min bwd 123973, max bwd 139940
Min long fwd: 73341, max long fwd 81486; min long bwd 146196, max long bwd 154690
Time taken by simulation: 1128 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 0 230166.85465330718
End of simulation:  Mini-batch time (usec) = 10536264
Min send: 10000000, max send 0
Min long send: 230303, max long send 257328
Min fwd: 54967, max fwd 67591; min bwd 130849, max bwd 143444
Min long fwd: 78331, max long fwd 89630; min long bwd 156920, max long bwd 164176
Time taken by simulation: 2203 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1910 microseconds

can't have 16 stages!
can't have 24 stages!
{1: 10.058641, 2: 8.250798, 3: 8.830859, 4: 6.512804, 6: 7.06287, 8: 10.536264, 12: 15.020502}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10}
best config is: 4 2
expected time is 6.512804
3 per stage
12 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 3
stage to rank map: 0,4,8;1,5,9;2,6,10;3,7,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8;1,5,9;2,6,10;3,7,11; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 229
using world size: 12 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 229
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8;1,5,9;2,6,10;3,7,11;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 12
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Signal handler called with signal 10


 STOPPING VARUNA !!



Parent process ID: 22286 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 6165271.97265625 0
End of simulation:  Mini-batch time (usec) = 8950037
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164547, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 53 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 14 0 2816083.251953125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7191944
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82709, max long fwd 89657; min long bwd 149926, max long bwd 157738
Time taken by simulation: 242 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 21 0 1916562.744140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6803052
Min send: 10000000, max send 0
Min long send: 38109, max long send 62549
Min fwd: 45773, max fwd 59456; min bwd 92874, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 96930, max long bwd 106927
Time taken by simulation: 612 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 1199719.8486328125 80871.45884831746
End of simulation:  Mini-batch time (usec) = 5670469
Min send: 10000000, max send 0
Min long send: 80925, max long send 104560
Min fwd: 47855, max fwd 59401; min bwd 106211, max bwd 126925
Min long fwd: 52606, max long fwd 62603; min long bwd 118442, max long bwd 125606
Time taken by simulation: 697 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 11 0 770127.8076171875 159635.3450129109
End of simulation:  Mini-batch time (usec) = 6157105
Min send: 10000000, max send 0
Min long send: 159689, max long send 184340
Min fwd: 52659, max fwd 63832; min bwd 123243, max bwd 140240
Min long fwd: 72998, max long fwd 78841; min long bwd 148211, max long bwd 154022
Time taken by simulation: 759 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 11 0 442992.24853515625 230166.85465330718
End of simulation:  Mini-batch time (usec) = 8313990
Min send: 10000000, max send 0
Min long send: 230166, max long send 255370
Min fwd: 54051, max fwd 66033; min bwd 132265, max bwd 140978
Min long fwd: 81808, max long fwd 89347; min long bwd 157650, max long bwd 164467
Time taken by simulation: 1051 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1961 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1921 microseconds

can't have 24 stages!
{1: 8.950037, 2: 7.191944, 3: 6.803052, 4: 5.670469, 6: 6.157105, 8: 8.31399, 12: 15.020502, 16: 20.148929}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13}
best config is: 4 2
expected time is 5.670469
4 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 4
stage to rank map: 0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15; --batch-size=32 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 229
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 229
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Parent process ID: 22545 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 6 0 6165271.97265625 0
End of simulation:  Mini-batch time (usec) = 8950037
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164547, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 49 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 14 0 2816083.251953125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7191944
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82709, max long fwd 89657; min long bwd 149926, max long bwd 157738
Time taken by simulation: 240 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 21 0 1916562.744140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6803052
Min send: 10000000, max send 0
Min long send: 38109, max long send 62549
Min fwd: 45773, max fwd 59456; min bwd 92874, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 96930, max long bwd 106927
Time taken by simulation: 608 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 1199719.8486328125 80871.45884831746
End of simulation:  Mini-batch time (usec) = 5670469
Min send: 10000000, max send 0
Min long send: 80925, max long send 104560
Min fwd: 47855, max fwd 59401; min bwd 106211, max bwd 126925
Min long fwd: 52606, max long fwd 62603; min long bwd 118442, max long bwd 125606
Time taken by simulation: 700 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 11 0 770127.8076171875 159635.3450129109
End of simulation:  Mini-batch time (usec) = 6157105
Min send: 10000000, max send 0
Min long send: 159689, max long send 184340
Min fwd: 52659, max fwd 63832; min bwd 123243, max bwd 140240
Min long fwd: 72998, max long fwd 78841; min long bwd 148211, max long bwd 154022
Time taken by simulation: 763 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 11 0 442992.24853515625 230166.85465330718
End of simulation:  Mini-batch time (usec) = 8313990
Min send: 10000000, max send 0
Min long send: 230166, max long send 255370
Min fwd: 54051, max fwd 66033; min bwd 132265, max bwd 140978
Min long fwd: 81808, max long fwd 89347; min long bwd 157650, max long bwd 164467
Time taken by simulation: 1113 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1901 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 1927 microseconds

can't have 24 stages!
{1: 8.950037, 2: 7.191944, 3: 6.803052, 4: 5.670469, 6: 6.157105, 8: 8.31399, 12: 15.020502, 16: 20.148929}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13}
best config is: 4 2
expected time is 5.670469
4 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 4
stage to rank map: 0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15; --batch-size=32 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 229
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 229
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12;1,5,9,13;2,6,10,14;3,7,11,15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Process done with return code 1
Parent process ID: 22814 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 5758860.83984375 0
End of simulation:  Mini-batch time (usec) = 9470530
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164303, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 66 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 18 0 2725911.1328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8250798
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82380, max long fwd 89657; min long bwd 149642, max long bwd 157738
Time taken by simulation: 305 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 25 0 1731851.1962890625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7585451
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 45773, max fwd 59456; min bwd 92460, max bwd 103910
Min long fwd: 56519, max long fwd 64913; min long bwd 96930, max long bwd 104722
Time taken by simulation: 773 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1050667.96875 80871.45884831746
End of simulation:  Mini-batch time (usec) = 6512804
Min send: 10000000, max send 0
Min long send: 80871, max long send 103337
Min fwd: 47099, max fwd 59401; min bwd 106731, max bwd 128252
Min long fwd: 54031, max long fwd 62603; min long bwd 118588, max long bwd 125970
Time taken by simulation: 966 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 16 0 548680.6640625 159635.3450129109
End of simulation:  Mini-batch time (usec) = 7062870
Min send: 10000000, max send 0
Min long send: 159689, max long send 182101
Min fwd: 52659, max fwd 63293; min bwd 123973, max bwd 139940
Min long fwd: 73341, max long fwd 81486; min long bwd 146196, max long bwd 154690
Time taken by simulation: 1115 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 0 230166.85465330718
End of simulation:  Mini-batch time (usec) = 10536264
Min send: 10000000, max send 0
Min long send: 230303, max long send 257328
Min fwd: 54967, max fwd 67591; min bwd 130849, max bwd 143444
Min long fwd: 78331, max long fwd 89630; min long bwd 156920, max long bwd 164176
Time taken by simulation: 2207 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1983 microseconds

can't have 16 stages!
can't have 24 stages!
{1: 9.47053, 2: 8.250798, 3: 7.585451, 4: 6.512804, 6: 7.06287, 8: 10.536264, 12: 15.020502}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10}
best config is: 4 2
expected time is 6.512804
3 per stage
12 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 3
stage to rank map: 0,4,8;1,5,9;2,6,10;3,7,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8;1,5,9;2,6,10;3,7,11; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 229
using world size: 12 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 229
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8;1,5,9;2,6,10;3,7,11;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 12
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Process done with return code 1
Parent process ID: 23069 node: 172.31.4.79
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 5758860.83984375 0
End of simulation:  Mini-batch time (usec) = 9470530
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 164303, max long fwd 168133; min long bwd 294106, max long bwd 302385
Time taken by simulation: 59 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 18 0 2725911.1328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8250798
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 76538, max fwd 84520; min bwd 141071, max bwd 148651
Min long fwd: 82380, max long fwd 89657; min long bwd 149642, max long bwd 157738
Time taken by simulation: 306 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 25 0 1731851.1962890625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7585451
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 45773, max fwd 59456; min bwd 92460, max bwd 103910
Min long fwd: 56519, max long fwd 64913; min long bwd 96930, max long bwd 104722
Time taken by simulation: 748 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1050667.96875 80871.45884831746
End of simulation:  Mini-batch time (usec) = 6512804
Min send: 10000000, max send 0
Min long send: 80871, max long send 103337
Min fwd: 47099, max fwd 59401; min bwd 106731, max bwd 128252
Min long fwd: 54031, max long fwd 62603; min long bwd 118588, max long bwd 125970
Time taken by simulation: 912 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 16 0 548680.6640625 159635.3450129109
End of simulation:  Mini-batch time (usec) = 7062870
Min send: 10000000, max send 0
Min long send: 159689, max long send 182101
Min fwd: 52659, max fwd 63293; min bwd 123973, max bwd 139940
Min long fwd: 73341, max long fwd 81486; min long bwd 146196, max long bwd 154690
Time taken by simulation: 1145 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 0 230166.85465330718
End of simulation:  Mini-batch time (usec) = 10536264
Min send: 10000000, max send 0
Min long send: 230303, max long send 257328
Min fwd: 54967, max fwd 67591; min bwd 130849, max bwd 143444
Min long fwd: 78331, max long fwd 89630; min long bwd 156920, max long bwd 164176
Time taken by simulation: 2164 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 13 0 0 361377.8571928701
End of simulation:  Mini-batch time (usec) = 15020502
Min send: 10000000, max send 0
Min long send: 361431, max long send 386584
Min fwd: 61398, max fwd 73009; min bwd 138510, max bwd 150849
Min long fwd: 109818, max long fwd 116156; min long bwd 179680, max long bwd 186710
Time taken by simulation: 1996 microseconds

can't have 16 stages!
can't have 24 stages!
{1: 9.47053, 2: 8.250798, 3: 7.585451, 4: 6.512804, 6: 7.06287, 8: 10.536264, 12: 15.020502}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10}
best config is: 4 2
expected time is 6.512804
3 per stage
12 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 3
stage to rank map: 0,4,8;1,5,9;2,6,10;3,7,11;
World size is 12
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8;1,5,9;2,6,10;3,7,11; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 229
using world size: 12 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 229
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8;1,5,9;2,6,10;3,7,11;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 12
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Process done with return code 1
