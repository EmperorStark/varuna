Parent process ID: 23096 node: 172.31.28.108
48 cutpoints
Stages 1
13 179408597504.00003
7 107169514393.59993
4 71132724223.99994
2 45211359129.60004
Predicted microbatch size for 1: 1
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 4 0 5914824.70703125 0
End of simulation:  Mini-batch time (usec) = 7779954
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 165849, max long fwd 168133; min long bwd 297881, max long bwd 302385
Time taken by simulation: 44 microseconds

Stages 2
13 91326429286.39996
7 54442902425.59998
4 36011032883.20001
2 22710326886.4
Predicted microbatch size for 2: 1
comm size 1638400
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 9 0 3040763.671875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5992953
Min send: 10000000, max send 0
Min long send: 38387, max long send 59301
Min fwd: 78572, max fwd 83279; min bwd 141605, max bwd 146473
Min long fwd: 83859, max long fwd 89657; min long bwd 149487, max long bwd 154838
Time taken by simulation: 171 microseconds

Stages 3
13 62378072166.39999
7 37276345241.600006
4 24698690662.400005
2 15681540915.199997
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 14 0 2006653.3203125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5367030
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 60689; min bwd 93028, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 98397, max long bwd 102857
Time taken by simulation: 418 microseconds

Stages 4
13 47854723993.6
7 28643536588.800003
4 19017705369.600002
2 12167147929.599998
3 15282454937.599998
Predicted microbatch size for 4: 2
comm size 3276800
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 9 0 1481951.2939453125 80871.45884831746
End of simulation:  Mini-batch time (usec) = 4325287
Min send: 10000000, max send 0
Min long send: 80953, max long send 103337
Min fwd: 48269, max fwd 58933; min bwd 108965, max bwd 128153
Min long fwd: 53770, max long fwd 61460; min long bwd 118329, max long bwd 124910
Time taken by simulation: 517 microseconds

Stages 6
13 33331375820.800003
7 20010727936.0
4 13336720076.8
5 15504158822.400002
Predicted microbatch size for 6: 4
comm size 6553600
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 8 0 876895.5078125 159635.3450129109
End of simulation:  Mini-batch time (usec) = 5601136
Min send: 10000000, max send 0
Min long send: 159772, max long send 184340
Min fwd: 53241, max fwd 62601; min bwd 123697, max bwd 141328
Min long fwd: 72998, max long fwd 79077; min long bwd 148883, max long bwd 151161
Time taken by simulation: 553 microseconds

Stages 8
13 26069701734.4
7 15694323609.6
4 10496227430.4
5 12173858304.0
6 13758925312.0
Predicted microbatch size for 8: 6
comm size 9830400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 7 0 610855.46875 230166.85465330718
End of simulation:  Mini-batch time (usec) = 7505355
Min send: 10000000, max send 0
Min long send: 230248, max long send 254871
Min fwd: 56011, max fwd 66812; min bwd 129842, max bwd 141350
Min long fwd: 83341, max long fwd 88072; min long bwd 159951, max long bwd 163059
Time taken by simulation: 717 microseconds

Stages 12
13 18808027648.0
7 11377919283.2
10 15096853196.8
11 16232151142.4
Predicted microbatch size for 12: 10
comm size 16384000
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 7 0 343797.36328125 361377.8571928701
End of simulation:  Mini-batch time (usec) = 13548777
Min send: 10000000, max send 0
Min long send: 361459, max long send 385871
Min fwd: 61802, max fwd 72928; min bwd 141491, max bwd 152040
Min long fwd: 108389, max long fwd 112365; min long bwd 179662, max long bwd 186130
Time taken by simulation: 1000 microseconds

Stages 16
13 15177190604.8
19 21144245452.8
16 18160685260.8
14 16174148608.0
Predicted microbatch size for 16: 13
comm size 21299200
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 10 0 0 425876.58448885847
End of simulation:  Mini-batch time (usec) = 20148929
Min send: 10000000, max send 0
Min long send: 425930, max long send 455880
Min fwd: 58146, max fwd 69239; min bwd 134282, max bwd 149519
Min long fwd: 116097, max long fwd 122130; min long bwd 192822, max long bwd 197216
Time taken by simulation: 2112 microseconds

Stages 24
13 11546353561.599998
19 16037201817.599998
16 13791433625.599998
17 14539214745.599998
18 15288667033.599998
Predicted microbatch size for 24: 17
comm size 27852800
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 8 0 0 548143.2004641461
End of simulation:  Mini-batch time (usec) = 33444830
Min send: 10000000, max send 0
Min long send: 548262, max long send 578147
Min fwd: 49763, max fwd 61192; min bwd 113823, max bwd 127856
Min long fwd: 124104, max long fwd 128023; min long bwd 187948, max long bwd 195033
Time taken by simulation: 2335 microseconds

{1: 7.779954, 2: 5.992953, 3: 5.36703, 4: 4.325287, 6: 5.601136, 8: 7.505355, 12: 13.548777, 16: 20.148929, 24: 33.44483}
{1: 1, 2: 1, 3: 1, 4: 2, 6: 4, 8: 6, 12: 10, 16: 13, 24: 17}
best config is: 4 2
expected time is 4.325287
7 per stage
28 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 2
data depth: 7
stage to rank map: 0,4,8,12,16,20,24;1,5,9,13,17,21,25;2,6,10,14,18,22,26;3,7,11,15,19,23,27;
World size is 28
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=2 --local_rank=0 --stage_to_rank_map=0,4,8,12,16,20,24;1,5,9,13,17,21,25;2,6,10,14,18,22,26;3,7,11,15,19,23,27; --batch-size=18 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 384
using world size: 28 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 18
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 2
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 384
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16,20,24;1,5,9,13,17,21,25;2,6,10,14,18,22,26;3,7,11,15,19,23,27;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 28
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
