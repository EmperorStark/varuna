Parent process ID: 28519 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14509 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 64
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... None
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1200000
    validation: 1280
    test:       640
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 4.064189434051514
SHARED WEIGHTS ARE
[(0, 15)]
this rank  0 is part of pipeline replica  0
64 chunks
 > number of parameters on model parallel rank 0: 612999168
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
WARNING: could not find the metadata file s3://spot-checkpoints/gpt/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
 > finished loading checkpoint in 0.201 seconds
setting training data start iteration to 0
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 74061.33 | train/valid/test data iterators: 154.17
training ...
START iteration 0, CKPT_AND_STOP: False
Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:12:33.710093] iteration        1/   18750 | elapsed time per iteration (ms): 19045.7 | learning rate: 8.000E-07 | lm loss: 1.164416E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 1 iterations memory (MB) | allocated: 8187.68212890625 | max allocated: 11696.29150390625 | reserved: 12088.0 | max reserved: 12088.0
time (ms) | optimizer: 37.87 | batch generator: 6.25
START iteration 1, CKPT_AND_STOP: False
Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:12:41.431284] iteration        2/   18750 | elapsed time per iteration (ms): 7721.2 | learning rate: 1.600E-06 | lm loss: 1.163565E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 3.01
START iteration 2, CKPT_AND_STOP: False
Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:12:49.192703] iteration        3/   18750 | elapsed time per iteration (ms): 7761.4 | learning rate: 2.400E-06 | lm loss: 1.160667E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 6.94
START iteration 3, CKPT_AND_STOP: False
Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:12:57.068891] iteration        4/   18750 | elapsed time per iteration (ms): 7876.2 | learning rate: 3.200E-06 | lm loss: 1.154187E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.94 | batch generator: 2.02
START iteration 4, CKPT_AND_STOP: False
Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:04.921573] iteration        5/   18750 | elapsed time per iteration (ms): 7852.7 | learning rate: 4.000E-06 | lm loss: 1.149327E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.06 | batch generator: 1.91
START iteration 5, CKPT_AND_STOP: False
Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:11.948224] iteration        6/   18750 | elapsed time per iteration (ms): 7026.6 | learning rate: 4.800E-06 | lm loss: 1.144057E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.24
START iteration 6, CKPT_AND_STOP: False
Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:18.689926] iteration        7/   18750 | elapsed time per iteration (ms): 6741.7 | learning rate: 5.600E-06 | lm loss: 1.139307E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.92 | batch generator: 10.46
START iteration 7, CKPT_AND_STOP: False
Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:25.439397] iteration        8/   18750 | elapsed time per iteration (ms): 6749.5 | learning rate: 6.400E-06 | lm loss: 1.135469E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 2.28
START iteration 8, CKPT_AND_STOP: False
Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:32.178620] iteration        9/   18750 | elapsed time per iteration (ms): 6739.2 | learning rate: 7.200E-06 | lm loss: 1.132388E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.94 | batch generator: 2.14
START iteration 9, CKPT_AND_STOP: False
Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:38.849881] iteration       10/   18750 | elapsed time per iteration (ms): 6671.2 | learning rate: 8.000E-06 | lm loss: 1.129623E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.93 | batch generator: 9.24
START iteration 10, CKPT_AND_STOP: False
Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:45.516872] iteration       11/   18750 | elapsed time per iteration (ms): 6667.0 | learning rate: 8.800E-06 | lm loss: 1.126814E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 2.02
START iteration 11, CKPT_AND_STOP: False
Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:52.291749] iteration       12/   18750 | elapsed time per iteration (ms): 6774.9 | learning rate: 9.600E-06 | lm loss: 1.124651E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.88
START iteration 12, CKPT_AND_STOP: False
Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:13:59.240958] iteration       13/   18750 | elapsed time per iteration (ms): 6949.2 | learning rate: 1.040E-05 | lm loss: 1.123192E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 2.30
START iteration 13, CKPT_AND_STOP: False
Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:05.922978] iteration       14/   18750 | elapsed time per iteration (ms): 6682.0 | learning rate: 1.120E-05 | lm loss: 1.121272E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.82
START iteration 14, CKPT_AND_STOP: False
Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:13.077032] iteration       15/   18750 | elapsed time per iteration (ms): 7154.0 | learning rate: 1.200E-05 | lm loss: 1.118078E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 3.21
START iteration 15, CKPT_AND_STOP: False
Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:19.781167] iteration       16/   18750 | elapsed time per iteration (ms): 6704.1 | learning rate: 1.280E-05 | lm loss: 1.116423E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.94 | batch generator: 2.11
START iteration 16, CKPT_AND_STOP: False
Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:26.891074] iteration       17/   18750 | elapsed time per iteration (ms): 7109.9 | learning rate: 1.360E-05 | lm loss: 1.114724E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 1.89
START iteration 17, CKPT_AND_STOP: False
Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:33.650683] iteration       18/   18750 | elapsed time per iteration (ms): 6759.6 | learning rate: 1.440E-05 | lm loss: 1.112900E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 1.91
START iteration 18, CKPT_AND_STOP: False
Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:40.431981] iteration       19/   18750 | elapsed time per iteration (ms): 6781.3 | learning rate: 1.520E-05 | lm loss: 1.111316E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 1.98
START iteration 19, CKPT_AND_STOP: False
Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:47.485523] iteration       20/   18750 | elapsed time per iteration (ms): 7053.5 | learning rate: 1.600E-05 | lm loss: 1.109174E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 9.06
START iteration 20, CKPT_AND_STOP: False
Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:14:54.213832] iteration       21/   18750 | elapsed time per iteration (ms): 6728.3 | learning rate: 1.680E-05 | lm loss: 1.105913E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.94 | batch generator: 1.97
START iteration 21, CKPT_AND_STOP: False
Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:00.842985] iteration       22/   18750 | elapsed time per iteration (ms): 6629.1 | learning rate: 1.760E-05 | lm loss: 1.104497E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 2.02
START iteration 22, CKPT_AND_STOP: False
Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:07.732522] iteration       23/   18750 | elapsed time per iteration (ms): 6889.5 | learning rate: 1.840E-05 | lm loss: 1.101732E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 2.08
START iteration 23, CKPT_AND_STOP: False
Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:14.712094] iteration       24/   18750 | elapsed time per iteration (ms): 6979.5 | learning rate: 1.920E-05 | lm loss: 1.099319E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 10.84
START iteration 24, CKPT_AND_STOP: False
Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:21.409873] iteration       25/   18750 | elapsed time per iteration (ms): 6697.8 | learning rate: 2.000E-05 | lm loss: 1.097681E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.25
START iteration 25, CKPT_AND_STOP: False
Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:28.293650] iteration       26/   18750 | elapsed time per iteration (ms): 6883.8 | learning rate: 2.080E-05 | lm loss: 1.095490E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 11.15
START iteration 26, CKPT_AND_STOP: False
Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:35.074981] iteration       27/   18750 | elapsed time per iteration (ms): 6781.3 | learning rate: 2.160E-05 | lm loss: 1.093244E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.93 | batch generator: 2.04
START iteration 27, CKPT_AND_STOP: False
Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:41.818419] iteration       28/   18750 | elapsed time per iteration (ms): 6743.4 | learning rate: 2.240E-05 | lm loss: 1.090836E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.93 | batch generator: 2.31
START iteration 28, CKPT_AND_STOP: False
Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:48.457333] iteration       29/   18750 | elapsed time per iteration (ms): 6638.9 | learning rate: 2.320E-05 | lm loss: 1.088748E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 2.02
START iteration 29, CKPT_AND_STOP: False
Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:15:55.178931] iteration       30/   18750 | elapsed time per iteration (ms): 6721.6 | learning rate: 2.400E-05 | lm loss: 1.086798E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.08
START iteration 30, CKPT_AND_STOP: False
Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:02.038344] iteration       31/   18750 | elapsed time per iteration (ms): 6859.3 | learning rate: 2.480E-05 | lm loss: 1.084773E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.07
START iteration 31, CKPT_AND_STOP: False
Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:08.687837] iteration       32/   18750 | elapsed time per iteration (ms): 6649.5 | learning rate: 2.560E-05 | lm loss: 1.082717E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.24
START iteration 32, CKPT_AND_STOP: False
Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:15.512139] iteration       33/   18750 | elapsed time per iteration (ms): 6824.3 | learning rate: 2.640E-05 | lm loss: 1.080583E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.12
START iteration 33, CKPT_AND_STOP: False
Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:22.484703] iteration       34/   18750 | elapsed time per iteration (ms): 6972.5 | learning rate: 2.720E-05 | lm loss: 1.078968E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 9.98
START iteration 34, CKPT_AND_STOP: False
Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:29.275061] iteration       35/   18750 | elapsed time per iteration (ms): 6790.3 | learning rate: 2.800E-05 | lm loss: 1.077066E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.95 | batch generator: 2.12
START iteration 35, CKPT_AND_STOP: False
Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:35.864726] iteration       36/   18750 | elapsed time per iteration (ms): 6589.6 | learning rate: 2.880E-05 | lm loss: 1.075798E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.92 | batch generator: 2.00
START iteration 36, CKPT_AND_STOP: False
Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:16:42.667325] iteration       37/   18750 | elapsed time per iteration (ms): 6802.6 | learning rate: 2.960E-05 | lm loss: 1.074479E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.93 | batch generator: 2.03
START iteration 37, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 38, CKPT_AND_STOP: True, flag: tensor([5], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      38 to s3://spot-checkpoints/gpt/iter_0000038/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000038/mp_rank_00/model_optim_rng.pt
Opt ckpt time 82.3778805732727
Process done with return code 0
Parent process ID: 30006 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 7319 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 38
using world size: 32 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 38
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 32
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1200000
    validation: 1280
    test:       640
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 2.5509092807769775
SHARED WEIGHTS ARE
[(0, 15)]
this rank  0 is part of pipeline replica  0
32 chunks
 > number of parameters on model parallel rank 0: 612999168
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000038/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000038/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 56.195 seconds
setting training data start iteration to 38
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 129437.46 | train/valid/test data iterators: 167.41
training ...
START iteration 38, CKPT_AND_STOP: False
Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:20:57.801952] iteration       39/   18750 | elapsed time per iteration (ms): 18715.6 | learning rate: 3.120E-05 | lm loss: 1.074588E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 39 iterations memory (MB) | allocated: 8187.05712890625 | max allocated: 12865.666015625 | reserved: 13574.0 | max reserved: 13574.0
time (ms) | optimizer: 31.23 | batch generator: 8.15
START iteration 39, CKPT_AND_STOP: False
Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:04.879460] iteration       40/   18750 | elapsed time per iteration (ms): 7077.5 | learning rate: 3.200E-05 | lm loss: 1.073071E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.45
START iteration 40, CKPT_AND_STOP: False
Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:11.850783] iteration       41/   18750 | elapsed time per iteration (ms): 6971.3 | learning rate: 3.280E-05 | lm loss: 1.071151E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.15 | batch generator: 8.86
START iteration 41, CKPT_AND_STOP: False
Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:18.790482] iteration       42/   18750 | elapsed time per iteration (ms): 6939.7 | learning rate: 3.360E-05 | lm loss: 1.070310E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.17 | batch generator: 2.03
START iteration 42, CKPT_AND_STOP: False
Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:25.754418] iteration       43/   18750 | elapsed time per iteration (ms): 6963.9 | learning rate: 3.440E-05 | lm loss: 1.069781E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.22 | batch generator: 1.97
START iteration 43, CKPT_AND_STOP: False
Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:31.533485] iteration       44/   18750 | elapsed time per iteration (ms): 5779.1 | learning rate: 3.520E-05 | lm loss: 1.069378E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.30
START iteration 44, CKPT_AND_STOP: False
Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:37.431334] iteration       45/   18750 | elapsed time per iteration (ms): 5897.8 | learning rate: 3.600E-05 | lm loss: 1.068611E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.06
START iteration 45, CKPT_AND_STOP: False
Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:43.387534] iteration       46/   18750 | elapsed time per iteration (ms): 5956.2 | learning rate: 3.680E-05 | lm loss: 1.068471E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 2.11
START iteration 46, CKPT_AND_STOP: False
Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:49.311300] iteration       47/   18750 | elapsed time per iteration (ms): 5923.8 | learning rate: 3.760E-05 | lm loss: 1.067566E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.96
START iteration 47, CKPT_AND_STOP: False
Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:21:55.211255] iteration       48/   18750 | elapsed time per iteration (ms): 5899.9 | learning rate: 3.840E-05 | lm loss: 1.067166E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 1.88
START iteration 48, CKPT_AND_STOP: False
Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:01.077962] iteration       49/   18750 | elapsed time per iteration (ms): 5866.7 | learning rate: 3.920E-05 | lm loss: 1.067166E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.80
START iteration 49, CKPT_AND_STOP: False
Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:07.087664] iteration       50/   18750 | elapsed time per iteration (ms): 6009.7 | learning rate: 4.000E-05 | lm loss: 1.066819E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 1.92
START iteration 50, CKPT_AND_STOP: False
Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:12.989427] iteration       51/   18750 | elapsed time per iteration (ms): 5901.7 | learning rate: 4.080E-05 | lm loss: 1.066823E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.05
START iteration 51, CKPT_AND_STOP: False
Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:18.844783] iteration       52/   18750 | elapsed time per iteration (ms): 5855.3 | learning rate: 4.160E-05 | lm loss: 1.066825E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.08
START iteration 52, CKPT_AND_STOP: False
Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:24.715002] iteration       53/   18750 | elapsed time per iteration (ms): 5870.2 | learning rate: 4.240E-05 | lm loss: 1.066772E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.11
START iteration 53, CKPT_AND_STOP: False
Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:30.582796] iteration       54/   18750 | elapsed time per iteration (ms): 5867.8 | learning rate: 4.320E-05 | lm loss: 1.066347E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.03
START iteration 54, CKPT_AND_STOP: False
Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:36.468556] iteration       55/   18750 | elapsed time per iteration (ms): 5885.7 | learning rate: 4.400E-05 | lm loss: 1.066315E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.19
START iteration 55, CKPT_AND_STOP: False
Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:42.439300] iteration       56/   18750 | elapsed time per iteration (ms): 5970.7 | learning rate: 4.480E-05 | lm loss: 1.066102E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.67
START iteration 56, CKPT_AND_STOP: False
Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:48.395874] iteration       57/   18750 | elapsed time per iteration (ms): 5956.6 | learning rate: 4.560E-05 | lm loss: 1.065951E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.04
START iteration 57, CKPT_AND_STOP: False
Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:22:54.969141] iteration       58/   18750 | elapsed time per iteration (ms): 6573.3 | learning rate: 4.640E-05 | lm loss: 1.066068E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.04
START iteration 58, CKPT_AND_STOP: False
Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:00.856968] iteration       59/   18750 | elapsed time per iteration (ms): 5887.8 | learning rate: 4.720E-05 | lm loss: 1.066336E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 10.41
START iteration 59, CKPT_AND_STOP: False
Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:06.844005] iteration       60/   18750 | elapsed time per iteration (ms): 5987.0 | learning rate: 4.800E-05 | lm loss: 1.066025E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.94
START iteration 60, CKPT_AND_STOP: False
Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:12.749366] iteration       61/   18750 | elapsed time per iteration (ms): 5905.3 | learning rate: 4.880E-05 | lm loss: 1.065901E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.22
START iteration 61, CKPT_AND_STOP: False
Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:18.818674] iteration       62/   18750 | elapsed time per iteration (ms): 6069.3 | learning rate: 4.960E-05 | lm loss: 1.065853E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.01
START iteration 62, CKPT_AND_STOP: False
Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:24.757876] iteration       63/   18750 | elapsed time per iteration (ms): 5939.2 | learning rate: 5.040E-05 | lm loss: 1.066109E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.07
START iteration 63, CKPT_AND_STOP: False
Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:30.705207] iteration       64/   18750 | elapsed time per iteration (ms): 5947.3 | learning rate: 5.120E-05 | lm loss: 1.066118E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.17
START iteration 64, CKPT_AND_STOP: False
Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:36.773851] iteration       65/   18750 | elapsed time per iteration (ms): 6068.6 | learning rate: 5.200E-05 | lm loss: 1.065915E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.22
START iteration 65, CKPT_AND_STOP: False
Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:42.695797] iteration       66/   18750 | elapsed time per iteration (ms): 5921.9 | learning rate: 5.280E-05 | lm loss: 1.065652E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.13
START iteration 66, CKPT_AND_STOP: False
Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:48.685254] iteration       67/   18750 | elapsed time per iteration (ms): 5989.4 | learning rate: 5.360E-05 | lm loss: 1.065789E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.96
START iteration 67, CKPT_AND_STOP: False
Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:23:54.642052] iteration       68/   18750 | elapsed time per iteration (ms): 5956.8 | learning rate: 5.440E-05 | lm loss: 1.065608E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.02
START iteration 68, CKPT_AND_STOP: False
Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:00.701789] iteration       69/   18750 | elapsed time per iteration (ms): 6059.7 | learning rate: 5.520E-05 | lm loss: 1.065330E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 15.16
START iteration 69, CKPT_AND_STOP: False
Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:06.665385] iteration       70/   18750 | elapsed time per iteration (ms): 5963.6 | learning rate: 5.600E-05 | lm loss: 1.065446E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.08
START iteration 70, CKPT_AND_STOP: False
Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:12.574261] iteration       71/   18750 | elapsed time per iteration (ms): 5908.8 | learning rate: 5.680E-05 | lm loss: 1.065648E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.01
START iteration 71, CKPT_AND_STOP: False
Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:18.524698] iteration       72/   18750 | elapsed time per iteration (ms): 5950.4 | learning rate: 5.760E-05 | lm loss: 1.065504E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.38
START iteration 72, CKPT_AND_STOP: False
Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:24.423614] iteration       73/   18750 | elapsed time per iteration (ms): 5898.9 | learning rate: 5.840E-05 | lm loss: 1.065503E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.86
START iteration 73, CKPT_AND_STOP: False
Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:30.375740] iteration       74/   18750 | elapsed time per iteration (ms): 5952.1 | learning rate: 5.920E-05 | lm loss: 1.065805E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.95
START iteration 74, CKPT_AND_STOP: False
Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:36.312152] iteration       75/   18750 | elapsed time per iteration (ms): 5936.4 | learning rate: 6.000E-05 | lm loss: 1.065639E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.10
START iteration 75, CKPT_AND_STOP: False
Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:42.277113] iteration       76/   18750 | elapsed time per iteration (ms): 5964.9 | learning rate: 6.080E-05 | lm loss: 1.065882E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.99
START iteration 76, CKPT_AND_STOP: False
Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:48.248507] iteration       77/   18750 | elapsed time per iteration (ms): 5971.4 | learning rate: 6.160E-05 | lm loss: 1.066012E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.13
START iteration 77, CKPT_AND_STOP: False
Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:24:54.209471] iteration       78/   18750 | elapsed time per iteration (ms): 5960.9 | learning rate: 6.240E-05 | lm loss: 1.065682E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 1.72
START iteration 78, CKPT_AND_STOP: False
Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:25:00.078961] iteration       79/   18750 | elapsed time per iteration (ms): 5869.5 | learning rate: 6.320E-05 | lm loss: 1.065989E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.17
START iteration 79, CKPT_AND_STOP: False
Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:25:06.163599] iteration       80/   18750 | elapsed time per iteration (ms): 6084.6 | learning rate: 6.400E-05 | lm loss: 1.065800E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.09 | batch generator: 2.05
START iteration 80, CKPT_AND_STOP: False
Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:25:12.095264] iteration       81/   18750 | elapsed time per iteration (ms): 5931.7 | learning rate: 6.480E-05 | lm loss: 1.065779E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 11.43
START iteration 81, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 82, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      82 to s3://spot-checkpoints/gpt/iter_0000082/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000082/mp_rank_00/model_optim_rng.pt
Opt ckpt time 54.54545497894287
Process done with return code 1
Parent process ID: 31372 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14427 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 82
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 64
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 82
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1200000
    validation: 1280
    test:       640
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
dry run time 2.163632869720459
SHARED WEIGHTS ARE
[(0, 15)]
this rank  0 is part of pipeline replica  0
64 chunks
 > number of parameters on model parallel rank 0: 612999168
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000082/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000082/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 60.966 seconds
setting training data start iteration to 82
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 133986.31 | train/valid/test data iterators: 201.15
training ...
Process done with return code 0
Parent process ID: 32146 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14518 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 82
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 64
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 82
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1200000
    validation: 1280
    test:       640
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 2.4499692916870117
SHARED WEIGHTS ARE
[(0, 15)]
this rank  0 is part of pipeline replica  0
64 chunks
 > number of parameters on model parallel rank 0: 612999168
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000082/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000082/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 66.610 seconds
setting training data start iteration to 82
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 139141.94 | train/valid/test data iterators: 176.95
training ...
START iteration 82, CKPT_AND_STOP: False
Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:32:12.538065] iteration       83/   18750 | elapsed time per iteration (ms): 19364.0 | learning rate: 6.640E-05 | lm loss: 1.067570E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 83 iterations memory (MB) | allocated: 8187.68212890625 | max allocated: 12866.291015625 | reserved: 13574.0 | max reserved: 13574.0
time (ms) | optimizer: 31.12 | batch generator: 8.67
START iteration 83, CKPT_AND_STOP: False
Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:32:21.767760] iteration       84/   18750 | elapsed time per iteration (ms): 9229.7 | learning rate: 6.720E-05 | lm loss: 1.066753E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.19 | batch generator: 3.07
START iteration 84, CKPT_AND_STOP: False
Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:32:30.041601] iteration       85/   18750 | elapsed time per iteration (ms): 8273.8 | learning rate: 6.800E-05 | lm loss: 1.065968E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.16
START iteration 85, CKPT_AND_STOP: False
Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:32:38.396995] iteration       86/   18750 | elapsed time per iteration (ms): 8355.4 | learning rate: 6.880E-05 | lm loss: 1.065968E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.07 | batch generator: 1.86
START iteration 86, CKPT_AND_STOP: False
Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:32:46.681606] iteration       87/   18750 | elapsed time per iteration (ms): 8284.6 | learning rate: 6.960E-05 | lm loss: 1.066061E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.37
START iteration 87, CKPT_AND_STOP: False
Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:32:54.042127] iteration       88/   18750 | elapsed time per iteration (ms): 7360.5 | learning rate: 7.040E-05 | lm loss: 1.066025E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.11
START iteration 88, CKPT_AND_STOP: False
Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:01.257580] iteration       89/   18750 | elapsed time per iteration (ms): 7215.4 | learning rate: 7.120E-05 | lm loss: 1.066168E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.05 | batch generator: 1.99
START iteration 89, CKPT_AND_STOP: False
Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:08.508024] iteration       90/   18750 | elapsed time per iteration (ms): 7250.4 | learning rate: 7.200E-05 | lm loss: 1.066432E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.05 | batch generator: 2.34
START iteration 90, CKPT_AND_STOP: False
Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:15.723206] iteration       91/   18750 | elapsed time per iteration (ms): 7215.1 | learning rate: 7.280E-05 | lm loss: 1.066270E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.93
START iteration 91, CKPT_AND_STOP: False
Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:23.087254] iteration       92/   18750 | elapsed time per iteration (ms): 7364.0 | learning rate: 7.360E-05 | lm loss: 1.066137E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.11
START iteration 92, CKPT_AND_STOP: False
Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:30.365166] iteration       93/   18750 | elapsed time per iteration (ms): 7277.9 | learning rate: 7.440E-05 | lm loss: 1.066364E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.14
START iteration 93, CKPT_AND_STOP: False
Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:37.557142] iteration       94/   18750 | elapsed time per iteration (ms): 7192.0 | learning rate: 7.520E-05 | lm loss: 1.066632E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 2.17
START iteration 94, CKPT_AND_STOP: False
Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:44.964173] iteration       95/   18750 | elapsed time per iteration (ms): 7407.0 | learning rate: 7.600E-05 | lm loss: 1.066389E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 2.10
START iteration 95, CKPT_AND_STOP: False
Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:52.179691] iteration       96/   18750 | elapsed time per iteration (ms): 7215.5 | learning rate: 7.680E-05 | lm loss: 1.066517E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.10
START iteration 96, CKPT_AND_STOP: False
Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:33:59.362520] iteration       97/   18750 | elapsed time per iteration (ms): 7182.8 | learning rate: 7.760E-05 | lm loss: 1.066313E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.99
START iteration 97, CKPT_AND_STOP: False
Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:06.702384] iteration       98/   18750 | elapsed time per iteration (ms): 7339.8 | learning rate: 7.840E-05 | lm loss: 1.066405E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.20
START iteration 98, CKPT_AND_STOP: False
Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:14.035982] iteration       99/   18750 | elapsed time per iteration (ms): 7333.6 | learning rate: 7.920E-05 | lm loss: 1.066461E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.30
START iteration 99, CKPT_AND_STOP: False
Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:21.234434] iteration      100/   18750 | elapsed time per iteration (ms): 7198.4 | learning rate: 8.000E-05 | lm loss: 1.066566E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 5.81
START iteration 100, CKPT_AND_STOP: False
Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:28.599025] iteration      101/   18750 | elapsed time per iteration (ms): 7364.6 | learning rate: 8.080E-05 | lm loss: 1.066558E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.96
START iteration 101, CKPT_AND_STOP: False
Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:35.765919] iteration      102/   18750 | elapsed time per iteration (ms): 7166.9 | learning rate: 8.160E-05 | lm loss: 1.066603E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.14
START iteration 102, CKPT_AND_STOP: False
Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:43.020747] iteration      103/   18750 | elapsed time per iteration (ms): 7254.8 | learning rate: 8.240E-05 | lm loss: 1.066640E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.00
START iteration 103, CKPT_AND_STOP: False
Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:50.352699] iteration      104/   18750 | elapsed time per iteration (ms): 7331.9 | learning rate: 8.320E-05 | lm loss: 1.066750E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.06
START iteration 104, CKPT_AND_STOP: False
Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:34:57.621595] iteration      105/   18750 | elapsed time per iteration (ms): 7268.9 | learning rate: 8.400E-05 | lm loss: 1.066858E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.13
START iteration 105, CKPT_AND_STOP: False
Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:04.992216] iteration      106/   18750 | elapsed time per iteration (ms): 7370.6 | learning rate: 8.480E-05 | lm loss: 1.066944E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.04
START iteration 106, CKPT_AND_STOP: False
Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:12.329275] iteration      107/   18750 | elapsed time per iteration (ms): 7337.0 | learning rate: 8.560E-05 | lm loss: 1.067013E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.06
START iteration 107, CKPT_AND_STOP: False
Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:19.745141] iteration      108/   18750 | elapsed time per iteration (ms): 7415.9 | learning rate: 8.640E-05 | lm loss: 1.067159E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.05 | batch generator: 2.14
START iteration 108, CKPT_AND_STOP: False
Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:26.998772] iteration      109/   18750 | elapsed time per iteration (ms): 7253.6 | learning rate: 8.720E-05 | lm loss: 1.066964E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 7.01
START iteration 109, CKPT_AND_STOP: False
Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:34.375609] iteration      110/   18750 | elapsed time per iteration (ms): 7376.8 | learning rate: 8.800E-05 | lm loss: 1.067100E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.20
START iteration 110, CKPT_AND_STOP: False
Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:41.602709] iteration      111/   18750 | elapsed time per iteration (ms): 7227.1 | learning rate: 8.880E-05 | lm loss: 1.067065E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.24
START iteration 111, CKPT_AND_STOP: False
Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:48.997671] iteration      112/   18750 | elapsed time per iteration (ms): 7394.9 | learning rate: 8.960E-05 | lm loss: 1.067032E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.99
START iteration 112, CKPT_AND_STOP: False
Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:35:56.296884] iteration      113/   18750 | elapsed time per iteration (ms): 7299.2 | learning rate: 9.040E-05 | lm loss: 1.067099E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.19
START iteration 113, CKPT_AND_STOP: False
Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:03.578606] iteration      114/   18750 | elapsed time per iteration (ms): 7281.7 | learning rate: 9.120E-05 | lm loss: 1.067258E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 1.94
START iteration 114, CKPT_AND_STOP: False
Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:10.773005] iteration      115/   18750 | elapsed time per iteration (ms): 7194.4 | learning rate: 9.200E-05 | lm loss: 1.067070E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.17
START iteration 115, CKPT_AND_STOP: False
Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:18.050378] iteration      116/   18750 | elapsed time per iteration (ms): 7277.4 | learning rate: 9.280E-05 | lm loss: 1.067284E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.15
START iteration 116, CKPT_AND_STOP: False
Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:25.330115] iteration      117/   18750 | elapsed time per iteration (ms): 7279.7 | learning rate: 9.360E-05 | lm loss: 1.067240E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.18
START iteration 117, CKPT_AND_STOP: False
Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:32.624910] iteration      118/   18750 | elapsed time per iteration (ms): 7294.8 | learning rate: 9.440E-05 | lm loss: 1.067142E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.84
START iteration 118, CKPT_AND_STOP: False
Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:39.806075] iteration      119/   18750 | elapsed time per iteration (ms): 7181.1 | learning rate: 9.520E-05 | lm loss: 1.067391E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.20
START iteration 119, CKPT_AND_STOP: False
Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:47.108909] iteration      120/   18750 | elapsed time per iteration (ms): 7302.8 | learning rate: 9.600E-05 | lm loss: 1.067587E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.04
START iteration 120, CKPT_AND_STOP: False
Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:36:54.408160] iteration      121/   18750 | elapsed time per iteration (ms): 7299.2 | learning rate: 9.680E-05 | lm loss: 1.067825E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 10.83
START iteration 121, CKPT_AND_STOP: False
Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:01.572867] iteration      122/   18750 | elapsed time per iteration (ms): 7164.7 | learning rate: 9.760E-05 | lm loss: 1.067678E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.14
START iteration 122, CKPT_AND_STOP: False
Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:08.842196] iteration      123/   18750 | elapsed time per iteration (ms): 7269.3 | learning rate: 9.840E-05 | lm loss: 1.067470E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.88
START iteration 123, CKPT_AND_STOP: False
Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:16.136824] iteration      124/   18750 | elapsed time per iteration (ms): 7294.6 | learning rate: 9.920E-05 | lm loss: 1.067673E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.92
START iteration 124, CKPT_AND_STOP: False
Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:23.341412] iteration      125/   18750 | elapsed time per iteration (ms): 7204.6 | learning rate: 1.000E-04 | lm loss: 1.067807E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.91
START iteration 125, CKPT_AND_STOP: False
Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:30.634597] iteration      126/   18750 | elapsed time per iteration (ms): 7293.2 | learning rate: 1.008E-04 | lm loss: 1.067685E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.97
START iteration 126, CKPT_AND_STOP: False
Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:37.888720] iteration      127/   18750 | elapsed time per iteration (ms): 7254.1 | learning rate: 1.016E-04 | lm loss: 1.067679E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.00
START iteration 127, CKPT_AND_STOP: False
Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:45.256320] iteration      128/   18750 | elapsed time per iteration (ms): 7367.6 | learning rate: 1.024E-04 | lm loss: 1.067420E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 11.01
START iteration 128, CKPT_AND_STOP: False
Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:37:53.289201] iteration      129/   18750 | elapsed time per iteration (ms): 8032.9 | learning rate: 1.032E-04 | lm loss: 1.067667E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.79
START iteration 129, CKPT_AND_STOP: False
Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:00.576868] iteration      130/   18750 | elapsed time per iteration (ms): 7287.7 | learning rate: 1.040E-04 | lm loss: 1.067597E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.36
START iteration 130, CKPT_AND_STOP: False
Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:07.860204] iteration      131/   18750 | elapsed time per iteration (ms): 7283.3 | learning rate: 1.048E-04 | lm loss: 1.067715E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.48
START iteration 131, CKPT_AND_STOP: False
Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:15.125271] iteration      132/   18750 | elapsed time per iteration (ms): 7265.0 | learning rate: 1.056E-04 | lm loss: 1.067571E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.02
START iteration 132, CKPT_AND_STOP: False
Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:22.383823] iteration      133/   18750 | elapsed time per iteration (ms): 7258.5 | learning rate: 1.064E-04 | lm loss: 1.067793E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 12.93
START iteration 133, CKPT_AND_STOP: False
Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:29.649116] iteration      134/   18750 | elapsed time per iteration (ms): 7265.3 | learning rate: 1.072E-04 | lm loss: 1.067573E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.89
START iteration 134, CKPT_AND_STOP: False
Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:36.974579] iteration      135/   18750 | elapsed time per iteration (ms): 7325.4 | learning rate: 1.080E-04 | lm loss: 1.067654E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.97
START iteration 135, CKPT_AND_STOP: False
Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:44.249794] iteration      136/   18750 | elapsed time per iteration (ms): 7275.2 | learning rate: 1.088E-04 | lm loss: 1.067919E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.01
START iteration 136, CKPT_AND_STOP: False
Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:51.521708] iteration      137/   18750 | elapsed time per iteration (ms): 7271.9 | learning rate: 1.096E-04 | lm loss: 1.067692E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.87
START iteration 137, CKPT_AND_STOP: False
Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:38:58.687007] iteration      138/   18750 | elapsed time per iteration (ms): 7165.3 | learning rate: 1.104E-04 | lm loss: 1.068200E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.92
START iteration 138, CKPT_AND_STOP: False
Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:05.999918] iteration      139/   18750 | elapsed time per iteration (ms): 7312.9 | learning rate: 1.112E-04 | lm loss: 1.067898E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 7.77
START iteration 139, CKPT_AND_STOP: False
Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:13.259376] iteration      140/   18750 | elapsed time per iteration (ms): 7259.4 | learning rate: 1.120E-04 | lm loss: 1.067805E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 2.14
START iteration 140, CKPT_AND_STOP: False
Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:20.500166] iteration      141/   18750 | elapsed time per iteration (ms): 7240.8 | learning rate: 1.128E-04 | lm loss: 1.068001E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.23
START iteration 141, CKPT_AND_STOP: False
Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:27.615396] iteration      142/   18750 | elapsed time per iteration (ms): 7115.2 | learning rate: 1.136E-04 | lm loss: 1.067616E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.13
START iteration 142, CKPT_AND_STOP: False
Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:34.932904] iteration      143/   18750 | elapsed time per iteration (ms): 7317.4 | learning rate: 1.144E-04 | lm loss: 1.067765E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 2.25
START iteration 143, CKPT_AND_STOP: False
Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:42.302264] iteration      144/   18750 | elapsed time per iteration (ms): 7369.3 | learning rate: 1.152E-04 | lm loss: 1.067857E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.75
START iteration 144, CKPT_AND_STOP: False
Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:49.560240] iteration      145/   18750 | elapsed time per iteration (ms): 7258.0 | learning rate: 1.160E-04 | lm loss: 1.067785E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.05 | batch generator: 2.28
START iteration 145, CKPT_AND_STOP: False
Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:39:56.818274] iteration      146/   18750 | elapsed time per iteration (ms): 7258.0 | learning rate: 1.168E-04 | lm loss: 1.067650E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.21
START iteration 146, CKPT_AND_STOP: False
Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:04.086941] iteration      147/   18750 | elapsed time per iteration (ms): 7268.7 | learning rate: 1.176E-04 | lm loss: 1.067764E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 8.01
START iteration 147, CKPT_AND_STOP: False
Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:11.352588] iteration      148/   18750 | elapsed time per iteration (ms): 7265.6 | learning rate: 1.184E-04 | lm loss: 1.067714E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.18
START iteration 148, CKPT_AND_STOP: False
Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:18.622409] iteration      149/   18750 | elapsed time per iteration (ms): 7269.8 | learning rate: 1.192E-04 | lm loss: 1.067775E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.10
START iteration 149, CKPT_AND_STOP: False
Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:25.892812] iteration      150/   18750 | elapsed time per iteration (ms): 7270.4 | learning rate: 1.200E-04 | lm loss: 1.067678E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 8.40
START iteration 150, CKPT_AND_STOP: False
Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:33.172683] iteration      151/   18750 | elapsed time per iteration (ms): 7279.8 | learning rate: 1.208E-04 | lm loss: 1.068025E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 1.89
START iteration 151, CKPT_AND_STOP: False
Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:40.384170] iteration      152/   18750 | elapsed time per iteration (ms): 7211.5 | learning rate: 1.216E-04 | lm loss: 1.068214E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.94
START iteration 152, CKPT_AND_STOP: False
Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:47.654735] iteration      153/   18750 | elapsed time per iteration (ms): 7270.5 | learning rate: 1.224E-04 | lm loss: 1.068115E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.07
START iteration 153, CKPT_AND_STOP: False
Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:40:55.091232] iteration      154/   18750 | elapsed time per iteration (ms): 7436.5 | learning rate: 1.232E-04 | lm loss: 1.067838E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 34.97 | batch generator: 2.01
START iteration 154, CKPT_AND_STOP: False
Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:41:02.334139] iteration      155/   18750 | elapsed time per iteration (ms): 7242.9 | learning rate: 1.240E-04 | lm loss: 1.067965E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.64
START iteration 155, CKPT_AND_STOP: False
Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:41:09.621396] iteration      156/   18750 | elapsed time per iteration (ms): 7287.2 | learning rate: 1.248E-04 | lm loss: 1.067914E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.96
START iteration 156, CKPT_AND_STOP: False
Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:41:17.375484] iteration      157/   18750 | elapsed time per iteration (ms): 7754.1 | learning rate: 1.256E-04 | lm loss: 1.067692E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.95
START iteration 157, CKPT_AND_STOP: False
Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:41:24.573050] iteration      158/   18750 | elapsed time per iteration (ms): 7197.6 | learning rate: 1.264E-04 | lm loss: 1.067793E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.99
START iteration 158, CKPT_AND_STOP: False
Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:41:31.932321] iteration      159/   18750 | elapsed time per iteration (ms): 7359.3 | learning rate: 1.272E-04 | lm loss: 1.067395E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.09
START iteration 159, CKPT_AND_STOP: False
Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:41:39.250859] iteration      160/   18750 | elapsed time per iteration (ms): 7318.5 | learning rate: 1.280E-04 | lm loss: 1.067615E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.75
START iteration 160, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 161, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     161 to s3://spot-checkpoints/gpt/iter_0000161/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000161/mp_rank_00/model_optim_rng.pt
Opt ckpt time 74.01901912689209
Process done with return code 0
Parent process ID: 33722 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 7413 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 161
using world size: 32 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 161
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 32
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1200000
    validation: 1280
    test:       640
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 3.3763463497161865
SHARED WEIGHTS ARE
[(0, 15)]
this rank  0 is part of pipeline replica  0
32 chunks
 > number of parameters on model parallel rank 0: 612999168
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000161/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000161/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 64.362 seconds
setting training data start iteration to 161
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 137538.70 | train/valid/test data iterators: 185.81
training ...
START iteration 161, CKPT_AND_STOP: False
Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:45:59.054197] iteration      162/   18750 | elapsed time per iteration (ms): 18513.5 | learning rate: 1.296E-04 | lm loss: 1.069641E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 162 iterations memory (MB) | allocated: 8187.05712890625 | max allocated: 12865.666015625 | reserved: 13574.0 | max reserved: 13574.0
time (ms) | optimizer: 31.21 | batch generator: 7.20
START iteration 162, CKPT_AND_STOP: False
Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:05.982302] iteration      163/   18750 | elapsed time per iteration (ms): 6928.1 | learning rate: 1.304E-04 | lm loss: 1.068907E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.09 | batch generator: 1.83
START iteration 163, CKPT_AND_STOP: False
Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:13.000762] iteration      164/   18750 | elapsed time per iteration (ms): 7018.4 | learning rate: 1.312E-04 | lm loss: 1.068165E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.54
START iteration 164, CKPT_AND_STOP: False
Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:20.022185] iteration      165/   18750 | elapsed time per iteration (ms): 7021.4 | learning rate: 1.320E-04 | lm loss: 1.067692E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.06 | batch generator: 2.14
START iteration 165, CKPT_AND_STOP: False
Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:26.908080] iteration      166/   18750 | elapsed time per iteration (ms): 6885.9 | learning rate: 1.328E-04 | lm loss: 1.067963E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.06
START iteration 166, CKPT_AND_STOP: False
Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:33.154357] iteration      167/   18750 | elapsed time per iteration (ms): 6246.2 | learning rate: 1.336E-04 | lm loss: 1.068071E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.05
START iteration 167, CKPT_AND_STOP: False
Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:39.038485] iteration      168/   18750 | elapsed time per iteration (ms): 5884.1 | learning rate: 1.344E-04 | lm loss: 1.067766E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.82
START iteration 168, CKPT_AND_STOP: False
Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:44.962898] iteration      169/   18750 | elapsed time per iteration (ms): 5924.4 | learning rate: 1.352E-04 | lm loss: 1.068495E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.05 | batch generator: 1.86
START iteration 169, CKPT_AND_STOP: False
Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:50.837345] iteration      170/   18750 | elapsed time per iteration (ms): 5874.4 | learning rate: 1.360E-04 | lm loss: 1.068392E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 2.08
START iteration 170, CKPT_AND_STOP: False
Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:46:56.807144] iteration      171/   18750 | elapsed time per iteration (ms): 5969.8 | learning rate: 1.368E-04 | lm loss: 1.068207E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.86
START iteration 171, CKPT_AND_STOP: False
Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:02.723740] iteration      172/   18750 | elapsed time per iteration (ms): 5916.6 | learning rate: 1.376E-04 | lm loss: 1.067848E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.18
START iteration 172, CKPT_AND_STOP: False
Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:08.649627] iteration      173/   18750 | elapsed time per iteration (ms): 5925.9 | learning rate: 1.384E-04 | lm loss: 1.067622E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.97
START iteration 173, CKPT_AND_STOP: False
Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:14.482856] iteration      174/   18750 | elapsed time per iteration (ms): 5833.2 | learning rate: 1.392E-04 | lm loss: 1.067626E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.30
START iteration 174, CKPT_AND_STOP: False
Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:20.303770] iteration      175/   18750 | elapsed time per iteration (ms): 5820.9 | learning rate: 1.400E-04 | lm loss: 1.068028E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.08
START iteration 175, CKPT_AND_STOP: False
Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:26.405571] iteration      176/   18750 | elapsed time per iteration (ms): 6101.8 | learning rate: 1.408E-04 | lm loss: 1.067819E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.95
START iteration 176, CKPT_AND_STOP: False
Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:32.279949] iteration      177/   18750 | elapsed time per iteration (ms): 5874.4 | learning rate: 1.416E-04 | lm loss: 1.067729E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.20
START iteration 177, CKPT_AND_STOP: False
Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:38.103554] iteration      178/   18750 | elapsed time per iteration (ms): 5823.6 | learning rate: 1.424E-04 | lm loss: 1.067829E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 10.38
START iteration 178, CKPT_AND_STOP: False
Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:44.000868] iteration      179/   18750 | elapsed time per iteration (ms): 5897.3 | learning rate: 1.432E-04 | lm loss: 1.067314E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.30
START iteration 179, CKPT_AND_STOP: False
Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:49.790191] iteration      180/   18750 | elapsed time per iteration (ms): 5789.3 | learning rate: 1.440E-04 | lm loss: 1.067422E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.02
START iteration 180, CKPT_AND_STOP: False
Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:47:55.616342] iteration      181/   18750 | elapsed time per iteration (ms): 5826.1 | learning rate: 1.448E-04 | lm loss: 1.067195E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 2.11
START iteration 181, CKPT_AND_STOP: False
Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:01.464286] iteration      182/   18750 | elapsed time per iteration (ms): 5847.9 | learning rate: 1.456E-04 | lm loss: 1.067373E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.20
START iteration 182, CKPT_AND_STOP: False
Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:07.386658] iteration      183/   18750 | elapsed time per iteration (ms): 5922.4 | learning rate: 1.464E-04 | lm loss: 1.067680E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 1.79
START iteration 183, CKPT_AND_STOP: False
Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:13.271630] iteration      184/   18750 | elapsed time per iteration (ms): 5885.0 | learning rate: 1.472E-04 | lm loss: 1.067607E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.07
START iteration 184, CKPT_AND_STOP: False
Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:19.204831] iteration      185/   18750 | elapsed time per iteration (ms): 5933.1 | learning rate: 1.480E-04 | lm loss: 1.067825E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 1.84
START iteration 185, CKPT_AND_STOP: False
Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:25.198123] iteration      186/   18750 | elapsed time per iteration (ms): 5993.3 | learning rate: 1.488E-04 | lm loss: 1.067294E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.96
START iteration 186, CKPT_AND_STOP: False
Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:31.124649] iteration      187/   18750 | elapsed time per iteration (ms): 5926.5 | learning rate: 1.496E-04 | lm loss: 1.067714E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 2.36
START iteration 187, CKPT_AND_STOP: False
Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:37.113640] iteration      188/   18750 | elapsed time per iteration (ms): 5989.0 | learning rate: 1.500E-04 | lm loss: 1.067486E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 1.79
START iteration 188, CKPT_AND_STOP: False
Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:43.051243] iteration      189/   18750 | elapsed time per iteration (ms): 5937.6 | learning rate: 1.500E-04 | lm loss: 1.067739E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 1.84
START iteration 189, CKPT_AND_STOP: False
Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:48.931620] iteration      190/   18750 | elapsed time per iteration (ms): 5880.4 | learning rate: 1.500E-04 | lm loss: 1.067512E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.05 | batch generator: 1.71
START iteration 190, CKPT_AND_STOP: False
Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:48:54.833822] iteration      191/   18750 | elapsed time per iteration (ms): 5902.2 | learning rate: 1.500E-04 | lm loss: 1.067186E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.84
START iteration 191, CKPT_AND_STOP: False
Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:00.751878] iteration      192/   18750 | elapsed time per iteration (ms): 5918.0 | learning rate: 1.500E-04 | lm loss: 1.067097E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 7.02
START iteration 192, CKPT_AND_STOP: False
Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:06.637313] iteration      193/   18750 | elapsed time per iteration (ms): 5885.4 | learning rate: 1.500E-04 | lm loss: 1.067254E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.75
START iteration 193, CKPT_AND_STOP: False
Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:12.679830] iteration      194/   18750 | elapsed time per iteration (ms): 6042.5 | learning rate: 1.500E-04 | lm loss: 1.067062E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.80
START iteration 194, CKPT_AND_STOP: False
Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:18.464297] iteration      195/   18750 | elapsed time per iteration (ms): 5784.5 | learning rate: 1.500E-04 | lm loss: 1.067011E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 1.87
START iteration 195, CKPT_AND_STOP: False
Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:24.389040] iteration      196/   18750 | elapsed time per iteration (ms): 5924.7 | learning rate: 1.500E-04 | lm loss: 1.067231E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.12
START iteration 196, CKPT_AND_STOP: False
Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:30.264293] iteration      197/   18750 | elapsed time per iteration (ms): 5875.2 | learning rate: 1.500E-04 | lm loss: 1.067225E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.62
START iteration 197, CKPT_AND_STOP: False
Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:36.141677] iteration      198/   18750 | elapsed time per iteration (ms): 5877.4 | learning rate: 1.500E-04 | lm loss: 1.067098E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.96
START iteration 198, CKPT_AND_STOP: False
Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:42.106174] iteration      199/   18750 | elapsed time per iteration (ms): 5964.5 | learning rate: 1.500E-04 | lm loss: 1.067116E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.21
START iteration 199, CKPT_AND_STOP: False
Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:47.978943] iteration      200/   18750 | elapsed time per iteration (ms): 5872.7 | learning rate: 1.500E-04 | lm loss: 1.067063E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.28
START iteration 200, CKPT_AND_STOP: False
Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:53.917112] iteration      201/   18750 | elapsed time per iteration (ms): 5938.2 | learning rate: 1.500E-04 | lm loss: 1.067078E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.96
START iteration 201, CKPT_AND_STOP: False
Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:49:59.863112] iteration      202/   18750 | elapsed time per iteration (ms): 5946.0 | learning rate: 1.500E-04 | lm loss: 1.066968E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.15
START iteration 202, CKPT_AND_STOP: False
Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:05.790167] iteration      203/   18750 | elapsed time per iteration (ms): 5927.0 | learning rate: 1.500E-04 | lm loss: 1.066775E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.21
START iteration 203, CKPT_AND_STOP: False
Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:11.687483] iteration      204/   18750 | elapsed time per iteration (ms): 5897.3 | learning rate: 1.500E-04 | lm loss: 1.066900E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.23
START iteration 204, CKPT_AND_STOP: False
Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:17.543373] iteration      205/   18750 | elapsed time per iteration (ms): 5855.9 | learning rate: 1.500E-04 | lm loss: 1.066695E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.31
START iteration 205, CKPT_AND_STOP: False
Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:23.528917] iteration      206/   18750 | elapsed time per iteration (ms): 5985.5 | learning rate: 1.500E-04 | lm loss: 1.066955E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 1.73
START iteration 206, CKPT_AND_STOP: False
Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:29.436872] iteration      207/   18750 | elapsed time per iteration (ms): 5907.9 | learning rate: 1.500E-04 | lm loss: 1.067113E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.03
START iteration 207, CKPT_AND_STOP: False
Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:35.332977] iteration      208/   18750 | elapsed time per iteration (ms): 5896.1 | learning rate: 1.500E-04 | lm loss: 1.066939E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.11
START iteration 208, CKPT_AND_STOP: False
Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:41.221068] iteration      209/   18750 | elapsed time per iteration (ms): 5888.1 | learning rate: 1.500E-04 | lm loss: 1.067029E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.12
START iteration 209, CKPT_AND_STOP: False
Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:47.158793] iteration      210/   18750 | elapsed time per iteration (ms): 5937.7 | learning rate: 1.500E-04 | lm loss: 1.066562E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.11
START iteration 210, CKPT_AND_STOP: False
Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:53.088292] iteration      211/   18750 | elapsed time per iteration (ms): 5929.5 | learning rate: 1.500E-04 | lm loss: 1.066775E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.08
START iteration 211, CKPT_AND_STOP: False
Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:50:59.011468] iteration      212/   18750 | elapsed time per iteration (ms): 5923.2 | learning rate: 1.500E-04 | lm loss: 1.110033E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.82
START iteration 212, CKPT_AND_STOP: False
Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:04.995322] iteration      213/   18750 | elapsed time per iteration (ms): 5983.8 | learning rate: 1.500E-04 | lm loss: 1.067313E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.03
START iteration 213, CKPT_AND_STOP: False
Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:10.833288] iteration      214/   18750 | elapsed time per iteration (ms): 5837.9 | learning rate: 1.500E-04 | lm loss: 1.068014E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 6.74
START iteration 214, CKPT_AND_STOP: False
Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:16.747784] iteration      215/   18750 | elapsed time per iteration (ms): 5914.5 | learning rate: 1.500E-04 | lm loss: 1.068373E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 1.90
START iteration 215, CKPT_AND_STOP: False
Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:22.649215] iteration      216/   18750 | elapsed time per iteration (ms): 5901.4 | learning rate: 1.500E-04 | lm loss: 1.068484E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 7.68
START iteration 216, CKPT_AND_STOP: False
Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:28.629224] iteration      217/   18750 | elapsed time per iteration (ms): 5980.0 | learning rate: 1.500E-04 | lm loss: 1.068116E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.94
START iteration 217, CKPT_AND_STOP: False
Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:34.567981] iteration      218/   18750 | elapsed time per iteration (ms): 5938.7 | learning rate: 1.500E-04 | lm loss: 1.068374E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.07
START iteration 218, CKPT_AND_STOP: False
Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:40.517372] iteration      219/   18750 | elapsed time per iteration (ms): 5949.4 | learning rate: 1.500E-04 | lm loss: 1.067902E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.88
START iteration 219, CKPT_AND_STOP: False
Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:46.460027] iteration      220/   18750 | elapsed time per iteration (ms): 5942.6 | learning rate: 1.500E-04 | lm loss: 1.067608E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.19
START iteration 220, CKPT_AND_STOP: False
Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:52.328792] iteration      221/   18750 | elapsed time per iteration (ms): 5868.7 | learning rate: 1.500E-04 | lm loss: 1.068401E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.87
START iteration 221, CKPT_AND_STOP: False
Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:51:58.198538] iteration      222/   18750 | elapsed time per iteration (ms): 5869.7 | learning rate: 1.500E-04 | lm loss: 1.068125E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.19
START iteration 222, CKPT_AND_STOP: False
Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:04.238476] iteration      223/   18750 | elapsed time per iteration (ms): 6039.9 | learning rate: 1.500E-04 | lm loss: 1.068254E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.98
START iteration 223, CKPT_AND_STOP: False
Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:10.132398] iteration      224/   18750 | elapsed time per iteration (ms): 5893.9 | learning rate: 1.500E-04 | lm loss: 1.068327E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.01
START iteration 224, CKPT_AND_STOP: False
Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:16.067417] iteration      225/   18750 | elapsed time per iteration (ms): 5935.0 | learning rate: 1.500E-04 | lm loss: 1.068369E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.23
START iteration 225, CKPT_AND_STOP: False
Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:22.007337] iteration      226/   18750 | elapsed time per iteration (ms): 5939.9 | learning rate: 1.500E-04 | lm loss: 1.068175E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.97
START iteration 226, CKPT_AND_STOP: False
Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:27.923454] iteration      227/   18750 | elapsed time per iteration (ms): 5916.1 | learning rate: 1.500E-04 | lm loss: 1.068373E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.78
START iteration 227, CKPT_AND_STOP: False
Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:33.887692] iteration      228/   18750 | elapsed time per iteration (ms): 5964.2 | learning rate: 1.500E-04 | lm loss: 1.068577E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 1.87
START iteration 228, CKPT_AND_STOP: False
Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:39.820602] iteration      229/   18750 | elapsed time per iteration (ms): 5932.9 | learning rate: 1.500E-04 | lm loss: 1.068317E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.64
START iteration 229, CKPT_AND_STOP: False
Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:45.755455] iteration      230/   18750 | elapsed time per iteration (ms): 5934.8 | learning rate: 1.500E-04 | lm loss: 1.068295E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.87
START iteration 230, CKPT_AND_STOP: False
Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:51.646906] iteration      231/   18750 | elapsed time per iteration (ms): 5891.4 | learning rate: 1.500E-04 | lm loss: 1.068042E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.80
START iteration 231, CKPT_AND_STOP: False
Finished iteration 232, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:52:57.659345] iteration      232/   18750 | elapsed time per iteration (ms): 6012.4 | learning rate: 1.500E-04 | lm loss: 1.068157E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.21
START iteration 232, CKPT_AND_STOP: False
Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:03.629213] iteration      233/   18750 | elapsed time per iteration (ms): 5969.8 | learning rate: 1.500E-04 | lm loss: 1.067948E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 1.69
START iteration 233, CKPT_AND_STOP: False
Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:09.523508] iteration      234/   18750 | elapsed time per iteration (ms): 5894.3 | learning rate: 1.500E-04 | lm loss: 1.068178E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.84
START iteration 234, CKPT_AND_STOP: False
Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:15.464307] iteration      235/   18750 | elapsed time per iteration (ms): 5940.8 | learning rate: 1.500E-04 | lm loss: 1.068520E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.96
START iteration 235, CKPT_AND_STOP: False
Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:21.278505] iteration      236/   18750 | elapsed time per iteration (ms): 5814.2 | learning rate: 1.500E-04 | lm loss: 1.068460E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.04
START iteration 236, CKPT_AND_STOP: False
Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:27.195049] iteration      237/   18750 | elapsed time per iteration (ms): 5916.5 | learning rate: 1.500E-04 | lm loss: 1.068136E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.49
START iteration 237, CKPT_AND_STOP: False
Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:33.162913] iteration      238/   18750 | elapsed time per iteration (ms): 5967.9 | learning rate: 1.500E-04 | lm loss: 1.067844E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 1.95
START iteration 238, CKPT_AND_STOP: False
Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:39.057846] iteration      239/   18750 | elapsed time per iteration (ms): 5894.9 | learning rate: 1.500E-04 | lm loss: 1.067689E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.07
START iteration 239, CKPT_AND_STOP: False
Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:44.925467] iteration      240/   18750 | elapsed time per iteration (ms): 5867.6 | learning rate: 1.500E-04 | lm loss: 1.067514E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 7.64
START iteration 240, CKPT_AND_STOP: False
Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:51.007968] iteration      241/   18750 | elapsed time per iteration (ms): 6082.5 | learning rate: 1.500E-04 | lm loss: 1.067629E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 6.51
START iteration 241, CKPT_AND_STOP: False
Finished iteration 242, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:53:56.901477] iteration      242/   18750 | elapsed time per iteration (ms): 5893.5 | learning rate: 1.500E-04 | lm loss: 1.067348E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.28
START iteration 242, CKPT_AND_STOP: False
Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:02.770439] iteration      243/   18750 | elapsed time per iteration (ms): 5868.9 | learning rate: 1.500E-04 | lm loss: 1.067241E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.78
START iteration 243, CKPT_AND_STOP: False
Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:08.723695] iteration      244/   18750 | elapsed time per iteration (ms): 5953.2 | learning rate: 1.500E-04 | lm loss: 1.067609E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.28
START iteration 244, CKPT_AND_STOP: False
Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:14.589292] iteration      245/   18750 | elapsed time per iteration (ms): 5865.6 | learning rate: 1.500E-04 | lm loss: 1.067924E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.84
START iteration 245, CKPT_AND_STOP: False
Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:20.499680] iteration      246/   18750 | elapsed time per iteration (ms): 5910.4 | learning rate: 1.500E-04 | lm loss: 1.067938E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.19
START iteration 246, CKPT_AND_STOP: False
Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:26.394185] iteration      247/   18750 | elapsed time per iteration (ms): 5894.5 | learning rate: 1.500E-04 | lm loss: 1.067895E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.40
START iteration 247, CKPT_AND_STOP: False
Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:32.286068] iteration      248/   18750 | elapsed time per iteration (ms): 5891.9 | learning rate: 1.500E-04 | lm loss: 1.067916E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.27
START iteration 248, CKPT_AND_STOP: False
Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:38.227651] iteration      249/   18750 | elapsed time per iteration (ms): 5941.6 | learning rate: 1.500E-04 | lm loss: 1.067942E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 2.56
START iteration 249, CKPT_AND_STOP: False
Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:44.143531] iteration      250/   18750 | elapsed time per iteration (ms): 5915.9 | learning rate: 1.500E-04 | lm loss: 1.067857E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.69
START iteration 250, CKPT_AND_STOP: False
Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:50.134302] iteration      251/   18750 | elapsed time per iteration (ms): 5990.8 | learning rate: 1.500E-04 | lm loss: 1.067587E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.06
START iteration 251, CKPT_AND_STOP: False
Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:54:56.066677] iteration      252/   18750 | elapsed time per iteration (ms): 5932.4 | learning rate: 1.500E-04 | lm loss: 1.067999E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 6.80
START iteration 252, CKPT_AND_STOP: False
Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:01.992812] iteration      253/   18750 | elapsed time per iteration (ms): 5926.1 | learning rate: 1.500E-04 | lm loss: 1.067858E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.82
START iteration 253, CKPT_AND_STOP: False
Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:07.949313] iteration      254/   18750 | elapsed time per iteration (ms): 5956.5 | learning rate: 1.500E-04 | lm loss: 1.067981E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 1.77
START iteration 254, CKPT_AND_STOP: False
Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:13.913777] iteration      255/   18750 | elapsed time per iteration (ms): 5964.4 | learning rate: 1.500E-04 | lm loss: 1.067549E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.79
START iteration 255, CKPT_AND_STOP: False
Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:19.782504] iteration      256/   18750 | elapsed time per iteration (ms): 5868.7 | learning rate: 1.500E-04 | lm loss: 1.067839E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.36
START iteration 256, CKPT_AND_STOP: False
Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:25.617163] iteration      257/   18750 | elapsed time per iteration (ms): 5834.6 | learning rate: 1.500E-04 | lm loss: 1.067488E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.97
START iteration 257, CKPT_AND_STOP: False
Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:31.471462] iteration      258/   18750 | elapsed time per iteration (ms): 5854.3 | learning rate: 1.500E-04 | lm loss: 1.067778E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.95
START iteration 258, CKPT_AND_STOP: False
Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:37.368939] iteration      259/   18750 | elapsed time per iteration (ms): 5897.5 | learning rate: 1.500E-04 | lm loss: 1.067782E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.67
START iteration 259, CKPT_AND_STOP: False
Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:43.314145] iteration      260/   18750 | elapsed time per iteration (ms): 5945.2 | learning rate: 1.500E-04 | lm loss: 1.067822E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.04 | batch generator: 2.44
START iteration 260, CKPT_AND_STOP: False
Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:49.131720] iteration      261/   18750 | elapsed time per iteration (ms): 5817.5 | learning rate: 1.500E-04 | lm loss: 1.067988E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.85
START iteration 261, CKPT_AND_STOP: False
Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:55:54.903271] iteration      262/   18750 | elapsed time per iteration (ms): 5771.5 | learning rate: 1.500E-04 | lm loss: 1.067452E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.05
START iteration 262, CKPT_AND_STOP: False
Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:00.770965] iteration      263/   18750 | elapsed time per iteration (ms): 5867.7 | learning rate: 1.500E-04 | lm loss: 1.067858E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.77
START iteration 263, CKPT_AND_STOP: False
Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:06.685898] iteration      264/   18750 | elapsed time per iteration (ms): 5914.9 | learning rate: 1.500E-04 | lm loss: 1.067374E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.17
START iteration 264, CKPT_AND_STOP: False
Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:12.537473] iteration      265/   18750 | elapsed time per iteration (ms): 5851.6 | learning rate: 1.500E-04 | lm loss: 1.067398E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.43
START iteration 265, CKPT_AND_STOP: False
Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:18.461770] iteration      266/   18750 | elapsed time per iteration (ms): 5924.3 | learning rate: 1.500E-04 | lm loss: 1.067944E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.05
START iteration 266, CKPT_AND_STOP: False
Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:24.360742] iteration      267/   18750 | elapsed time per iteration (ms): 5899.0 | learning rate: 1.500E-04 | lm loss: 1.067225E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.42
START iteration 267, CKPT_AND_STOP: False
Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:30.233342] iteration      268/   18750 | elapsed time per iteration (ms): 5872.6 | learning rate: 1.500E-04 | lm loss: 1.067367E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.90
START iteration 268, CKPT_AND_STOP: False
Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:36.090168] iteration      269/   18750 | elapsed time per iteration (ms): 5856.8 | learning rate: 1.500E-04 | lm loss: 1.067161E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.13
START iteration 269, CKPT_AND_STOP: False
Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:41.966984] iteration      270/   18750 | elapsed time per iteration (ms): 5876.8 | learning rate: 1.500E-04 | lm loss: 1.067078E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.09 | batch generator: 2.14
START iteration 270, CKPT_AND_STOP: False
Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:47.868985] iteration      271/   18750 | elapsed time per iteration (ms): 5902.0 | learning rate: 1.500E-04 | lm loss: 1.067243E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.94
START iteration 271, CKPT_AND_STOP: False
Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:53.795536] iteration      272/   18750 | elapsed time per iteration (ms): 5926.5 | learning rate: 1.500E-04 | lm loss: 1.067645E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.96
START iteration 272, CKPT_AND_STOP: False
Finished iteration 273, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:56:59.734159] iteration      273/   18750 | elapsed time per iteration (ms): 5938.6 | learning rate: 1.500E-04 | lm loss: 1.067254E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.71
START iteration 273, CKPT_AND_STOP: False
Finished iteration 274, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:05.666896] iteration      274/   18750 | elapsed time per iteration (ms): 5932.7 | learning rate: 1.500E-04 | lm loss: 1.067318E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.15
START iteration 274, CKPT_AND_STOP: False
Finished iteration 275, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:11.558447] iteration      275/   18750 | elapsed time per iteration (ms): 5891.5 | learning rate: 1.500E-04 | lm loss: 1.067413E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.90
START iteration 275, CKPT_AND_STOP: False
Finished iteration 276, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:17.440375] iteration      276/   18750 | elapsed time per iteration (ms): 5881.9 | learning rate: 1.500E-04 | lm loss: 1.067351E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.13
START iteration 276, CKPT_AND_STOP: False
Finished iteration 277, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:23.452254] iteration      277/   18750 | elapsed time per iteration (ms): 6011.9 | learning rate: 1.500E-04 | lm loss: 1.067112E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.89
START iteration 277, CKPT_AND_STOP: False
Finished iteration 278, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:29.394939] iteration      278/   18750 | elapsed time per iteration (ms): 5942.7 | learning rate: 1.500E-04 | lm loss: 1.067185E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.03
START iteration 278, CKPT_AND_STOP: False
Finished iteration 279, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:35.269796] iteration      279/   18750 | elapsed time per iteration (ms): 5874.8 | learning rate: 1.500E-04 | lm loss: 1.067334E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 1.86
START iteration 279, CKPT_AND_STOP: False
Finished iteration 280, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:41.139998] iteration      280/   18750 | elapsed time per iteration (ms): 5870.2 | learning rate: 1.500E-04 | lm loss: 1.067271E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.11
START iteration 280, CKPT_AND_STOP: False
Finished iteration 281, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:47.029775] iteration      281/   18750 | elapsed time per iteration (ms): 5889.8 | learning rate: 1.500E-04 | lm loss: 1.067330E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.20
START iteration 281, CKPT_AND_STOP: False
Finished iteration 282, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:52.855907] iteration      282/   18750 | elapsed time per iteration (ms): 5826.1 | learning rate: 1.500E-04 | lm loss: 1.067430E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.60
START iteration 282, CKPT_AND_STOP: False
Finished iteration 283, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:57:58.808033] iteration      283/   18750 | elapsed time per iteration (ms): 5952.1 | learning rate: 1.500E-04 | lm loss: 1.067563E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.12
START iteration 283, CKPT_AND_STOP: False
Finished iteration 284, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:04.737364] iteration      284/   18750 | elapsed time per iteration (ms): 5929.3 | learning rate: 1.500E-04 | lm loss: 1.067419E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.08
START iteration 284, CKPT_AND_STOP: False
Finished iteration 285, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:10.669321] iteration      285/   18750 | elapsed time per iteration (ms): 5931.9 | learning rate: 1.500E-04 | lm loss: 1.067528E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 9.00
START iteration 285, CKPT_AND_STOP: False
Finished iteration 286, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:16.443754] iteration      286/   18750 | elapsed time per iteration (ms): 5774.4 | learning rate: 1.500E-04 | lm loss: 1.067160E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.69
START iteration 286, CKPT_AND_STOP: False
Finished iteration 287, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:22.317816] iteration      287/   18750 | elapsed time per iteration (ms): 5874.0 | learning rate: 1.500E-04 | lm loss: 1.067254E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.01 | batch generator: 2.50
START iteration 287, CKPT_AND_STOP: False
Finished iteration 288, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:28.280307] iteration      288/   18750 | elapsed time per iteration (ms): 5962.5 | learning rate: 1.500E-04 | lm loss: 1.067203E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.54
START iteration 288, CKPT_AND_STOP: False
Finished iteration 289, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:34.221610] iteration      289/   18750 | elapsed time per iteration (ms): 5941.3 | learning rate: 1.500E-04 | lm loss: 1.067517E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 2.38
START iteration 289, CKPT_AND_STOP: False
Finished iteration 290, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:40.201493] iteration      290/   18750 | elapsed time per iteration (ms): 5979.9 | learning rate: 1.500E-04 | lm loss: 1.067132E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.90
START iteration 290, CKPT_AND_STOP: False
Finished iteration 291, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:46.043674] iteration      291/   18750 | elapsed time per iteration (ms): 5842.2 | learning rate: 1.500E-04 | lm loss: 1.067504E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.91
START iteration 291, CKPT_AND_STOP: False
Finished iteration 292, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:51.953983] iteration      292/   18750 | elapsed time per iteration (ms): 5910.3 | learning rate: 1.500E-04 | lm loss: 1.067343E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.27
START iteration 292, CKPT_AND_STOP: False
Finished iteration 293, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:58:57.929348] iteration      293/   18750 | elapsed time per iteration (ms): 5975.3 | learning rate: 1.500E-04 | lm loss: 1.067242E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.28
START iteration 293, CKPT_AND_STOP: False
Finished iteration 294, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:03.856014] iteration      294/   18750 | elapsed time per iteration (ms): 5926.7 | learning rate: 1.500E-04 | lm loss: 1.067569E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.03 | batch generator: 1.90
START iteration 294, CKPT_AND_STOP: False
Finished iteration 295, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:09.833096] iteration      295/   18750 | elapsed time per iteration (ms): 5977.1 | learning rate: 1.500E-04 | lm loss: 1.067718E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.02
START iteration 295, CKPT_AND_STOP: False
Finished iteration 296, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:16.018796] iteration      296/   18750 | elapsed time per iteration (ms): 6185.7 | learning rate: 1.500E-04 | lm loss: 1.067451E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 8.44
START iteration 296, CKPT_AND_STOP: False
Finished iteration 297, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:22.006981] iteration      297/   18750 | elapsed time per iteration (ms): 5988.2 | learning rate: 1.500E-04 | lm loss: 1.067137E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.96 | batch generator: 2.24
START iteration 297, CKPT_AND_STOP: False
Finished iteration 298, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:27.857282] iteration      298/   18750 | elapsed time per iteration (ms): 5850.3 | learning rate: 1.500E-04 | lm loss: 1.066724E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 1.99
START iteration 298, CKPT_AND_STOP: False
Finished iteration 299, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:33.756473] iteration      299/   18750 | elapsed time per iteration (ms): 5899.2 | learning rate: 1.500E-04 | lm loss: 1.066825E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 1.89
START iteration 299, CKPT_AND_STOP: False
Finished iteration 300, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:39.587867] iteration      300/   18750 | elapsed time per iteration (ms): 5831.4 | learning rate: 1.500E-04 | lm loss: 1.067132E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.02 | batch generator: 1.73
START iteration 300, CKPT_AND_STOP: False
Finished iteration 301, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:45.393391] iteration      301/   18750 | elapsed time per iteration (ms): 5805.5 | learning rate: 1.500E-04 | lm loss: 1.066919E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.98 | batch generator: 2.12
START iteration 301, CKPT_AND_STOP: False
Finished iteration 302, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:51.273696] iteration      302/   18750 | elapsed time per iteration (ms): 5880.3 | learning rate: 1.500E-04 | lm loss: 1.066992E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 1.86
START iteration 302, CKPT_AND_STOP: False
Finished iteration 303, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 08:59:57.204130] iteration      303/   18750 | elapsed time per iteration (ms): 5930.4 | learning rate: 1.500E-04 | lm loss: 1.067039E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 2.16
START iteration 303, CKPT_AND_STOP: False
Finished iteration 304, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 09:00:03.087265] iteration      304/   18750 | elapsed time per iteration (ms): 5883.1 | learning rate: 1.500E-04 | lm loss: 1.066954E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 31.00 | batch generator: 9.16
START iteration 304, CKPT_AND_STOP: False
Finished iteration 305, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 09:00:08.965810] iteration      305/   18750 | elapsed time per iteration (ms): 5878.5 | learning rate: 1.500E-04 | lm loss: 1.067105E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.99 | batch generator: 2.37
START iteration 305, CKPT_AND_STOP: False
Finished iteration 306, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
[2022-11-28 09:00:14.905651] iteration      306/   18750 | elapsed time per iteration (ms): 5939.8 | learning rate: 1.500E-04 | lm loss: 1.067212E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 30.97 | batch generator: 2.18
START iteration 306, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
Finished iteration 307, CKPT_AND_STOP: True, flag: tensor([6], dtype=torch.int32)
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     307 to s3://spot-checkpoints/gpt/iter_0000307/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000307/mp_rank_00/model_optim_rng.pt
Opt ckpt time 57.69077157974243
Process done with return code 1
Parent process ID: 35251 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14361 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 307
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 64
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 307
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Parent process ID: 35722 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14527 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 307
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 64
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 307
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Process done with return code 1
Parent process ID: 36163 node: 172.31.28.108
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14452 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 307
using world size: 16 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 64
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 4096
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 32
  num_layers ...................... 32
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 307
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 16
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
Process done with return code 1
