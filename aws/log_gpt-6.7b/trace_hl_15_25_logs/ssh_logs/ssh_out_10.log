Parent process ID: 18354 node: 172.31.19.171
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14317 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16
dry run time 4.364764451980591
SHARED WEIGHTS ARE
[(0, 15)]
this rank  10 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 0.205 seconds
START iteration 0, CKPT_AND_STOP: False
Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 1, CKPT_AND_STOP: False
Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 2, CKPT_AND_STOP: False
Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 3, CKPT_AND_STOP: False
Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 4, CKPT_AND_STOP: False
Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 5, CKPT_AND_STOP: False
Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 6, CKPT_AND_STOP: False
Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 7, CKPT_AND_STOP: False
Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 8, CKPT_AND_STOP: False
Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 9, CKPT_AND_STOP: False
Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 10, CKPT_AND_STOP: False
Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 11, CKPT_AND_STOP: False
Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 12, CKPT_AND_STOP: False
Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 13, CKPT_AND_STOP: False
Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 14, CKPT_AND_STOP: False
Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 15, CKPT_AND_STOP: False
Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 16, CKPT_AND_STOP: False
Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 17, CKPT_AND_STOP: False
Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 18, CKPT_AND_STOP: False
Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 19, CKPT_AND_STOP: False
Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 20, CKPT_AND_STOP: False
Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 21, CKPT_AND_STOP: False
Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 22, CKPT_AND_STOP: False
Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 23, CKPT_AND_STOP: False
Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 24, CKPT_AND_STOP: False
Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 25, CKPT_AND_STOP: False
Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 26, CKPT_AND_STOP: False
Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 27, CKPT_AND_STOP: False
Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 28, CKPT_AND_STOP: False
Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 29, CKPT_AND_STOP: False
Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 30, CKPT_AND_STOP: False
Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 31, CKPT_AND_STOP: False
Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 32, CKPT_AND_STOP: False
Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 33, CKPT_AND_STOP: False
Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 34, CKPT_AND_STOP: False
Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 35, CKPT_AND_STOP: False
Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 36, CKPT_AND_STOP: False
Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 37, CKPT_AND_STOP: False
Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([5], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 10 signal handler called with signal 10
Opt ckpt time 55.6338369846344
Process done with return code 0
Parent process ID: 18915 node: 172.31.19.171
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 6852 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 38
dry run time 3.1444015502929688
SHARED WEIGHTS ARE
[(0, 15)]
this rank  10 is part of pipeline replica  0
32 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 56.206 seconds
START iteration 38, CKPT_AND_STOP: False
Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 39, CKPT_AND_STOP: False
Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 40, CKPT_AND_STOP: False
Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 41, CKPT_AND_STOP: False
Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 42, CKPT_AND_STOP: False
Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 43, CKPT_AND_STOP: False
Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 44, CKPT_AND_STOP: False
Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 45, CKPT_AND_STOP: False
Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 46, CKPT_AND_STOP: False
Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 47, CKPT_AND_STOP: False
Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 48, CKPT_AND_STOP: False
Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 49, CKPT_AND_STOP: False
Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 50, CKPT_AND_STOP: False
Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 51, CKPT_AND_STOP: False
Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 52, CKPT_AND_STOP: False
Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 53, CKPT_AND_STOP: False
Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 54, CKPT_AND_STOP: False
Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 55, CKPT_AND_STOP: False
Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 56, CKPT_AND_STOP: False
Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 57, CKPT_AND_STOP: False
Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 58, CKPT_AND_STOP: False
Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 59, CKPT_AND_STOP: False
Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 60, CKPT_AND_STOP: False
Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 61, CKPT_AND_STOP: False
Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 62, CKPT_AND_STOP: False
Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 63, CKPT_AND_STOP: False
Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 64, CKPT_AND_STOP: False
Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 65, CKPT_AND_STOP: False
Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 66, CKPT_AND_STOP: False
Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 67, CKPT_AND_STOP: False
Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 68, CKPT_AND_STOP: False
Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 69, CKPT_AND_STOP: False
Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 70, CKPT_AND_STOP: False
Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 71, CKPT_AND_STOP: False
Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 72, CKPT_AND_STOP: False
Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 73, CKPT_AND_STOP: False
Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 74, CKPT_AND_STOP: False
Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 75, CKPT_AND_STOP: False
Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 76, CKPT_AND_STOP: False
Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 77, CKPT_AND_STOP: False
Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 78, CKPT_AND_STOP: False
Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 79, CKPT_AND_STOP: False
Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 80, CKPT_AND_STOP: False
Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 81, CKPT_AND_STOP: False
Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 10 signal handler called with signal 10
Opt ckpt time 31.40077018737793
Process done with return code 1
Parent process ID: 15683 node: 172.31.17.44
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14246 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 82
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 10 signal handler called with signal 10
dry run time 3.091733932495117
SHARED WEIGHTS ARE
[(0, 15)]
this rank  10 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 60.966 seconds
Process done with return code 0
Parent process ID: 16189 node: 172.31.17.44
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14336 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 82
dry run time 3.0872244834899902
SHARED WEIGHTS ARE
[(0, 15)]
this rank  10 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 66.630 seconds
START iteration 82, CKPT_AND_STOP: False
Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 83, CKPT_AND_STOP: False
Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 84, CKPT_AND_STOP: False
Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 85, CKPT_AND_STOP: False
Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 86, CKPT_AND_STOP: False
Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 87, CKPT_AND_STOP: False
Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 88, CKPT_AND_STOP: False
Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 89, CKPT_AND_STOP: False
Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 90, CKPT_AND_STOP: False
Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 91, CKPT_AND_STOP: False
Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 92, CKPT_AND_STOP: False
Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 93, CKPT_AND_STOP: False
Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 94, CKPT_AND_STOP: False
Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 95, CKPT_AND_STOP: False
Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 96, CKPT_AND_STOP: False
Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 97, CKPT_AND_STOP: False
Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 98, CKPT_AND_STOP: False
Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 99, CKPT_AND_STOP: False
Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 100, CKPT_AND_STOP: False
Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 101, CKPT_AND_STOP: False
Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 102, CKPT_AND_STOP: False
Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 103, CKPT_AND_STOP: False
Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 104, CKPT_AND_STOP: False
Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 105, CKPT_AND_STOP: False
Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 106, CKPT_AND_STOP: False
Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 107, CKPT_AND_STOP: False
Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 108, CKPT_AND_STOP: False
Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 109, CKPT_AND_STOP: False
Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 110, CKPT_AND_STOP: False
Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 111, CKPT_AND_STOP: False
Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 112, CKPT_AND_STOP: False
Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 113, CKPT_AND_STOP: False
Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 114, CKPT_AND_STOP: False
Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 115, CKPT_AND_STOP: False
Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 116, CKPT_AND_STOP: False
Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 117, CKPT_AND_STOP: False
Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 118, CKPT_AND_STOP: False
Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 119, CKPT_AND_STOP: False
Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 120, CKPT_AND_STOP: False
Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 121, CKPT_AND_STOP: False
Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 122, CKPT_AND_STOP: False
Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 123, CKPT_AND_STOP: False
Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 124, CKPT_AND_STOP: False
Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 125, CKPT_AND_STOP: False
Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 126, CKPT_AND_STOP: False
Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 127, CKPT_AND_STOP: False
Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 128, CKPT_AND_STOP: False
Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 129, CKPT_AND_STOP: False
Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 130, CKPT_AND_STOP: False
Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 131, CKPT_AND_STOP: False
Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 132, CKPT_AND_STOP: False
Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 133, CKPT_AND_STOP: False
Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 134, CKPT_AND_STOP: False
Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 135, CKPT_AND_STOP: False
Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 136, CKPT_AND_STOP: False
Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 137, CKPT_AND_STOP: False
Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 138, CKPT_AND_STOP: False
Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 139, CKPT_AND_STOP: False
Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 140, CKPT_AND_STOP: False
Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 141, CKPT_AND_STOP: False
Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 142, CKPT_AND_STOP: False
Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 143, CKPT_AND_STOP: False
Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 144, CKPT_AND_STOP: False
Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 145, CKPT_AND_STOP: False
Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 146, CKPT_AND_STOP: False
Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 147, CKPT_AND_STOP: False
Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 148, CKPT_AND_STOP: False
Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 149, CKPT_AND_STOP: False
Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 150, CKPT_AND_STOP: False
Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 151, CKPT_AND_STOP: False
Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 152, CKPT_AND_STOP: False
Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 153, CKPT_AND_STOP: False
Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 154, CKPT_AND_STOP: False
Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 155, CKPT_AND_STOP: False
Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 156, CKPT_AND_STOP: False
Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 157, CKPT_AND_STOP: False
Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 158, CKPT_AND_STOP: False
Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 159, CKPT_AND_STOP: False
Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 160, CKPT_AND_STOP: False
Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 10 signal handler called with signal 10
Opt ckpt time 51.694557428359985
Process done with return code 0
Parent process ID: 17017 node: 172.31.17.44
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 6841 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 161
dry run time 3.0057947635650635
SHARED WEIGHTS ARE
[(0, 15)]
this rank  10 is part of pipeline replica  0
32 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 64.374 seconds
START iteration 161, CKPT_AND_STOP: False
Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 162, CKPT_AND_STOP: False
Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 163, CKPT_AND_STOP: False
Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 164, CKPT_AND_STOP: False
Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 165, CKPT_AND_STOP: False
Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 166, CKPT_AND_STOP: False
Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 167, CKPT_AND_STOP: False
Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 168, CKPT_AND_STOP: False
Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 169, CKPT_AND_STOP: False
Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 170, CKPT_AND_STOP: False
Finished iteration 171, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 171, CKPT_AND_STOP: False
Finished iteration 172, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 172, CKPT_AND_STOP: False
Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 173, CKPT_AND_STOP: False
Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 174, CKPT_AND_STOP: False
Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 175, CKPT_AND_STOP: False
Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 176, CKPT_AND_STOP: False
Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 177, CKPT_AND_STOP: False
Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 178, CKPT_AND_STOP: False
Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 179, CKPT_AND_STOP: False
Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 180, CKPT_AND_STOP: False
Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 181, CKPT_AND_STOP: False
Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 182, CKPT_AND_STOP: False
Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 183, CKPT_AND_STOP: False
Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 184, CKPT_AND_STOP: False
Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 185, CKPT_AND_STOP: False
Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 186, CKPT_AND_STOP: False
Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 187, CKPT_AND_STOP: False
Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 188, CKPT_AND_STOP: False
Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 189, CKPT_AND_STOP: False
Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 190, CKPT_AND_STOP: False
Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 191, CKPT_AND_STOP: False
Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 192, CKPT_AND_STOP: False
Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 193, CKPT_AND_STOP: False
Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 194, CKPT_AND_STOP: False
Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 195, CKPT_AND_STOP: False
Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 196, CKPT_AND_STOP: False
Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 197, CKPT_AND_STOP: False
Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 198, CKPT_AND_STOP: False
Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 199, CKPT_AND_STOP: False
Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 200, CKPT_AND_STOP: False
Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 201, CKPT_AND_STOP: False
Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 202, CKPT_AND_STOP: False
Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 203, CKPT_AND_STOP: False
Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 204, CKPT_AND_STOP: False
Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 205, CKPT_AND_STOP: False
Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 206, CKPT_AND_STOP: False
Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 207, CKPT_AND_STOP: False
Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 208, CKPT_AND_STOP: False
Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 209, CKPT_AND_STOP: False
Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 210, CKPT_AND_STOP: False
Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 211, CKPT_AND_STOP: False
Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 212, CKPT_AND_STOP: False
Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 213, CKPT_AND_STOP: False
Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 214, CKPT_AND_STOP: False
Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 215, CKPT_AND_STOP: False
Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 216, CKPT_AND_STOP: False
Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 217, CKPT_AND_STOP: False
Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 218, CKPT_AND_STOP: False
Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 219, CKPT_AND_STOP: False
Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 220, CKPT_AND_STOP: False
Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 221, CKPT_AND_STOP: False
Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 222, CKPT_AND_STOP: False
Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 223, CKPT_AND_STOP: False
Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 224, CKPT_AND_STOP: False
Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 225, CKPT_AND_STOP: False
Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 226, CKPT_AND_STOP: False
Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 227, CKPT_AND_STOP: False
Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 228, CKPT_AND_STOP: False
Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 229, CKPT_AND_STOP: False
Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 230, CKPT_AND_STOP: False
Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 231, CKPT_AND_STOP: False
Finished iteration 232, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 232, CKPT_AND_STOP: False
Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 233, CKPT_AND_STOP: False
Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 234, CKPT_AND_STOP: False
Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 235, CKPT_AND_STOP: False
Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 236, CKPT_AND_STOP: False
Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 237, CKPT_AND_STOP: False
Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 238, CKPT_AND_STOP: False
Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 239, CKPT_AND_STOP: False
Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 240, CKPT_AND_STOP: False
Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 241, CKPT_AND_STOP: False
Finished iteration 242, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 242, CKPT_AND_STOP: False
Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 243, CKPT_AND_STOP: False
Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 244, CKPT_AND_STOP: False
Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 245, CKPT_AND_STOP: False
Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 246, CKPT_AND_STOP: False
Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 247, CKPT_AND_STOP: False
Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 248, CKPT_AND_STOP: False
Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 249, CKPT_AND_STOP: False
Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 250, CKPT_AND_STOP: False
Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 251, CKPT_AND_STOP: False
Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 252, CKPT_AND_STOP: False
Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 253, CKPT_AND_STOP: False
Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 254, CKPT_AND_STOP: False
Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 255, CKPT_AND_STOP: False
Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 256, CKPT_AND_STOP: False
Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 257, CKPT_AND_STOP: False
Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 258, CKPT_AND_STOP: False
Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 259, CKPT_AND_STOP: False
Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 260, CKPT_AND_STOP: False
Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 261, CKPT_AND_STOP: False
Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 262, CKPT_AND_STOP: False
Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 263, CKPT_AND_STOP: False
Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 264, CKPT_AND_STOP: False
Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 265, CKPT_AND_STOP: False
Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 266, CKPT_AND_STOP: False
Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 267, CKPT_AND_STOP: False
Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 268, CKPT_AND_STOP: False
Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 269, CKPT_AND_STOP: False
Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 270, CKPT_AND_STOP: False
Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 271, CKPT_AND_STOP: False
Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 272, CKPT_AND_STOP: False
Finished iteration 273, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 273, CKPT_AND_STOP: False
Finished iteration 274, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 274, CKPT_AND_STOP: False
Finished iteration 275, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 275, CKPT_AND_STOP: False
Finished iteration 276, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 276, CKPT_AND_STOP: False
Finished iteration 277, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 277, CKPT_AND_STOP: False
Finished iteration 278, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 278, CKPT_AND_STOP: False
Finished iteration 279, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 279, CKPT_AND_STOP: False
Finished iteration 280, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 280, CKPT_AND_STOP: False
Finished iteration 281, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 281, CKPT_AND_STOP: False
Finished iteration 282, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 282, CKPT_AND_STOP: False
Finished iteration 283, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 283, CKPT_AND_STOP: False
Finished iteration 284, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 284, CKPT_AND_STOP: False
Finished iteration 285, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 285, CKPT_AND_STOP: False
Finished iteration 286, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 286, CKPT_AND_STOP: False
Finished iteration 287, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 287, CKPT_AND_STOP: False
Finished iteration 288, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 288, CKPT_AND_STOP: False
Finished iteration 289, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 289, CKPT_AND_STOP: False
Finished iteration 290, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 290, CKPT_AND_STOP: False
Finished iteration 291, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 291, CKPT_AND_STOP: False
Finished iteration 292, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 292, CKPT_AND_STOP: False
Finished iteration 293, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 293, CKPT_AND_STOP: False
Finished iteration 294, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 294, CKPT_AND_STOP: False
Finished iteration 295, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 295, CKPT_AND_STOP: False
Finished iteration 296, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 296, CKPT_AND_STOP: False
Finished iteration 297, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 297, CKPT_AND_STOP: False
Finished iteration 298, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 298, CKPT_AND_STOP: False
Finished iteration 299, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 299, CKPT_AND_STOP: False
Finished iteration 300, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 300, CKPT_AND_STOP: False
Finished iteration 301, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 301, CKPT_AND_STOP: False
Finished iteration 302, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 302, CKPT_AND_STOP: False
Finished iteration 303, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 303, CKPT_AND_STOP: False
Finished iteration 304, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 304, CKPT_AND_STOP: False
Finished iteration 305, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 305, CKPT_AND_STOP: False
Finished iteration 306, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 306, CKPT_AND_STOP: False
Finished iteration 307, CKPT_AND_STOP: False, flag: tensor([6], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 10 signal handler called with signal 10
Opt ckpt time 29.733147144317627
Process done with return code 1
Parent process ID: 16321 node: 172.31.21.109
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14350 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 307
Signal handler called with signal 10


 STOPPING VARUNA !!



Parent process ID: 20756 node: 172.31.21.254
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14369 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 307
Signal handler called with signal 10


 STOPPING VARUNA !!



Parent process ID: 21075 node: 172.31.21.254
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14344 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(10, 11)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=10 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 307
Signal handler called with signal 10


 STOPPING VARUNA !!



