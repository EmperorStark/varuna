Parent process ID: 7148 node: 172.31.24.208
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 6836 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16
dry run time 2.8845012187957764
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
32 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 0.202 seconds
START iteration 0, CKPT_AND_STOP: False
Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 1, CKPT_AND_STOP: False
Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 2, CKPT_AND_STOP: False
Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 3, CKPT_AND_STOP: False
Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 4, CKPT_AND_STOP: False
Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 5, CKPT_AND_STOP: False
Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 6, CKPT_AND_STOP: False
Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 7, CKPT_AND_STOP: False
Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 8, CKPT_AND_STOP: False
Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 9, CKPT_AND_STOP: False
Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 10, CKPT_AND_STOP: False
Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 11, CKPT_AND_STOP: False
Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 12, CKPT_AND_STOP: False
Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 13, CKPT_AND_STOP: False
Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 14, CKPT_AND_STOP: False
Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 15, CKPT_AND_STOP: False
Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 16, CKPT_AND_STOP: False
Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 17, CKPT_AND_STOP: False
Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 18, CKPT_AND_STOP: False
Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 19, CKPT_AND_STOP: False
Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 20, CKPT_AND_STOP: False
Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 21, CKPT_AND_STOP: False
Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 22, CKPT_AND_STOP: False
Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 23, CKPT_AND_STOP: False
Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 24, CKPT_AND_STOP: False
Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 25, CKPT_AND_STOP: False
Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 26, CKPT_AND_STOP: False
Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 27, CKPT_AND_STOP: False
Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 28, CKPT_AND_STOP: False
Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 29, CKPT_AND_STOP: False
Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 30, CKPT_AND_STOP: False
Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 31, CKPT_AND_STOP: False
Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 32, CKPT_AND_STOP: False
Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 33, CKPT_AND_STOP: False
Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 34, CKPT_AND_STOP: False
Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 35, CKPT_AND_STOP: False
Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 36, CKPT_AND_STOP: False
Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 37, CKPT_AND_STOP: False
Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 38, CKPT_AND_STOP: False
Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 39, CKPT_AND_STOP: False
Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 40, CKPT_AND_STOP: False
Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 41, CKPT_AND_STOP: False
Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 42, CKPT_AND_STOP: False
Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 43, CKPT_AND_STOP: False
Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 44, CKPT_AND_STOP: False
Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 45, CKPT_AND_STOP: False
Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 46, CKPT_AND_STOP: False
Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 47, CKPT_AND_STOP: False
Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 48, CKPT_AND_STOP: False
Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 49, CKPT_AND_STOP: False
Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 50, CKPT_AND_STOP: False
Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 51, CKPT_AND_STOP: False
Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 52, CKPT_AND_STOP: False
Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 53, CKPT_AND_STOP: False
Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 54, CKPT_AND_STOP: False
Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 55, CKPT_AND_STOP: False
Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 56, CKPT_AND_STOP: False
Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 57, CKPT_AND_STOP: False
Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([3], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Opt ckpt time 30.43812608718872
Process done with return code 1
Parent process ID: 7532 node: 172.31.30.99
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14329 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 58
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Parent process ID: 7829 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14363 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 58
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
dry run time 3.310713291168213
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
Process done with return code 1
Parent process ID: 8282 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14317 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 58
dry run time 2.800807476043701
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 55.906 seconds
Process done with return code 0
Parent process ID: 8788 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14269 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 58
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
dry run time 3.1427226066589355
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
Parent process ID: 9195 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14278 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 58
dry run time 3.154419183731079
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 66.133 seconds
START iteration 58, CKPT_AND_STOP: False
Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 59, CKPT_AND_STOP: False
Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([1], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Opt ckpt time 51.574026584625244
Process done with return code 0
Parent process ID: 9703 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14364 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 60
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
dry run time 4.349516153335571
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
Process done with return code 1
Parent process ID: 10112 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14324 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 60
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Process done with return code 1
Parent process ID: 10458 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14320 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 60
Process done with return code 1
Parent process ID: 10781 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14354 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 60
dry run time 3.645677328109741
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 56.340 seconds
START iteration 60, CKPT_AND_STOP: False
Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 61, CKPT_AND_STOP: False
Parent process ID: 11159 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14241 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 60
dry run time 4.78639554977417
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 56.207 seconds
START iteration 60, CKPT_AND_STOP: False
Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 61, CKPT_AND_STOP: False
Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 62, CKPT_AND_STOP: False
Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([2], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Opt ckpt time 49.06334114074707
Process done with return code 0
Parent process ID: 11646 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14274 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 63
dry run time 3.120283603668213
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
 > finished loading checkpoint in 56.000 seconds
Process done with return code 0
Parent process ID: 12094 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14240 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 63
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
dry run time 2.9062349796295166
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
Parent process ID: 12489 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 6840 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 63
dry run time 2.829310417175293
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
32 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 57.857 seconds
START iteration 63, CKPT_AND_STOP: False
Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 64, CKPT_AND_STOP: False
Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 65, CKPT_AND_STOP: False
Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 66, CKPT_AND_STOP: False
Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 67, CKPT_AND_STOP: False
Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 68, CKPT_AND_STOP: False
Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 69, CKPT_AND_STOP: False
Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 70, CKPT_AND_STOP: False
Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 71, CKPT_AND_STOP: False
Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 72, CKPT_AND_STOP: False
Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 73, CKPT_AND_STOP: False
Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 74, CKPT_AND_STOP: False
Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 75, CKPT_AND_STOP: False
Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 76, CKPT_AND_STOP: False
Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 77, CKPT_AND_STOP: False
Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 78, CKPT_AND_STOP: False
Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 79, CKPT_AND_STOP: False
Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 80, CKPT_AND_STOP: False
Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 81, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Finished iteration 82, CKPT_AND_STOP: True, flag: tensor([7], dtype=torch.int32)
Begin to save checkpont and exit
Opt ckpt time 48.391865730285645
Process done with return code 1
Parent process ID: 13029 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14311 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 82
dry run time 3.2078192234039307
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 59.562 seconds
Process done with return code 0
Parent process ID: 13528 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 32 0 1012957.5805664062 97573.47158206406
End of simulation:  Mini-batch time (usec) = 7597541
Min send: 10000000, max send 0
Min long send: 97574, max long send 132431
Min fwd: 6100, max fwd 20465; min bwd 38913, max bwd 52197
Min long fwd: 17844, max long fwd 23576; min long bwd 47901, max long bwd 57977
Time taken by simulation: 6944 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 7.597541}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 7.597541
2 per stage
32 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 2
stage to rank map: 0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,16;1,17;2,18;3,19;4,20;5,21;6,22;7,23;8,24;9,25;10,26;11,27;12,28;13,29;14,30;15,31; --batch-size=32 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 82
dry run time 3.0799288749694824
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
32 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 57.705 seconds
START iteration 82, CKPT_AND_STOP: False
Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 83, CKPT_AND_STOP: False
Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 84, CKPT_AND_STOP: False
Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 85, CKPT_AND_STOP: False
Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 86, CKPT_AND_STOP: False
Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 87, CKPT_AND_STOP: False
Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 88, CKPT_AND_STOP: False
Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 89, CKPT_AND_STOP: False
Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 90, CKPT_AND_STOP: False
Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 91, CKPT_AND_STOP: False
Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 92, CKPT_AND_STOP: False
Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 93, CKPT_AND_STOP: False
Finished iteration 94, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 94, CKPT_AND_STOP: False
Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 95, CKPT_AND_STOP: False
Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 96, CKPT_AND_STOP: False
Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 97, CKPT_AND_STOP: False
Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 98, CKPT_AND_STOP: False
Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 99, CKPT_AND_STOP: False
Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([4], dtype=torch.int32)
Begin to save checkpont and exit
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 4 signal handler called with signal 10
Opt ckpt time 36.47126269340515
Process done with return code 1
Parent process ID: 14117 node: 172.31.22.229
32 cutpoints
Stages 1
Micro-bs 1 Max mem: 128933453516.80006
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 64530397593.59998
Predicted microbatch size for 2: -1
Stages 4
Micro-bs 1 Max mem: 34529746124.799995
Predicted microbatch size for 4: -1
Stages 8
Micro-bs 1 Max mem: 19529420390.4
Predicted microbatch size for 8: -1
Stages 16
Micro-bs 1 Max mem: 12029257523.2
Predicted microbatch size for 16: 1
comm size 4194304
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 0 97573.47158206406
End of simulation:  Mini-batch time (usec) = 9027729
Min send: 10000000, max send 0
Min long send: 97573, max long send 127662
Min fwd: 6433, max fwd 20013; min bwd 37432, max bwd 53429
Min long fwd: 15602, max long fwd 25495; min long bwd 46790, max long bwd 57770
Time taken by simulation: 14345 microseconds

{1: inf, 2: inf, 4: inf, 8: inf, 16: 9.027729}
{1: -1, 2: -1, 4: -1, 8: -1, 16: 1}
best config is: 16 1
expected time is 9.027729
1 per stage
16 servers!
Config:
ranks: range(4, 5)
train batch size: 64
partitions: 16
chunk_size: 1
data depth: 1
stage to rank map: 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;
World size is 16
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=4 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15; --batch-size=64 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 100
dry run time 1.9827399253845215
SHARED WEIGHTS ARE
[(0, 15)]
this rank  4 is part of pipeline replica  0
64 chunks
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
 > finished loading checkpoint in 57.073 seconds
START iteration 100, CKPT_AND_STOP: False
Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 101, CKPT_AND_STOP: False
Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 102, CKPT_AND_STOP: False
Finished iteration 103, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 103, CKPT_AND_STOP: False
Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 104, CKPT_AND_STOP: False
Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 105, CKPT_AND_STOP: False
Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 106, CKPT_AND_STOP: False
Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 107, CKPT_AND_STOP: False
Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 108, CKPT_AND_STOP: False
Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 109, CKPT_AND_STOP: False
Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 110, CKPT_AND_STOP: False
Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 111, CKPT_AND_STOP: False
Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 112, CKPT_AND_STOP: False
Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 113, CKPT_AND_STOP: False
Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 114, CKPT_AND_STOP: False
Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 115, CKPT_AND_STOP: False
Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 116, CKPT_AND_STOP: False
Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 117, CKPT_AND_STOP: False
Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 118, CKPT_AND_STOP: False
Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 119, CKPT_AND_STOP: False
Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 120, CKPT_AND_STOP: False
Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 121, CKPT_AND_STOP: False
Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 122, CKPT_AND_STOP: False
Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 123, CKPT_AND_STOP: False
Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 124, CKPT_AND_STOP: False
Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 125, CKPT_AND_STOP: False
Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 126, CKPT_AND_STOP: False
Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 127, CKPT_AND_STOP: False
Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 128, CKPT_AND_STOP: False
Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 129, CKPT_AND_STOP: False
Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 130, CKPT_AND_STOP: False
Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 131, CKPT_AND_STOP: False
Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 132, CKPT_AND_STOP: False
Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 133, CKPT_AND_STOP: False
Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 134, CKPT_AND_STOP: False
Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 135, CKPT_AND_STOP: False
Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 136, CKPT_AND_STOP: False
Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 137, CKPT_AND_STOP: False
Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 138, CKPT_AND_STOP: False
Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 139, CKPT_AND_STOP: False
Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 140, CKPT_AND_STOP: False
Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 141, CKPT_AND_STOP: False
Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32)
START iteration 142, CKPT_AND_STOP: False
