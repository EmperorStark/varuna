Terminated
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 103, in pretrain
    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider, get_batch_fn)
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 280, in setup_model_and_optimizer
    args.iteration = load_checkpoint(model, optimizer, lr_scheduler)
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/checkpointing.py", line 285, in load_checkpoint
    torch.distributed.barrier()
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2783, in barrier
    work.wait()
RuntimeError: [/opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [172.31.21.109]:61844
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 103, in pretrain
    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider, get_batch_fn)
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 268, in setup_model_and_optimizer
    model, profiler = get_model(model_provider_func, get_batch_fn)
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 172, in get_model
    model = Varuna( model, args.stage_to_rank_map, get_batch_fn, global_batch_size,
  File "/home/ubuntu/varuna/varuna/varuna.py", line 132, in __init__
    self.model.initialize( get_batch_fn, from_cache=from_cache )
  File "/home/ubuntu/varuna/varuna/partitioned_model.py", line 224, in initialize
    self.dry_run(get_batch_fn, from_cache)
  File "/home/ubuntu/varuna/varuna/partitioned_model.py", line 242, in dry_run
    dist.barrier()
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2783, in barrier
    work.wait()
RuntimeError: [/opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [172.31.22.229]:21942
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 96, in pretrain
    train_ds, valid_ds, test_ds = build_train_valid_test_datasets(train_valid_test_dataset_provider)
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 627, in build_train_valid_test_datasets
    pipeline_parallel_size, data_parallel_size = get_varuna_config(args.stage_to_rank_map)
  File "/home/ubuntu/varuna/varuna/utils.py", line 160, in get_varuna_config
    stage_to_rank_map = parse_stage_to_rank_map(stage_to_rank_map_str)
  File "/home/ubuntu/varuna/varuna/utils.py", line 150, in parse_stage_to_rank_map
    assert partitions > 0, "Invalid stage to rank map for Varuna!"
AssertionError: Invalid stage to rank map for Varuna!
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 96, in pretrain
    train_ds, valid_ds, test_ds = build_train_valid_test_datasets(train_valid_test_dataset_provider)
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 627, in build_train_valid_test_datasets
    pipeline_parallel_size, data_parallel_size = get_varuna_config(args.stage_to_rank_map)
  File "/home/ubuntu/varuna/varuna/utils.py", line 160, in get_varuna_config
    stage_to_rank_map = parse_stage_to_rank_map(stage_to_rank_map_str)
  File "/home/ubuntu/varuna/varuna/utils.py", line 150, in parse_stage_to_rank_map
    assert partitions > 0, "Invalid stage to rank map for Varuna!"
AssertionError: Invalid stage to rank map for Varuna!
Terminated
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 90, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 74, in initialize_megatron
    finish_mpu_init()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 119, in _initialize_distributed
    torch.distributed.init_process_group(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    default_pg = _new_process_group_helper(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 703, in _new_process_group_helper
    pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 113
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 90, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 74, in initialize_megatron
    finish_mpu_init()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 119, in _initialize_distributed
    torch.distributed.init_process_group(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    default_pg = _new_process_group_helper(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 703, in _new_process_group_helper
    pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 113
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 90, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 74, in initialize_megatron
    finish_mpu_init()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 119, in _initialize_distributed
    torch.distributed.init_process_group(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    default_pg = _new_process_group_helper(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 703, in _new_process_group_helper
    pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 90
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 90, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 74, in initialize_megatron
    finish_mpu_init()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 119, in _initialize_distributed
    torch.distributed.init_process_group(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    default_pg = _new_process_group_helper(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 703, in _new_process_group_helper
    pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 90
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 90, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 74, in initialize_megatron
    finish_mpu_init()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 119, in _initialize_distributed
    torch.distributed.init_process_group(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    default_pg = _new_process_group_helper(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 703, in _new_process_group_helper
    pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 97
Traceback (most recent call last):
  File "/home/ubuntu/varuna_examples/Megatron-LM/pretrain_gpt2.py", line 170, in <module>
    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/training.py", line 90, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 74, in initialize_megatron
    finish_mpu_init()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/home/ubuntu/varuna_examples/Megatron-LM/megatron/initialize.py", line 119, in _initialize_distributed
    torch.distributed.init_process_group(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    default_pg = _new_process_group_helper(
  File "/opt/conda/envs/varuna/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 703, in _new_process_group_helper
    pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/conda/conda-bld/pytorch_1646755888534/work/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 97
