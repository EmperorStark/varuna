Parent process ID: 18360 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 12 0 2007202.880859375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4913759
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 58711; min bwd 92748, max bwd 103910
Min long fwd: 56704, max long fwd 63285; min long bwd 98353, max long bwd 106648
Time taken by simulation: 371 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 1727535.5224609375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4820532
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69446, max bwd 80083
Min long fwd: 43192, max long fwd 50561; min long bwd 74672, max long bwd 80151
Time taken by simulation: 692 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 25 0 976051.4526367188 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4466988
Min send: 10000000, max send 0
Min long send: 38237, max long send 65217
Min fwd: 19236, max fwd 35580; min bwd 42158, max bwd 57089
Min long fwd: 24076, max long fwd 35713; min long bwd 50505, max long bwd 59414
Time taken by simulation: 1794 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 32 0 700971.3134765625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4198444
Min send: 10000000, max send 0
Min long send: 38109, max long send 68059
Min fwd: 11544, max fwd 27276; min bwd 28686, max bwd 44788
Min long fwd: 22228, max long fwd 29781; min long bwd 41714, max long bwd 48296
Time taken by simulation: 3168 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10417 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 294405.0598144531 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5043807
Min send: 10000000, max send 0
Min long send: 38109, max long send 77793
Min fwd: 803, max fwd 17174; min bwd 10907, max bwd 28118
Min long fwd: 12084, max long fwd 20913; min long bwd 17914, max long bwd 26363
Time taken by simulation: 14273 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46977 microseconds

{1: inf, 2: inf, 3: 4.913759, 4: 4.820532, 6: 4.466988, 8: 4.198444, 12: 5.360725, 16: 5.043807, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.198444
4 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 4
stage to rank map: 0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31; --batch-size=32 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16
using world size: 32 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... None
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 32
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2400000
    validation: 2560
    test:       1280
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.9185206890106201
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
32 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
WARNING: could not find the metadata file s3://spot-checkpoints/gpt/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
 > finished loading checkpoint in 0.251 seconds
setting training data start iteration to 0
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 17754.59 | train/valid/test data iterators: 124.60
training ...
START iteration 0, CKPT_AND_STOP: False
[2022-12-05 12:21:27.126142] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 11617.977
[2022-12-05 12:21:27.126887] iteration        1/   18750 | elapsed time per iteration (ms): 11618.8 | learning rate: 8.000E-07 | lm loss: 1.114050E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 1 iterations memory (MB) | allocated: 3589.96337890625 | max allocated: 5125.26416015625 | reserved: 5522.0 | max reserved: 5522.0
time (ms) | optimizer: 19.19 | batch generator: 6.65
START iteration 1, CKPT_AND_STOP: False
[2022-12-05 12:21:31.807563] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4681.489
[2022-12-05 12:21:31.808182] iteration        2/   18750 | elapsed time per iteration (ms): 4681.3 | learning rate: 1.600E-06 | lm loss: 1.114102E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.09 | batch generator: 2.06
START iteration 2, CKPT_AND_STOP: False
[2022-12-05 12:21:36.502593] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4695.035
[2022-12-05 12:21:36.503143] iteration        3/   18750 | elapsed time per iteration (ms): 4694.9 | learning rate: 2.400E-06 | lm loss: 1.113020E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.07 | batch generator: 2.25
START iteration 3, CKPT_AND_STOP: False
[2022-12-05 12:21:41.149881] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4647.284
[2022-12-05 12:21:41.150416] iteration        4/   18750 | elapsed time per iteration (ms): 4647.3 | learning rate: 3.200E-06 | lm loss: 1.111805E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.05 | batch generator: 2.19
START iteration 4, CKPT_AND_STOP: False
[2022-12-05 12:21:46.719657] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5569.775
[2022-12-05 12:21:46.720318] iteration        5/   18750 | elapsed time per iteration (ms): 5569.9 | learning rate: 4.000E-06 | lm loss: 1.109947E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.07 | batch generator: 2.82
START iteration 5, CKPT_AND_STOP: False
[2022-12-05 12:21:50.358382] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3638.725
[2022-12-05 12:21:50.358891] iteration        6/   18750 | elapsed time per iteration (ms): 3638.6 | learning rate: 4.800E-06 | lm loss: 1.108905E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 1.99
START iteration 6, CKPT_AND_STOP: False
[2022-12-05 12:21:53.938159] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3579.777
[2022-12-05 12:21:53.938543] iteration        7/   18750 | elapsed time per iteration (ms): 3579.6 | learning rate: 5.600E-06 | lm loss: 1.108082E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.18
START iteration 7, CKPT_AND_STOP: False
[2022-12-05 12:21:57.455516] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3517.356
[2022-12-05 12:21:57.456045] iteration        8/   18750 | elapsed time per iteration (ms): 3517.5 | learning rate: 6.400E-06 | lm loss: 1.106456E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.15
START iteration 8, CKPT_AND_STOP: False
[2022-12-05 12:22:01.533584] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4078.070
[2022-12-05 12:22:01.534029] iteration        9/   18750 | elapsed time per iteration (ms): 4078.0 | learning rate: 7.200E-06 | lm loss: 1.105469E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.28
START iteration 9, CKPT_AND_STOP: False
[2022-12-05 12:22:05.664591] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4131.005
[2022-12-05 12:22:05.665129] iteration       10/   18750 | elapsed time per iteration (ms): 4131.1 | learning rate: 8.000E-06 | lm loss: 1.104092E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 1.92
START iteration 10, CKPT_AND_STOP: False
[2022-12-05 12:22:09.425924] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3761.334
[2022-12-05 12:22:09.426491] iteration       11/   18750 | elapsed time per iteration (ms): 3761.3 | learning rate: 8.800E-06 | lm loss: 1.103559E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 1.86
START iteration 11, CKPT_AND_STOP: False
[2022-12-05 12:22:13.084109] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3658.185
[2022-12-05 12:22:13.084608] iteration       12/   18750 | elapsed time per iteration (ms): 3658.1 | learning rate: 9.600E-06 | lm loss: 1.102647E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.28
START iteration 12, CKPT_AND_STOP: False
[2022-12-05 12:22:16.704553] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3620.444
[2022-12-05 12:22:16.704948] iteration       13/   18750 | elapsed time per iteration (ms): 3620.3 | learning rate: 1.040E-05 | lm loss: 1.102173E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.03
START iteration 13, CKPT_AND_STOP: False
[2022-12-05 12:22:21.285845] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4581.293
[2022-12-05 12:22:21.286289] iteration       14/   18750 | elapsed time per iteration (ms): 4581.3 | learning rate: 1.120E-05 | lm loss: 1.101502E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.56
START iteration 14, CKPT_AND_STOP: False
[2022-12-05 12:22:24.913474] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3627.627
[2022-12-05 12:22:24.913847] iteration       15/   18750 | elapsed time per iteration (ms): 3627.5 | learning rate: 1.200E-05 | lm loss: 1.101466E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.17
START iteration 15, CKPT_AND_STOP: False
[2022-12-05 12:22:28.511456] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3597.987
[2022-12-05 12:22:28.511844] iteration       16/   18750 | elapsed time per iteration (ms): 3598.0 | learning rate: 1.280E-05 | lm loss: 1.099956E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.03 | batch generator: 2.08
START iteration 16, CKPT_AND_STOP: False
[2022-12-05 12:22:32.093636] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3582.176
[2022-12-05 12:22:32.094025] iteration       17/   18750 | elapsed time per iteration (ms): 3582.2 | learning rate: 1.360E-05 | lm loss: 1.098608E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.05
START iteration 17, CKPT_AND_STOP: False
[2022-12-05 12:22:35.698892] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3605.256
[2022-12-05 12:22:35.699452] iteration       18/   18750 | elapsed time per iteration (ms): 3605.4 | learning rate: 1.440E-05 | lm loss: 1.099833E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.85
START iteration 18, CKPT_AND_STOP: False
[2022-12-05 12:22:39.283396] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3584.504
[2022-12-05 12:22:39.283778] iteration       19/   18750 | elapsed time per iteration (ms): 3584.3 | learning rate: 1.520E-05 | lm loss: 1.098259E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.03 | batch generator: 2.24
START iteration 19, CKPT_AND_STOP: False
[2022-12-05 12:22:42.854340] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3570.943
[2022-12-05 12:22:42.854758] iteration       20/   18750 | elapsed time per iteration (ms): 3571.0 | learning rate: 1.600E-05 | lm loss: 1.096187E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.96
START iteration 20, CKPT_AND_STOP: False
[2022-12-05 12:22:46.463268] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3608.933
[2022-12-05 12:22:46.463685] iteration       21/   18750 | elapsed time per iteration (ms): 3608.9 | learning rate: 1.680E-05 | lm loss: 1.095847E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 1.91
START iteration 21, CKPT_AND_STOP: False
[2022-12-05 12:22:50.015841] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3552.567
[2022-12-05 12:22:50.016408] iteration       22/   18750 | elapsed time per iteration (ms): 3552.7 | learning rate: 1.760E-05 | lm loss: 1.094913E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.22
START iteration 22, CKPT_AND_STOP: False
[2022-12-05 12:22:53.698261] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3682.422
[2022-12-05 12:22:53.698693] iteration       23/   18750 | elapsed time per iteration (ms): 3682.3 | learning rate: 1.840E-05 | lm loss: 1.094041E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.44
START iteration 23, CKPT_AND_STOP: False
[2022-12-05 12:22:57.279969] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3581.707
[2022-12-05 12:22:57.280382] iteration       24/   18750 | elapsed time per iteration (ms): 3581.7 | learning rate: 1.920E-05 | lm loss: 1.093374E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.24
START iteration 24, CKPT_AND_STOP: False
[2022-12-05 12:23:00.851796] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3571.828
[2022-12-05 12:23:00.852168] iteration       25/   18750 | elapsed time per iteration (ms): 3571.8 | learning rate: 2.000E-05 | lm loss: 1.092244E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.96
START iteration 25, CKPT_AND_STOP: False
[2022-12-05 12:23:04.445878] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3594.081
[2022-12-05 12:23:04.446484] iteration       26/   18750 | elapsed time per iteration (ms): 3594.3 | learning rate: 2.080E-05 | lm loss: 1.091016E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.01 | batch generator: 1.87
START iteration 26, CKPT_AND_STOP: False
[2022-12-05 12:23:08.135555] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3689.677
[2022-12-05 12:23:08.135949] iteration       27/   18750 | elapsed time per iteration (ms): 3689.4 | learning rate: 2.160E-05 | lm loss: 1.089460E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.94
START iteration 27, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 12:23:11.689605] Finished iteration 28, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 3554.051
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      28 to s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
Opt ckpt time 20.16629385948181
Process done with return code 0
Parent process ID: 19390 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 14 0 2006653.3203125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5367030
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 60689; min bwd 93028, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 98397, max long bwd 102857
Time taken by simulation: 422 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 18 0 1481951.2939453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4877293
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69316, max bwd 77937
Min long fwd: 43561, max long fwd 51496; min long bwd 71918, max long bwd 80151
Time taken by simulation: 978 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 876895.5078125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5017062
Min send: 10000000, max send 0
Min long send: 38071, max long send 66276
Min fwd: 18837, max fwd 35859; min bwd 42514, max bwd 56347
Min long fwd: 27753, max long fwd 35222; min long bwd 51765, max long bwd 59571
Time taken by simulation: 2303 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4237 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10621 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30349 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46135 microseconds

{1: inf, 2: inf, 3: 5.36703, 4: 4.877293, 6: 5.017062, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.799842
3 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 3
stage to rank map: 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 28
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 28
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.788536548614502
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
42 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 43.369 seconds
setting training data start iteration to 28
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 60656.01 | train/valid/test data iterators: 202.73
training ...
Process done with return code 0
Parent process ID: 20422 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 2198285.64453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5986016
Min send: 10000000, max send 0
Min long send: 38293, max long send 60521
Min fwd: 45773, max fwd 59456; min bwd 93556, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 97741, max long bwd 104518
Time taken by simulation: 506 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1478731.4453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5493991
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32128, max fwd 48180; min bwd 67776, max bwd 77208
Min long fwd: 43175, max long fwd 51496; min long bwd 74514, max long bwd 81205
Time taken by simulation: 899 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 876895.5078125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5017062
Min send: 10000000, max send 0
Min long send: 38071, max long send 66276
Min fwd: 18837, max fwd 35859; min bwd 42514, max bwd 56347
Min long fwd: 27753, max long fwd 35222; min long bwd 51765, max long bwd 59571
Time taken by simulation: 2278 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4814 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10808 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30793 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46687 microseconds

{1: inf, 2: inf, 3: 5.986016, 4: 5.493991, 6: 5.017062, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.799842
3 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 3
stage to rank map: 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 28
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 28
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.9501330852508545
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
42 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
  successfully loaded s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 40.940 seconds
setting training data start iteration to 28
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 58380.17 | train/valid/test data iterators: 217.45
training ...
Process done with return code 0
Parent process ID: 21527 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 2198285.64453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5986016
Min send: 10000000, max send 0
Min long send: 38293, max long send 60521
Min fwd: 45773, max fwd 59456; min bwd 93556, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 97741, max long bwd 104518
Time taken by simulation: 480 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1478731.4453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5493991
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32128, max fwd 48180; min bwd 67776, max bwd 77208
Min long fwd: 43175, max long fwd 51496; min long bwd 74514, max long bwd 81205
Time taken by simulation: 1023 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 876895.5078125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5017062
Min send: 10000000, max send 0
Min long send: 38071, max long send 66276
Min fwd: 18837, max fwd 35859; min bwd 42514, max bwd 56347
Min long fwd: 27753, max long fwd 35222; min long bwd 51765, max long bwd 59571
Time taken by simulation: 2243 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4334 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10715 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30718 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46355 microseconds

{1: inf, 2: inf, 3: 5.986016, 4: 5.493991, 6: 5.017062, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.799842
3 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 3
stage to rank map: 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 28
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 28
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 1.0298609733581543
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
42 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000028/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 36.532 seconds
setting training data start iteration to 28
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 53975.08 | train/valid/test data iterators: 210.93
training ...
START iteration 28, CKPT_AND_STOP: False
[2022-12-05 12:29:19.260314] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 12541.842
[2022-12-05 12:29:19.261130] iteration       29/   18750 | elapsed time per iteration (ms): 12542.7 | learning rate: 2.320E-05 | lm loss: 1.087743E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 29 iterations memory (MB) | allocated: 3614.34619140625 | max allocated: 6243.17919921875 | reserved: 6842.0 | max reserved: 6842.0
time (ms) | optimizer: 14.42 | batch generator: 8.19
START iteration 29, CKPT_AND_STOP: False
[2022-12-05 12:29:24.335778] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5075.493
[2022-12-05 12:29:24.336382] iteration       30/   18750 | elapsed time per iteration (ms): 5075.2 | learning rate: 2.400E-05 | lm loss: 1.086933E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.06 | batch generator: 4.42
START iteration 30, CKPT_AND_STOP: False
[2022-12-05 12:29:30.938347] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6602.568
[2022-12-05 12:29:30.938949] iteration       31/   18750 | elapsed time per iteration (ms): 6602.5 | learning rate: 2.480E-05 | lm loss: 1.086184E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.12 | batch generator: 4.50
START iteration 31, CKPT_AND_STOP: False
[2022-12-05 12:29:36.122673] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5184.326
[2022-12-05 12:29:36.123263] iteration       32/   18750 | elapsed time per iteration (ms): 5184.3 | learning rate: 2.560E-05 | lm loss: 1.084694E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.08 | batch generator: 2.42
START iteration 32, CKPT_AND_STOP: False
[2022-12-05 12:29:41.502923] Finished iteration 33, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5380.250
[2022-12-05 12:29:41.503523] iteration       33/   18750 | elapsed time per iteration (ms): 5380.2 | learning rate: 2.640E-05 | lm loss: 1.083515E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.17 | batch generator: 4.59
START iteration 33, CKPT_AND_STOP: False
[2022-12-05 12:29:45.604915] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4101.992
[2022-12-05 12:29:45.605402] iteration       34/   18750 | elapsed time per iteration (ms): 4101.9 | learning rate: 2.720E-05 | lm loss: 1.083014E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.12
START iteration 34, CKPT_AND_STOP: False
[2022-12-05 12:29:49.685924] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4081.010
[2022-12-05 12:29:49.686351] iteration       35/   18750 | elapsed time per iteration (ms): 4080.9 | learning rate: 2.800E-05 | lm loss: 1.081792E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.16
START iteration 35, CKPT_AND_STOP: False
[2022-12-05 12:29:53.836862] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4150.939
[2022-12-05 12:29:53.837264] iteration       36/   18750 | elapsed time per iteration (ms): 4150.9 | learning rate: 2.880E-05 | lm loss: 1.080050E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.29
START iteration 36, CKPT_AND_STOP: False
[2022-12-05 12:29:57.931612] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4094.749
[2022-12-05 12:29:57.932006] iteration       37/   18750 | elapsed time per iteration (ms): 4094.7 | learning rate: 2.960E-05 | lm loss: 1.079308E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.96
START iteration 37, CKPT_AND_STOP: False
[2022-12-05 12:30:01.999076] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4067.464
[2022-12-05 12:30:01.999478] iteration       38/   18750 | elapsed time per iteration (ms): 4067.5 | learning rate: 3.040E-05 | lm loss: 1.078750E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.96
START iteration 38, CKPT_AND_STOP: False
[2022-12-05 12:30:06.045438] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4046.360
[2022-12-05 12:30:06.045979] iteration       39/   18750 | elapsed time per iteration (ms): 4046.5 | learning rate: 3.120E-05 | lm loss: 1.077321E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.91
START iteration 39, CKPT_AND_STOP: False
[2022-12-05 12:30:10.124650] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4079.213
[2022-12-05 12:30:10.125035] iteration       40/   18750 | elapsed time per iteration (ms): 4079.0 | learning rate: 3.200E-05 | lm loss: 1.076734E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.26
START iteration 40, CKPT_AND_STOP: False
[2022-12-05 12:30:14.249586] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4124.936
[2022-12-05 12:30:14.250108] iteration       41/   18750 | elapsed time per iteration (ms): 4125.0 | learning rate: 3.280E-05 | lm loss: 1.075451E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.05
START iteration 41, CKPT_AND_STOP: False
[2022-12-05 12:30:18.365368] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4115.764
[2022-12-05 12:30:18.365791] iteration       42/   18750 | elapsed time per iteration (ms): 4115.7 | learning rate: 3.360E-05 | lm loss: 1.074965E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.25
START iteration 42, CKPT_AND_STOP: False
[2022-12-05 12:30:22.483958] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4118.609
[2022-12-05 12:30:22.484378] iteration       43/   18750 | elapsed time per iteration (ms): 4118.6 | learning rate: 3.440E-05 | lm loss: 1.073886E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.15
START iteration 43, CKPT_AND_STOP: False
[2022-12-05 12:30:26.525273] Finished iteration 44, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4041.315
[2022-12-05 12:30:26.525698] iteration       44/   18750 | elapsed time per iteration (ms): 4041.3 | learning rate: 3.520E-05 | lm loss: 1.073371E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.96
START iteration 44, CKPT_AND_STOP: False
[2022-12-05 12:30:30.516235] Finished iteration 45, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3990.961
[2022-12-05 12:30:30.516653] iteration       45/   18750 | elapsed time per iteration (ms): 3990.9 | learning rate: 3.600E-05 | lm loss: 1.071747E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.25
START iteration 45, CKPT_AND_STOP: False
[2022-12-05 12:30:34.600437] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4084.202
[2022-12-05 12:30:34.600864] iteration       46/   18750 | elapsed time per iteration (ms): 4084.2 | learning rate: 3.680E-05 | lm loss: 1.071705E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.01
START iteration 46, CKPT_AND_STOP: False
[2022-12-05 12:30:38.726390] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4125.953
[2022-12-05 12:30:38.726794] iteration       47/   18750 | elapsed time per iteration (ms): 4125.9 | learning rate: 3.760E-05 | lm loss: 1.071118E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 1.81
START iteration 47, CKPT_AND_STOP: False
[2022-12-05 12:30:42.952620] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4226.229
[2022-12-05 12:30:42.953016] iteration       48/   18750 | elapsed time per iteration (ms): 4226.2 | learning rate: 3.840E-05 | lm loss: 1.070346E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.06
START iteration 48, CKPT_AND_STOP: False
[2022-12-05 12:30:47.039117] Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4086.498
[2022-12-05 12:30:47.039502] iteration       49/   18750 | elapsed time per iteration (ms): 4086.5 | learning rate: 3.920E-05 | lm loss: 1.069747E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 1.91
START iteration 49, CKPT_AND_STOP: False
[2022-12-05 12:30:51.070522] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4031.405
[2022-12-05 12:30:51.070982] iteration       50/   18750 | elapsed time per iteration (ms): 4031.5 | learning rate: 4.000E-05 | lm loss: 1.069415E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.83
START iteration 50, CKPT_AND_STOP: False
[2022-12-05 12:30:55.116329] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4045.807
[2022-12-05 12:30:55.116728] iteration       51/   18750 | elapsed time per iteration (ms): 4045.7 | learning rate: 4.080E-05 | lm loss: 1.068944E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.40
START iteration 51, CKPT_AND_STOP: False
[2022-12-05 12:30:59.191018] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4074.688
[2022-12-05 12:30:59.191486] iteration       52/   18750 | elapsed time per iteration (ms): 4074.7 | learning rate: 4.160E-05 | lm loss: 1.068083E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.10
START iteration 52, CKPT_AND_STOP: False
[2022-12-05 12:31:03.260676] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4069.658
[2022-12-05 12:31:03.261143] iteration       53/   18750 | elapsed time per iteration (ms): 4069.6 | learning rate: 4.240E-05 | lm loss: 1.067782E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.28
START iteration 53, CKPT_AND_STOP: False
[2022-12-05 12:31:07.280906] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4020.231
[2022-12-05 12:31:07.281400] iteration       54/   18750 | elapsed time per iteration (ms): 4020.2 | learning rate: 4.320E-05 | lm loss: 1.067307E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.34
START iteration 54, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 12:31:11.342194] Finished iteration 55, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 4061.287
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      55 to s3://spot-checkpoints/gpt/iter_0000055/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000055/mp_rank_00/model_optim_rng.pt
Opt ckpt time 19.640706539154053
Process done with return code 0
Parent process ID: 22887 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 18 0 1846814.0869140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5987018
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 59834; min bwd 93871, max bwd 104244
Min long fwd: 57033, max long fwd 64913; min long bwd 98140, max long bwd 103204
Time taken by simulation: 539 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 25 0 1338646.484375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5804683
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 30866, max fwd 48132; min bwd 66753, max bwd 79482
Min long fwd: 43307, max long fwd 50971; min long bwd 73461, max long bwd 80147
Time taken by simulation: 1068 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 42 0 770127.8076171875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5988334
Min send: 10000000, max send 0
Min long send: 38055, max long send 68059
Min fwd: 18837, max fwd 35859; min bwd 42358, max bwd 55388
Min long fwd: 27768, max long fwd 35092; min long bwd 51025, max long bwd 59414
Time taken by simulation: 2954 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 442992.24853515625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6489103
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 9901, max fwd 27419; min bwd 27605, max bwd 45180
Min long fwd: 21186, max long fwd 32479; min long bwd 41203, max long bwd 48557
Time taken by simulation: 6656 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8759578
Min send: 10000000, max send 0
Min long send: 38055, max long send 72424
Min fwd: 5999, max fwd 22973; min bwd 17039, max bwd 34800
Min long fwd: 12470, max long fwd 25017; min long bwd 25566, max long bwd 36212
Time taken by simulation: 21548 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 29731 microseconds

can't have 24 stages!
{1: inf, 2: inf, 3: 5.987018, 4: 5.804683, 6: 5.988334, 8: 6.489103, 12: 8.759578, 16: 7.580423}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1}
best config is: 4 1
expected time is 5.804683
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 1
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 55
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 55
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.87900710105896
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000055/mp_rank_00/model_optim_rng.pt
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000055/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 64.960 seconds
setting training data start iteration to 55
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 82408.28 | train/valid/test data iterators: 275.89
training ...
Process done with return code 0
Parent process ID: 24228 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 18 0 1846814.0869140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5987018
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 59834; min bwd 93871, max bwd 104244
Min long fwd: 57033, max long fwd 64913; min long bwd 98140, max long bwd 103204
Time taken by simulation: 533 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 25 0 1338646.484375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5804683
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 30866, max fwd 48132; min bwd 66753, max bwd 79482
Min long fwd: 43307, max long fwd 50971; min long bwd 73461, max long bwd 80147
Time taken by simulation: 1126 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 42 0 770127.8076171875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5988334
Min send: 10000000, max send 0
Min long send: 38055, max long send 68059
Min fwd: 18837, max fwd 35859; min bwd 42358, max bwd 55388
Min long fwd: 27768, max long fwd 35092; min long bwd 51025, max long bwd 59414
Time taken by simulation: 2992 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 442992.24853515625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6489103
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 9901, max fwd 27419; min bwd 27605, max bwd 45180
Min long fwd: 21186, max long fwd 32479; min long bwd 41203, max long bwd 48557
Time taken by simulation: 6795 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8759578
Min send: 10000000, max send 0
Min long send: 38055, max long send 72424
Min fwd: 5999, max fwd 22973; min bwd 17039, max bwd 34800
Min long fwd: 12470, max long fwd 25017; min long bwd 25566, max long bwd 36212
Time taken by simulation: 21646 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 29835 microseconds

can't have 24 stages!
{1: inf, 2: inf, 3: 5.987018, 4: 5.804683, 6: 5.988334, 8: 6.489103, 12: 8.759578, 16: 7.580423}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1}
best config is: 4 1
expected time is 5.804683
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 1
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 55
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 55
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 1.2388837337493896
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000055/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000055/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 82.544 seconds
setting training data start iteration to 55
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 100388.19 | train/valid/test data iterators: 285.94
training ...
START iteration 55, CKPT_AND_STOP: False
[2022-12-05 12:36:16.794494] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9406.677
[2022-12-05 12:36:16.795321] iteration       56/   18750 | elapsed time per iteration (ms): 9407.5 | learning rate: 4.480E-05 | lm loss: 1.066373E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 56 iterations memory (MB) | allocated: 6137.60009765625 | max allocated: 10903.54638671875 | reserved: 11818.0 | max reserved: 11818.0
time (ms) | optimizer: 23.83 | batch generator: 5.75
START iteration 56, CKPT_AND_STOP: False
[2022-12-05 12:36:22.861076] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6066.602
[2022-12-05 12:36:22.861711] iteration       57/   18750 | elapsed time per iteration (ms): 6066.4 | learning rate: 4.560E-05 | lm loss: 1.066178E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.95 | batch generator: 2.05
START iteration 57, CKPT_AND_STOP: False
[2022-12-05 12:36:28.820856] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5959.780
[2022-12-05 12:36:28.821528] iteration       58/   18750 | elapsed time per iteration (ms): 5959.8 | learning rate: 4.640E-05 | lm loss: 1.065809E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.97 | batch generator: 2.12
START iteration 58, CKPT_AND_STOP: False
[2022-12-05 12:36:34.856339] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6035.484
[2022-12-05 12:36:34.856922] iteration       59/   18750 | elapsed time per iteration (ms): 6035.4 | learning rate: 4.720E-05 | lm loss: 1.066163E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.77 | batch generator: 1.93
START iteration 59, CKPT_AND_STOP: False
[2022-12-05 12:36:40.798128] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5941.787
[2022-12-05 12:36:40.798729] iteration       60/   18750 | elapsed time per iteration (ms): 5941.8 | learning rate: 4.800E-05 | lm loss: 1.065574E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.75 | batch generator: 2.05
START iteration 60, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 12:36:45.729639] Finished iteration 61, CKPT_AND_STOP: True, flag: tensor([6], dtype=torch.int32), speed: 4931.514
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      61 to s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
Opt ckpt time 21.27012538909912
Process done with return code 0
Parent process ID: 25638 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 2198285.64453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5986016
Min send: 10000000, max send 0
Min long send: 38293, max long send 60521
Min fwd: 45773, max fwd 59456; min bwd 93556, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 97741, max long bwd 104518
Time taken by simulation: 517 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1478731.4453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5493991
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32128, max fwd 48180; min bwd 67776, max bwd 77208
Min long fwd: 43175, max long fwd 51496; min long bwd 74514, max long bwd 81205
Time taken by simulation: 906 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 876895.5078125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5017062
Min send: 10000000, max send 0
Min long send: 38071, max long send 66276
Min fwd: 18837, max fwd 35859; min bwd 42514, max bwd 56347
Min long fwd: 27753, max long fwd 35222; min long bwd 51765, max long bwd 59571
Time taken by simulation: 2315 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4385 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10895 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30689 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 49046 microseconds

{1: inf, 2: inf, 3: 5.986016, 4: 5.493991, 6: 5.017062, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.799842
3 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 3
stage to rank map: 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 61
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 61
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.8764653205871582
SHARED WEIGHTS ARE
[(0, 7)]
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
this rank  0 is part of pipeline replica  0
42 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 38.568 seconds
setting training data start iteration to 61
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 55931.39 | train/valid/test data iterators: 227.28
training ...
Process done with return code 0
Parent process ID: 26662 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 16 0 2198285.64453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5986016
Min send: 10000000, max send 0
Min long send: 38293, max long send 60521
Min fwd: 45773, max fwd 59456; min bwd 93556, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 97741, max long bwd 104518
Time taken by simulation: 476 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 21 0 1478731.4453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5493991
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32128, max fwd 48180; min bwd 67776, max bwd 77208
Min long fwd: 43175, max long fwd 51496; min long bwd 74514, max long bwd 81205
Time taken by simulation: 1027 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 876895.5078125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5017062
Min send: 10000000, max send 0
Min long send: 38071, max long send 66276
Min fwd: 18837, max fwd 35859; min bwd 42514, max bwd 56347
Min long fwd: 27753, max long fwd 35222; min long bwd 51765, max long bwd 59571
Time taken by simulation: 2272 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4455 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10591 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30306 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 47541 microseconds

{1: inf, 2: inf, 3: 5.986016, 4: 5.493991, 6: 5.017062, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.799842
3 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 3
stage to rank map: 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 61
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 61
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.873974084854126
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
42 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 35.934 seconds
setting training data start iteration to 61
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 53258.06 | train/valid/test data iterators: 235.45
training ...
Process done with return code 0
Parent process ID: 27711 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 18 0 1846814.0869140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5987018
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 59834; min bwd 93871, max bwd 104244
Min long fwd: 57033, max long fwd 64913; min long bwd 98140, max long bwd 103204
Time taken by simulation: 539 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 25 0 1338646.484375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5804683
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 30866, max fwd 48132; min bwd 66753, max bwd 79482
Min long fwd: 43307, max long fwd 50971; min long bwd 73461, max long bwd 80147
Time taken by simulation: 1071 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 42 0 770127.8076171875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5988334
Min send: 10000000, max send 0
Min long send: 38055, max long send 68059
Min fwd: 18837, max fwd 35859; min bwd 42358, max bwd 55388
Min long fwd: 27768, max long fwd 35092; min long bwd 51025, max long bwd 59414
Time taken by simulation: 2990 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 442992.24853515625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6489103
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 9901, max fwd 27419; min bwd 27605, max bwd 45180
Min long fwd: 21186, max long fwd 32479; min long bwd 41203, max long bwd 48557
Time taken by simulation: 6582 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8759578
Min send: 10000000, max send 0
Min long send: 38055, max long send 72424
Min fwd: 5999, max fwd 22973; min bwd 17039, max bwd 34800
Min long fwd: 12470, max long fwd 25017; min long bwd 25566, max long bwd 36212
Time taken by simulation: 21627 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30586 microseconds

can't have 24 stages!
{1: inf, 2: inf, 3: 5.987018, 4: 5.804683, 6: 5.988334, 8: 6.489103, 12: 8.759578, 16: 7.580423}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1}
best config is: 4 1
expected time is 5.804683
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 1
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 61
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 61
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.9174857139587402
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
Process done with return code 1
Parent process ID: 29182 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 21 0 1916562.744140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6803052
Min send: 10000000, max send 0
Min long send: 38109, max long send 62549
Min fwd: 45773, max fwd 59456; min bwd 92874, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 96930, max long bwd 106927
Time taken by simulation: 631 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 25 0 1338646.484375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5804683
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 30866, max fwd 48132; min bwd 66753, max bwd 79482
Min long fwd: 43307, max long fwd 50971; min long bwd 73461, max long bwd 80147
Time taken by simulation: 1060 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 42 0 770127.8076171875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5988334
Min send: 10000000, max send 0
Min long send: 38055, max long send 68059
Min fwd: 18837, max fwd 35859; min bwd 42358, max bwd 55388
Min long fwd: 27768, max long fwd 35092; min long bwd 51025, max long bwd 59414
Time taken by simulation: 3073 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 442992.24853515625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6489103
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 9901, max fwd 27419; min bwd 27605, max bwd 45180
Min long fwd: 21186, max long fwd 32479; min long bwd 41203, max long bwd 48557
Time taken by simulation: 6486 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8759578
Min send: 10000000, max send 0
Min long send: 38055, max long send 72424
Min fwd: 5999, max fwd 22973; min bwd 17039, max bwd 34800
Min long fwd: 12470, max long fwd 25017; min long bwd 25566, max long bwd 36212
Time taken by simulation: 21839 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 29728 microseconds

can't have 24 stages!
{1: inf, 2: inf, 3: 6.803052, 4: 5.804683, 6: 5.988334, 8: 6.489103, 12: 8.759578, 16: 7.580423}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1}
best config is: 4 1
expected time is 5.804683
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 1
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 61
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 61
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
dry run time 1.331519365310669
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 68.034 seconds
setting training data start iteration to 61
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 86187.67 | train/valid/test data iterators: 289.53
training ...
Process done with return code 0
Parent process ID: 30600 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 18 0 1846814.0869140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5987018
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 59834; min bwd 93871, max bwd 104244
Min long fwd: 57033, max long fwd 64913; min long bwd 98140, max long bwd 103204
Time taken by simulation: 539 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 25 0 1338646.484375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5804683
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 30866, max fwd 48132; min bwd 66753, max bwd 79482
Min long fwd: 43307, max long fwd 50971; min long bwd 73461, max long bwd 80147
Time taken by simulation: 1066 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 42 0 770127.8076171875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5988334
Min send: 10000000, max send 0
Min long send: 38055, max long send 68059
Min fwd: 18837, max fwd 35859; min bwd 42358, max bwd 55388
Min long fwd: 27768, max long fwd 35092; min long bwd 51025, max long bwd 59414
Time taken by simulation: 2976 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 442992.24853515625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6489103
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 9901, max fwd 27419; min bwd 27605, max bwd 45180
Min long fwd: 21186, max long fwd 32479; min long bwd 41203, max long bwd 48557
Time taken by simulation: 6723 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8759578
Min send: 10000000, max send 0
Min long send: 38055, max long send 72424
Min fwd: 5999, max fwd 22973; min bwd 17039, max bwd 34800
Min long fwd: 12470, max long fwd 25017; min long bwd 25566, max long bwd 36212
Time taken by simulation: 21385 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30090 microseconds

can't have 24 stages!
{1: inf, 2: inf, 3: 5.987018, 4: 5.804683, 6: 5.988334, 8: 6.489103, 12: 8.759578, 16: 7.580423}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1}
best config is: 4 1
expected time is 5.804683
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 1
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 61
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 61
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.9569053649902344
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000061/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 86.336 seconds
setting training data start iteration to 61
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 103864.13 | train/valid/test data iterators: 291.47
training ...
START iteration 61, CKPT_AND_STOP: False
[2022-12-05 12:46:02.448388] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9485.488
[2022-12-05 12:46:02.449151] iteration       62/   18750 | elapsed time per iteration (ms): 9486.3 | learning rate: 4.960E-05 | lm loss: 1.065259E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 62 iterations memory (MB) | allocated: 6137.60009765625 | max allocated: 10903.54638671875 | reserved: 11818.0 | max reserved: 11818.0
time (ms) | optimizer: 24.23 | batch generator: 5.61
START iteration 62, CKPT_AND_STOP: False
[2022-12-05 12:46:08.359712] Finished iteration 63, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5911.351
[2022-12-05 12:46:08.360265] iteration       63/   18750 | elapsed time per iteration (ms): 5911.1 | learning rate: 5.040E-05 | lm loss: 1.065049E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.92
START iteration 63, CKPT_AND_STOP: False
[2022-12-05 12:46:14.255454] Finished iteration 64, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5895.742
[2022-12-05 12:46:14.256085] iteration       64/   18750 | elapsed time per iteration (ms): 5895.8 | learning rate: 5.120E-05 | lm loss: 1.064808E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.92 | batch generator: 1.91
START iteration 64, CKPT_AND_STOP: False
[2022-12-05 12:46:20.175480] Finished iteration 65, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5920.027
[2022-12-05 12:46:20.176052] iteration       65/   18750 | elapsed time per iteration (ms): 5919.9 | learning rate: 5.200E-05 | lm loss: 1.064508E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 2.39
START iteration 65, CKPT_AND_STOP: False
[2022-12-05 12:46:26.127986] Finished iteration 66, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5952.507
[2022-12-05 12:46:26.128423] iteration       66/   18750 | elapsed time per iteration (ms): 5952.3 | learning rate: 5.280E-05 | lm loss: 1.064466E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.85 | batch generator: 2.28
START iteration 66, CKPT_AND_STOP: False
[2022-12-05 12:46:30.992212] Finished iteration 67, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4864.224
[2022-12-05 12:46:30.992759] iteration       67/   18750 | elapsed time per iteration (ms): 4864.3 | learning rate: 5.360E-05 | lm loss: 1.064060E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.71 | batch generator: 2.11
START iteration 67, CKPT_AND_STOP: False
[2022-12-05 12:46:35.869286] Finished iteration 68, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4877.074
[2022-12-05 12:46:35.869729] iteration       68/   18750 | elapsed time per iteration (ms): 4877.0 | learning rate: 5.440E-05 | lm loss: 1.064211E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 2.19
START iteration 68, CKPT_AND_STOP: False
[2022-12-05 12:46:40.810151] Finished iteration 69, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4940.866
[2022-12-05 12:46:40.810592] iteration       69/   18750 | elapsed time per iteration (ms): 4940.8 | learning rate: 5.520E-05 | lm loss: 1.064059E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 2.22
START iteration 69, CKPT_AND_STOP: False
[2022-12-05 12:46:45.691123] Finished iteration 70, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4880.973
[2022-12-05 12:46:45.691570] iteration       70/   18750 | elapsed time per iteration (ms): 4881.0 | learning rate: 5.600E-05 | lm loss: 1.064071E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.62 | batch generator: 2.00
START iteration 70, CKPT_AND_STOP: False
[2022-12-05 12:46:50.676936] Finished iteration 71, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4985.813
[2022-12-05 12:46:50.677582] iteration       71/   18750 | elapsed time per iteration (ms): 4986.0 | learning rate: 5.680E-05 | lm loss: 1.063785E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 2.18
START iteration 71, CKPT_AND_STOP: False
[2022-12-05 12:46:55.653363] Finished iteration 72, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4976.427
[2022-12-05 12:46:55.653793] iteration       72/   18750 | elapsed time per iteration (ms): 4976.2 | learning rate: 5.760E-05 | lm loss: 1.062318E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 2.06
START iteration 72, CKPT_AND_STOP: False
[2022-12-05 12:47:00.530470] Finished iteration 73, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4877.106
[2022-12-05 12:47:00.531085] iteration       73/   18750 | elapsed time per iteration (ms): 4877.3 | learning rate: 5.840E-05 | lm loss: 1.063456E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.87
START iteration 73, CKPT_AND_STOP: False
[2022-12-05 12:47:05.487680] Finished iteration 74, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4957.211
[2022-12-05 12:47:05.488166] iteration       74/   18750 | elapsed time per iteration (ms): 4957.1 | learning rate: 5.920E-05 | lm loss: 1.063276E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.68 | batch generator: 1.92
START iteration 74, CKPT_AND_STOP: False
[2022-12-05 12:47:10.424859] Finished iteration 75, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4937.179
[2022-12-05 12:47:10.425269] iteration       75/   18750 | elapsed time per iteration (ms): 4937.1 | learning rate: 6.000E-05 | lm loss: 1.063537E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.70 | batch generator: 1.89
START iteration 75, CKPT_AND_STOP: False
[2022-12-05 12:47:15.286610] Finished iteration 76, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4861.750
[2022-12-05 12:47:15.287072] iteration       76/   18750 | elapsed time per iteration (ms): 4861.8 | learning rate: 6.080E-05 | lm loss: 1.063254E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.91
START iteration 76, CKPT_AND_STOP: False
[2022-12-05 12:47:20.146407] Finished iteration 77, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4859.797
[2022-12-05 12:47:20.146860] iteration       77/   18750 | elapsed time per iteration (ms): 4859.8 | learning rate: 6.160E-05 | lm loss: 1.063336E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.64 | batch generator: 2.23
START iteration 77, CKPT_AND_STOP: False
[2022-12-05 12:47:25.092827] Finished iteration 78, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4946.420
[2022-12-05 12:47:25.093272] iteration       78/   18750 | elapsed time per iteration (ms): 4946.4 | learning rate: 6.240E-05 | lm loss: 1.063138E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.96
START iteration 78, CKPT_AND_STOP: False
[2022-12-05 12:47:29.940240] Finished iteration 79, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4847.414
[2022-12-05 12:47:29.940656] iteration       79/   18750 | elapsed time per iteration (ms): 4847.4 | learning rate: 6.320E-05 | lm loss: 1.063136E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.62 | batch generator: 2.06
START iteration 79, CKPT_AND_STOP: False
[2022-12-05 12:47:34.839164] Finished iteration 80, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4898.923
[2022-12-05 12:47:34.839581] iteration       80/   18750 | elapsed time per iteration (ms): 4898.9 | learning rate: 6.400E-05 | lm loss: 1.063093E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.75 | batch generator: 2.02
START iteration 80, CKPT_AND_STOP: False
[2022-12-05 12:47:39.745634] Finished iteration 81, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4906.471
[2022-12-05 12:47:39.746058] iteration       81/   18750 | elapsed time per iteration (ms): 4906.5 | learning rate: 6.480E-05 | lm loss: 1.062922E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.95
START iteration 81, CKPT_AND_STOP: False
[2022-12-05 12:47:44.672953] Finished iteration 82, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4927.319
[2022-12-05 12:47:44.673423] iteration       82/   18750 | elapsed time per iteration (ms): 4927.3 | learning rate: 6.560E-05 | lm loss: 1.062758E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.90
START iteration 82, CKPT_AND_STOP: False
[2022-12-05 12:47:49.600292] Finished iteration 83, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4927.339
[2022-12-05 12:47:49.600716] iteration       83/   18750 | elapsed time per iteration (ms): 4927.3 | learning rate: 6.640E-05 | lm loss: 1.062539E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.98
START iteration 83, CKPT_AND_STOP: False
[2022-12-05 12:47:54.448870] Finished iteration 84, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4848.578
[2022-12-05 12:47:54.449348] iteration       84/   18750 | elapsed time per iteration (ms): 4848.6 | learning rate: 6.720E-05 | lm loss: 1.062803E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.83
START iteration 84, CKPT_AND_STOP: False
[2022-12-05 12:47:59.357257] Finished iteration 85, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4908.386
[2022-12-05 12:47:59.357680] iteration       85/   18750 | elapsed time per iteration (ms): 4908.3 | learning rate: 6.800E-05 | lm loss: 1.062622E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.81
START iteration 85, CKPT_AND_STOP: False
[2022-12-05 12:48:04.236853] Finished iteration 86, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4879.596
[2022-12-05 12:48:04.237272] iteration       86/   18750 | elapsed time per iteration (ms): 4879.6 | learning rate: 6.880E-05 | lm loss: 1.062659E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.61 | batch generator: 1.78
START iteration 86, CKPT_AND_STOP: False
[2022-12-05 12:48:09.140504] Finished iteration 87, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4903.651
[2022-12-05 12:48:09.140898] iteration       87/   18750 | elapsed time per iteration (ms): 4903.6 | learning rate: 6.960E-05 | lm loss: 1.062629E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.73
START iteration 87, CKPT_AND_STOP: False
[2022-12-05 12:48:14.076753] Finished iteration 88, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4936.250
[2022-12-05 12:48:14.077194] iteration       88/   18750 | elapsed time per iteration (ms): 4936.3 | learning rate: 7.040E-05 | lm loss: 1.062860E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.62 | batch generator: 1.87
START iteration 88, CKPT_AND_STOP: False
[2022-12-05 12:48:19.068029] Finished iteration 89, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4991.274
[2022-12-05 12:48:19.068566] iteration       89/   18750 | elapsed time per iteration (ms): 4991.4 | learning rate: 7.120E-05 | lm loss: 1.062392E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.67 | batch generator: 1.75
START iteration 89, CKPT_AND_STOP: False
[2022-12-05 12:48:23.972470] Finished iteration 90, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4904.440
[2022-12-05 12:48:23.973093] iteration       90/   18750 | elapsed time per iteration (ms): 4904.5 | learning rate: 7.200E-05 | lm loss: 1.062199E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.61 | batch generator: 2.32
START iteration 90, CKPT_AND_STOP: False
[2022-12-05 12:48:28.872103] Finished iteration 91, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4899.635
[2022-12-05 12:48:28.872494] iteration       91/   18750 | elapsed time per iteration (ms): 4899.4 | learning rate: 7.280E-05 | lm loss: 1.062533E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 1.94
START iteration 91, CKPT_AND_STOP: False
[2022-12-05 12:48:33.863424] Finished iteration 92, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4991.321
[2022-12-05 12:48:33.863864] iteration       92/   18750 | elapsed time per iteration (ms): 4991.3 | learning rate: 7.360E-05 | lm loss: 1.062291E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.66 | batch generator: 1.87
START iteration 92, CKPT_AND_STOP: False
[2022-12-05 12:48:38.756599] Finished iteration 93, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4893.175
[2022-12-05 12:48:38.757197] iteration       93/   18750 | elapsed time per iteration (ms): 4893.3 | learning rate: 7.440E-05 | lm loss: 1.062403E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.63 | batch generator: 2.02
START iteration 93, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 12:48:43.611818] Finished iteration 94, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 4855.219
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration      94 to s3://spot-checkpoints/gpt/iter_0000094/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000094/mp_rank_00/model_optim_rng.pt
Opt ckpt time 22.89843487739563
Process done with return code 0
Parent process ID: 32443 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 18 0 1846814.0869140625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5987018
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 59834; min bwd 93871, max bwd 104244
Min long fwd: 57033, max long fwd 64913; min long bwd 98140, max long bwd 103204
Time taken by simulation: 567 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 25 0 1338646.484375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5804683
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 30866, max fwd 48132; min bwd 66753, max bwd 79482
Min long fwd: 43307, max long fwd 50971; min long bwd 73461, max long bwd 80147
Time taken by simulation: 1070 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 42 0 770127.8076171875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5988334
Min send: 10000000, max send 0
Min long send: 38055, max long send 68059
Min fwd: 18837, max fwd 35859; min bwd 42358, max bwd 55388
Min long fwd: 27768, max long fwd 35092; min long bwd 51025, max long bwd 59414
Time taken by simulation: 3131 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 64 0 442992.24853515625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6489103
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 9901, max fwd 27419; min bwd 27605, max bwd 45180
Min long fwd: 21186, max long fwd 32479; min long bwd 41203, max long bwd 48557
Time taken by simulation: 6525 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 8759578
Min send: 10000000, max send 0
Min long send: 38055, max long send 72424
Min fwd: 5999, max fwd 22973; min bwd 17039, max bwd 34800
Min long fwd: 12470, max long fwd 25017; min long bwd 25566, max long bwd 36212
Time taken by simulation: 21531 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30481 microseconds

can't have 24 stages!
{1: inf, 2: inf, 3: 5.987018, 4: 5.804683, 6: 5.988334, 8: 6.489103, 12: 8.759578, 16: 7.580423}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1}
best config is: 4 1
expected time is 5.804683
5 per stage
20 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 4
chunk_size: 1
data depth: 5
stage to rank map: 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
World size is 20
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 94
using world size: 20 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 94
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,4,8,12,16;1,5,9,13,17;2,6,10,14,18;3,7,11,15,19;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 20
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 1.3001744747161865
SHARED WEIGHTS ARE
[(0, 3)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 451014400
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000094/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000094/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 70.762 seconds
setting training data start iteration to 94
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 88526.79 | train/valid/test data iterators: 327.25
training ...
START iteration 94, CKPT_AND_STOP: False
[2022-12-05 12:51:01.028998] Finished iteration 95, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 9615.845
[2022-12-05 12:51:01.029849] iteration       95/   18750 | elapsed time per iteration (ms): 9616.7 | learning rate: 7.600E-05 | lm loss: 1.062307E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 95 iterations memory (MB) | allocated: 6137.60009765625 | max allocated: 10903.54638671875 | reserved: 11818.0 | max reserved: 11818.0
time (ms) | optimizer: 23.93 | batch generator: 5.42
START iteration 95, CKPT_AND_STOP: False
[2022-12-05 12:51:07.015379] Finished iteration 96, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5986.416
[2022-12-05 12:51:07.016051] iteration       96/   18750 | elapsed time per iteration (ms): 5986.2 | learning rate: 7.680E-05 | lm loss: 1.062375E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 24.05 | batch generator: 1.85
START iteration 96, CKPT_AND_STOP: False
[2022-12-05 12:51:12.958216] Finished iteration 97, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5942.839
[2022-12-05 12:51:12.958781] iteration       97/   18750 | elapsed time per iteration (ms): 5942.7 | learning rate: 7.760E-05 | lm loss: 1.062142E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 24.04 | batch generator: 1.87
START iteration 97, CKPT_AND_STOP: False
[2022-12-05 12:51:18.940163] Finished iteration 98, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5981.947
[2022-12-05 12:51:18.940738] iteration       98/   18750 | elapsed time per iteration (ms): 5981.9 | learning rate: 7.840E-05 | lm loss: 1.062037E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.76 | batch generator: 2.03
START iteration 98, CKPT_AND_STOP: False
[2022-12-05 12:51:24.753581] Finished iteration 99, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5813.417
[2022-12-05 12:51:24.754219] iteration       99/   18750 | elapsed time per iteration (ms): 5813.5 | learning rate: 7.920E-05 | lm loss: 1.061893E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.78 | batch generator: 2.42
START iteration 99, CKPT_AND_STOP: False
[2022-12-05 12:51:29.627486] Finished iteration 100, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4873.905
[2022-12-05 12:51:29.628014] iteration      100/   18750 | elapsed time per iteration (ms): 4873.8 | learning rate: 8.000E-05 | lm loss: 1.062098E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.72 | batch generator: 2.21
START iteration 100, CKPT_AND_STOP: False
[2022-12-05 12:51:34.469179] Finished iteration 101, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4841.694
[2022-12-05 12:51:34.469737] iteration      101/   18750 | elapsed time per iteration (ms): 4841.7 | learning rate: 8.080E-05 | lm loss: 1.062014E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 2.22
START iteration 101, CKPT_AND_STOP: False
[2022-12-05 12:51:39.348616] Finished iteration 102, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4879.438
[2022-12-05 12:51:39.349009] iteration      102/   18750 | elapsed time per iteration (ms): 4879.3 | learning rate: 8.160E-05 | lm loss: 1.062055E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 23.65 | batch generator: 2.21
START iteration 102, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-12-05 12:51:44.176739] Finished iteration 103, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 4828.123
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     103 to s3://spot-checkpoints/gpt/iter_0000103/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000103/mp_rank_00/model_optim_rng.pt
Opt ckpt time 19.97672152519226
Process done with return code 0
Parent process ID: 34271 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 12 0 2007202.880859375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4913759
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 58711; min bwd 92748, max bwd 103910
Min long fwd: 56704, max long fwd 63285; min long bwd 98353, max long bwd 106648
Time taken by simulation: 381 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 18 0 1481951.2939453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4877293
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69316, max bwd 77937
Min long fwd: 43561, max long fwd 51496; min long bwd 71918, max long bwd 80151
Time taken by simulation: 794 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 25 0 976051.4526367188 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4466988
Min send: 10000000, max send 0
Min long send: 38237, max long send 65217
Min fwd: 19236, max fwd 35580; min bwd 42158, max bwd 57089
Min long fwd: 24076, max long fwd 35713; min long bwd 50505, max long bwd 59414
Time taken by simulation: 1764 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4265 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10836 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 31146 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46743 microseconds

{1: inf, 2: inf, 3: 4.913759, 4: 4.877293, 6: 4.466988, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 6 1
expected time is 4.466988
5 per stage
30 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 6
chunk_size: 1
data depth: 5
stage to rank map: 0,6,12,18,24;1,7,13,19,25;2,8,14,20,26;3,9,15,21,27;4,10,16,22,28;5,11,17,23,29;
World size is 30
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,6,12,18,24;1,7,13,19,25;2,8,14,20,26;3,9,15,21,27;4,10,16,22,28;5,11,17,23,29; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 103
using world size: 30 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 103
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,6,12,18,24;1,7,13,19,25;2,8,14,20,26;3,9,15,21,27;4,10,16,22,28;5,11,17,23,29;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 30
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.9197845458984375
SHARED WEIGHTS ARE
[(0, 5)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 328051200
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000103/mp_rank_00/model_optim_rng.pt
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000103/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 50.450 seconds
setting training data start iteration to 103
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 68068.73 | train/valid/test data iterators: 271.97
training ...
Process done with return code 0
Parent process ID: 35638 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 12 0 2007202.880859375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4913759
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 58711; min bwd 92748, max bwd 103910
Min long fwd: 56704, max long fwd 63285; min long bwd 98353, max long bwd 106648
Time taken by simulation: 424 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 1727535.5224609375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4820532
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69446, max bwd 80083
Min long fwd: 43192, max long fwd 50561; min long bwd 74672, max long bwd 80151
Time taken by simulation: 691 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 25 0 976051.4526367188 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4466988
Min send: 10000000, max send 0
Min long send: 38237, max long send 65217
Min fwd: 19236, max fwd 35580; min bwd 42158, max bwd 57089
Min long fwd: 24076, max long fwd 35713; min long bwd 50505, max long bwd 59414
Time taken by simulation: 2041 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 32 0 700971.3134765625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4198444
Min send: 10000000, max send 0
Min long send: 38109, max long send 68059
Min fwd: 11544, max fwd 27276; min bwd 28686, max bwd 44788
Min long fwd: 22228, max long fwd 29781; min long bwd 41714, max long bwd 48296
Time taken by simulation: 3209 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 11235 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 294405.0598144531 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5043807
Min send: 10000000, max send 0
Min long send: 38109, max long send 77793
Min fwd: 803, max fwd 17174; min bwd 10907, max bwd 28118
Min long fwd: 12084, max long fwd 20913; min long bwd 17914, max long bwd 26363
Time taken by simulation: 14490 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46989 microseconds

{1: inf, 2: inf, 3: 4.913759, 4: 4.820532, 6: 4.466988, 8: 4.198444, 12: 5.360725, 16: 5.043807, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.198444
4 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 4
stage to rank map: 0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31; --batch-size=32 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 103
using world size: 32 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 103
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 32
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2400000
    validation: 2560
    test:       1280
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.8223836421966553
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
32 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000103/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000103/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 43.995 seconds
setting training data start iteration to 103
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 61644.04 | train/valid/test data iterators: 249.32
training ...
START iteration 103, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-12-05 12:55:05.372213] Finished iteration 104, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 11978.581
[2022-12-05 12:55:05.374736] iteration      104/   18750 | elapsed time per iteration (ms): 11981.1 | learning rate: 8.160E-05 | loss scale: 65536.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
after 104 iterations memory (MB) | allocated: 3614.150390625 | max allocated: 6242.98388671875 | reserved: 6842.0 | max reserved: 6842.0
time (ms) | optimizer: 2.68 | batch generator: 6.84
START iteration 104, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-12-05 12:55:10.048912] Finished iteration 105, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4676.728
[2022-12-05 12:55:10.049556] iteration      105/   18750 | elapsed time per iteration (ms): 4674.8 | learning rate: 8.160E-05 | loss scale: 32768.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | optimizer: 3.68 | batch generator: 2.26
START iteration 105, CKPT_AND_STOP: False
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-12-05 12:55:14.678560] Finished iteration 106, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4629.650
[2022-12-05 12:55:14.678993] iteration      106/   18750 | elapsed time per iteration (ms): 4629.4 | learning rate: 8.160E-05 | loss scale: 16384.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | optimizer: 2.70 | batch generator: 3.89
START iteration 106, CKPT_AND_STOP: False
[2022-12-05 12:55:19.347980] Finished iteration 107, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4669.420
[2022-12-05 12:55:19.348551] iteration      107/   18750 | elapsed time per iteration (ms): 4669.5 | learning rate: 8.240E-05 | lm loss: 1.078639E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.59 | batch generator: 2.71
START iteration 107, CKPT_AND_STOP: False
[2022-12-05 12:55:24.012499] Finished iteration 108, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4664.519
[2022-12-05 12:55:24.012928] iteration      108/   18750 | elapsed time per iteration (ms): 4664.3 | learning rate: 8.320E-05 | lm loss: 1.061315E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.03 | batch generator: 2.15
START iteration 108, CKPT_AND_STOP: False
[2022-12-05 12:55:27.603107] Finished iteration 109, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3590.608
[2022-12-05 12:55:27.603536] iteration      109/   18750 | elapsed time per iteration (ms): 3590.6 | learning rate: 8.400E-05 | lm loss: 1.061341E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.21
START iteration 109, CKPT_AND_STOP: False
[2022-12-05 12:55:31.227377] Finished iteration 110, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3624.269
[2022-12-05 12:55:31.227791] iteration      110/   18750 | elapsed time per iteration (ms): 3624.2 | learning rate: 8.480E-05 | lm loss: 1.061424E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 4.08
START iteration 110, CKPT_AND_STOP: False
[2022-12-05 12:55:34.784169] Finished iteration 111, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3556.792
[2022-12-05 12:55:34.784589] iteration      111/   18750 | elapsed time per iteration (ms): 3556.8 | learning rate: 8.560E-05 | lm loss: 1.061467E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 4.35
START iteration 111, CKPT_AND_STOP: False
[2022-12-05 12:55:38.398194] Finished iteration 112, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3614.024
[2022-12-05 12:55:38.398647] iteration      112/   18750 | elapsed time per iteration (ms): 3614.0 | learning rate: 8.640E-05 | lm loss: 1.061275E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.19
START iteration 112, CKPT_AND_STOP: False
[2022-12-05 12:55:42.084736] Finished iteration 113, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3686.546
[2022-12-05 12:55:42.085120] iteration      113/   18750 | elapsed time per iteration (ms): 3686.5 | learning rate: 8.720E-05 | lm loss: 1.061251E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.15
START iteration 113, CKPT_AND_STOP: False
[2022-12-05 12:55:45.646051] Finished iteration 114, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3561.315
[2022-12-05 12:55:45.646473] iteration      114/   18750 | elapsed time per iteration (ms): 3561.3 | learning rate: 8.800E-05 | lm loss: 1.061567E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.80
START iteration 114, CKPT_AND_STOP: False
[2022-12-05 12:55:49.213601] Finished iteration 115, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3567.549
[2022-12-05 12:55:49.214000] iteration      115/   18750 | elapsed time per iteration (ms): 3567.5 | learning rate: 8.880E-05 | lm loss: 1.061466E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 3.08
START iteration 115, CKPT_AND_STOP: False
[2022-12-05 12:55:52.779932] Finished iteration 116, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3566.330
[2022-12-05 12:55:52.780350] iteration      116/   18750 | elapsed time per iteration (ms): 3566.3 | learning rate: 8.960E-05 | lm loss: 1.061783E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.83
START iteration 116, CKPT_AND_STOP: False
[2022-12-05 12:55:56.362651] Finished iteration 117, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3582.716
[2022-12-05 12:55:56.363087] iteration      117/   18750 | elapsed time per iteration (ms): 3582.7 | learning rate: 9.040E-05 | lm loss: 1.064072E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.86
START iteration 117, CKPT_AND_STOP: False
[2022-12-05 12:55:59.969046] Finished iteration 118, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3606.400
[2022-12-05 12:55:59.969475] iteration      118/   18750 | elapsed time per iteration (ms): 3606.4 | learning rate: 9.120E-05 | lm loss: 1.064417E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.33
START iteration 118, CKPT_AND_STOP: False
[2022-12-05 12:56:03.547146] Finished iteration 119, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3578.102
[2022-12-05 12:56:03.547550] iteration      119/   18750 | elapsed time per iteration (ms): 3578.1 | learning rate: 9.200E-05 | lm loss: 1.064173E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.08
START iteration 119, CKPT_AND_STOP: False
[2022-12-05 12:56:07.135407] Finished iteration 120, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3588.257
[2022-12-05 12:56:07.135812] iteration      120/   18750 | elapsed time per iteration (ms): 3588.2 | learning rate: 9.280E-05 | lm loss: 1.064208E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.01 | batch generator: 1.87
START iteration 120, CKPT_AND_STOP: False
[2022-12-05 12:56:10.795506] Finished iteration 121, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3660.098
[2022-12-05 12:56:10.795957] iteration      121/   18750 | elapsed time per iteration (ms): 3660.1 | learning rate: 9.360E-05 | lm loss: 1.064019E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.47
START iteration 121, CKPT_AND_STOP: False
[2022-12-05 12:56:14.442377] Finished iteration 122, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3646.871
[2022-12-05 12:56:14.442773] iteration      122/   18750 | elapsed time per iteration (ms): 3646.8 | learning rate: 9.440E-05 | lm loss: 1.063943E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.90
START iteration 122, CKPT_AND_STOP: False
[2022-12-05 12:56:18.077669] Finished iteration 123, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3635.292
[2022-12-05 12:56:18.078117] iteration      123/   18750 | elapsed time per iteration (ms): 3635.3 | learning rate: 9.520E-05 | lm loss: 1.062589E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.03
START iteration 123, CKPT_AND_STOP: False
[2022-12-05 12:56:21.755643] Finished iteration 124, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3677.974
[2022-12-05 12:56:21.756050] iteration      124/   18750 | elapsed time per iteration (ms): 3677.9 | learning rate: 9.600E-05 | lm loss: 1.062029E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.36
START iteration 124, CKPT_AND_STOP: False
[2022-12-05 12:56:25.401195] Finished iteration 125, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3645.552
[2022-12-05 12:56:25.401628] iteration      125/   18750 | elapsed time per iteration (ms): 3645.6 | learning rate: 9.680E-05 | lm loss: 1.062024E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.02 | batch generator: 2.07
START iteration 125, CKPT_AND_STOP: False
[2022-12-05 12:56:29.022575] Finished iteration 126, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3621.379
[2022-12-05 12:56:29.023071] iteration      126/   18750 | elapsed time per iteration (ms): 3621.4 | learning rate: 9.760E-05 | lm loss: 1.061930E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.27
START iteration 126, CKPT_AND_STOP: False
[2022-12-05 12:56:32.547881] Finished iteration 127, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3525.306
[2022-12-05 12:56:32.548286] iteration      127/   18750 | elapsed time per iteration (ms): 3525.2 | learning rate: 9.840E-05 | lm loss: 1.061885E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.90
START iteration 127, CKPT_AND_STOP: False
[2022-12-05 12:56:36.173692] Finished iteration 128, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3625.811
[2022-12-05 12:56:36.174093] iteration      128/   18750 | elapsed time per iteration (ms): 3625.8 | learning rate: 9.920E-05 | lm loss: 1.062007E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.22
START iteration 128, CKPT_AND_STOP: False
[2022-12-05 12:56:39.827053] Finished iteration 129, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3653.364
[2022-12-05 12:56:39.827444] iteration      129/   18750 | elapsed time per iteration (ms): 3653.3 | learning rate: 1.000E-04 | lm loss: 1.062078E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.03
START iteration 129, CKPT_AND_STOP: False
[2022-12-05 12:56:43.445623] Finished iteration 130, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3618.567
[2022-12-05 12:56:43.446169] iteration      130/   18750 | elapsed time per iteration (ms): 3618.7 | learning rate: 1.008E-04 | lm loss: 1.062146E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.01
START iteration 130, CKPT_AND_STOP: False
[2022-12-05 12:56:47.109079] Finished iteration 131, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3663.455
[2022-12-05 12:56:47.109779] iteration      131/   18750 | elapsed time per iteration (ms): 3663.6 | learning rate: 1.016E-04 | lm loss: 1.062205E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.97
START iteration 131, CKPT_AND_STOP: False
[2022-12-05 12:56:50.714282] Finished iteration 132, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3605.207
[2022-12-05 12:56:50.714671] iteration      132/   18750 | elapsed time per iteration (ms): 3604.9 | learning rate: 1.024E-04 | lm loss: 1.061699E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.23
START iteration 132, CKPT_AND_STOP: False
[2022-12-05 12:56:54.290185] Finished iteration 133, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3575.904
[2022-12-05 12:56:54.290569] iteration      133/   18750 | elapsed time per iteration (ms): 3575.9 | learning rate: 1.032E-04 | lm loss: 1.061841E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.92
START iteration 133, CKPT_AND_STOP: False
[2022-12-05 12:56:57.863338] Finished iteration 134, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3573.149
[2022-12-05 12:56:57.863764] iteration      134/   18750 | elapsed time per iteration (ms): 3573.2 | learning rate: 1.040E-04 | lm loss: 1.061708E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.17
START iteration 134, CKPT_AND_STOP: False
[2022-12-05 12:57:01.447449] Finished iteration 135, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3584.111
[2022-12-05 12:57:01.447946] iteration      135/   18750 | elapsed time per iteration (ms): 3584.2 | learning rate: 1.048E-04 | lm loss: 1.061687E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.29
START iteration 135, CKPT_AND_STOP: False
[2022-12-05 12:57:05.073165] Finished iteration 136, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3625.716
[2022-12-05 12:57:05.073617] iteration      136/   18750 | elapsed time per iteration (ms): 3625.7 | learning rate: 1.056E-04 | lm loss: 1.061475E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.27
START iteration 136, CKPT_AND_STOP: False
[2022-12-05 12:57:08.751948] Finished iteration 137, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3678.782
[2022-12-05 12:57:08.752336] iteration      137/   18750 | elapsed time per iteration (ms): 3678.7 | learning rate: 1.064E-04 | lm loss: 1.061721E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.93
START iteration 137, CKPT_AND_STOP: False
[2022-12-05 12:57:12.324372] Finished iteration 138, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3572.424
[2022-12-05 12:57:12.324938] iteration      138/   18750 | elapsed time per iteration (ms): 3572.6 | learning rate: 1.072E-04 | lm loss: 1.061577E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.00
START iteration 138, CKPT_AND_STOP: False
[2022-12-05 12:57:15.986888] Finished iteration 139, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3662.517
[2022-12-05 12:57:15.987273] iteration      139/   18750 | elapsed time per iteration (ms): 3662.3 | learning rate: 1.080E-04 | lm loss: 1.062446E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.32
START iteration 139, CKPT_AND_STOP: False
[2022-12-05 12:57:19.623511] Finished iteration 140, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3636.623
[2022-12-05 12:57:19.623881] iteration      140/   18750 | elapsed time per iteration (ms): 3636.6 | learning rate: 1.088E-04 | lm loss: 1.062659E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.91 | batch generator: 1.90
START iteration 140, CKPT_AND_STOP: False
[2022-12-05 12:57:23.272302] Finished iteration 141, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3648.790
[2022-12-05 12:57:23.272725] iteration      141/   18750 | elapsed time per iteration (ms): 3648.8 | learning rate: 1.096E-04 | lm loss: 1.062413E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.95
START iteration 141, CKPT_AND_STOP: False
[2022-12-05 12:57:26.889620] Finished iteration 142, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3617.319
[2022-12-05 12:57:26.890048] iteration      142/   18750 | elapsed time per iteration (ms): 3617.3 | learning rate: 1.104E-04 | lm loss: 1.062595E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.94
START iteration 142, CKPT_AND_STOP: False
[2022-12-05 12:57:30.465267] Finished iteration 143, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3575.647
[2022-12-05 12:57:30.465707] iteration      143/   18750 | elapsed time per iteration (ms): 3575.6 | learning rate: 1.112E-04 | lm loss: 1.062459E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.80
START iteration 143, CKPT_AND_STOP: False
[2022-12-05 12:57:34.091894] Finished iteration 144, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3626.626
[2022-12-05 12:57:34.092431] iteration      144/   18750 | elapsed time per iteration (ms): 3626.7 | learning rate: 1.120E-04 | lm loss: 1.062460E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.90
START iteration 144, CKPT_AND_STOP: False
[2022-12-05 12:57:37.641431] Finished iteration 145, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3549.539
[2022-12-05 12:57:37.641836] iteration      145/   18750 | elapsed time per iteration (ms): 3549.4 | learning rate: 1.128E-04 | lm loss: 1.062362E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.48
START iteration 145, CKPT_AND_STOP: False
[2022-12-05 12:57:41.227522] Finished iteration 146, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3586.089
[2022-12-05 12:57:41.227908] iteration      146/   18750 | elapsed time per iteration (ms): 3586.1 | learning rate: 1.136E-04 | lm loss: 1.062308E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.90
START iteration 146, CKPT_AND_STOP: False
[2022-12-05 12:57:44.786670] Finished iteration 147, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3559.149
[2022-12-05 12:57:44.787058] iteration      147/   18750 | elapsed time per iteration (ms): 3559.1 | learning rate: 1.144E-04 | lm loss: 1.062306E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.91
START iteration 147, CKPT_AND_STOP: False
[2022-12-05 12:57:48.397575] Finished iteration 148, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3610.905
[2022-12-05 12:57:48.397973] iteration      148/   18750 | elapsed time per iteration (ms): 3610.9 | learning rate: 1.152E-04 | lm loss: 1.062218E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.83
START iteration 148, CKPT_AND_STOP: False
[2022-12-05 12:57:51.982194] Finished iteration 149, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3584.619
[2022-12-05 12:57:51.982724] iteration      149/   18750 | elapsed time per iteration (ms): 3584.7 | learning rate: 1.160E-04 | lm loss: 1.062201E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.96
START iteration 149, CKPT_AND_STOP: False
[2022-12-05 12:57:55.611981] Finished iteration 150, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3629.787
[2022-12-05 12:57:55.612352] iteration      150/   18750 | elapsed time per iteration (ms): 3629.6 | learning rate: 1.168E-04 | lm loss: 1.062193E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.16
START iteration 150, CKPT_AND_STOP: False
[2022-12-05 12:57:59.252005] Finished iteration 151, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3640.022
[2022-12-05 12:57:59.252411] iteration      151/   18750 | elapsed time per iteration (ms): 3640.0 | learning rate: 1.176E-04 | lm loss: 1.062312E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.79
START iteration 151, CKPT_AND_STOP: False
[2022-12-05 12:58:02.842299] Finished iteration 152, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3590.295
[2022-12-05 12:58:02.842695] iteration      152/   18750 | elapsed time per iteration (ms): 3590.3 | learning rate: 1.184E-04 | lm loss: 1.062326E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.07
START iteration 152, CKPT_AND_STOP: False
[2022-12-05 12:58:06.433811] Finished iteration 153, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3591.512
[2022-12-05 12:58:06.434424] iteration      153/   18750 | elapsed time per iteration (ms): 3591.7 | learning rate: 1.192E-04 | lm loss: 1.062380E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.05 | batch generator: 2.31
START iteration 153, CKPT_AND_STOP: False
[2022-12-05 12:58:10.061010] Finished iteration 154, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3627.199
[2022-12-05 12:58:10.061498] iteration      154/   18750 | elapsed time per iteration (ms): 3627.1 | learning rate: 1.200E-04 | lm loss: 1.062098E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.74
START iteration 154, CKPT_AND_STOP: False
[2022-12-05 12:58:13.761568] Finished iteration 155, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3700.562
[2022-12-05 12:58:13.762010] iteration      155/   18750 | elapsed time per iteration (ms): 3700.5 | learning rate: 1.208E-04 | lm loss: 1.062186E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.04
START iteration 155, CKPT_AND_STOP: False
[2022-12-05 12:58:17.386498] Finished iteration 156, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3624.925
[2022-12-05 12:58:17.386898] iteration      156/   18750 | elapsed time per iteration (ms): 3624.9 | learning rate: 1.216E-04 | lm loss: 1.061965E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.49
START iteration 156, CKPT_AND_STOP: False
[2022-12-05 12:58:21.095553] Finished iteration 157, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3709.059
[2022-12-05 12:58:21.095939] iteration      157/   18750 | elapsed time per iteration (ms): 3709.0 | learning rate: 1.224E-04 | lm loss: 1.062098E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.43
START iteration 157, CKPT_AND_STOP: False
[2022-12-05 12:58:24.691664] Finished iteration 158, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.108
[2022-12-05 12:58:24.692125] iteration      158/   18750 | elapsed time per iteration (ms): 3596.2 | learning rate: 1.232E-04 | lm loss: 1.062264E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.14
START iteration 158, CKPT_AND_STOP: False
[2022-12-05 12:58:28.302401] Finished iteration 159, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3610.738
[2022-12-05 12:58:28.302999] iteration      159/   18750 | elapsed time per iteration (ms): 3610.9 | learning rate: 1.240E-04 | lm loss: 1.062010E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.04 | batch generator: 2.61
START iteration 159, CKPT_AND_STOP: False
[2022-12-05 12:58:31.859727] Finished iteration 160, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3557.325
[2022-12-05 12:58:31.860102] iteration      160/   18750 | elapsed time per iteration (ms): 3557.1 | learning rate: 1.248E-04 | lm loss: 1.061906E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.99
START iteration 160, CKPT_AND_STOP: False
[2022-12-05 12:58:35.539264] Finished iteration 161, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3679.537
[2022-12-05 12:58:35.539663] iteration      161/   18750 | elapsed time per iteration (ms): 3679.5 | learning rate: 1.256E-04 | lm loss: 1.062101E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.28
START iteration 161, CKPT_AND_STOP: False
[2022-12-05 12:58:39.134872] Finished iteration 162, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3595.609
[2022-12-05 12:58:39.135331] iteration      162/   18750 | elapsed time per iteration (ms): 3595.7 | learning rate: 1.264E-04 | lm loss: 1.062031E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.90
START iteration 162, CKPT_AND_STOP: False
[2022-12-05 12:58:42.767943] Finished iteration 163, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3633.072
[2022-12-05 12:58:42.768392] iteration      163/   18750 | elapsed time per iteration (ms): 3633.0 | learning rate: 1.272E-04 | lm loss: 1.062071E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.05
START iteration 163, CKPT_AND_STOP: False
[2022-12-05 12:58:46.391615] Finished iteration 164, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3623.669
[2022-12-05 12:58:46.392220] iteration      164/   18750 | elapsed time per iteration (ms): 3623.8 | learning rate: 1.280E-04 | lm loss: 1.062198E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.03
START iteration 164, CKPT_AND_STOP: False
[2022-12-05 12:58:50.013731] Finished iteration 165, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3622.122
[2022-12-05 12:58:50.014130] iteration      165/   18750 | elapsed time per iteration (ms): 3621.9 | learning rate: 1.288E-04 | lm loss: 1.062192E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.04
START iteration 165, CKPT_AND_STOP: False
[2022-12-05 12:58:53.644850] Finished iteration 166, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3631.119
[2022-12-05 12:58:53.645239] iteration      166/   18750 | elapsed time per iteration (ms): 3631.1 | learning rate: 1.296E-04 | lm loss: 1.062172E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.89
START iteration 166, CKPT_AND_STOP: False
[2022-12-05 12:58:57.217640] Finished iteration 167, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3572.783
[2022-12-05 12:58:57.218257] iteration      167/   18750 | elapsed time per iteration (ms): 3573.0 | learning rate: 1.304E-04 | lm loss: 1.062376E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.05
START iteration 167, CKPT_AND_STOP: False
[2022-12-05 12:59:00.906284] Finished iteration 168, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3688.646
[2022-12-05 12:59:00.906719] iteration      168/   18750 | elapsed time per iteration (ms): 3688.4 | learning rate: 1.312E-04 | lm loss: 1.062242E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.90
START iteration 168, CKPT_AND_STOP: False
[2022-12-05 12:59:04.537101] Finished iteration 169, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3630.823
[2022-12-05 12:59:04.537573] iteration      169/   18750 | elapsed time per iteration (ms): 3630.8 | learning rate: 1.320E-04 | lm loss: 1.062254E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.87
START iteration 169, CKPT_AND_STOP: False
[2022-12-05 12:59:08.171651] Finished iteration 170, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3634.546
[2022-12-05 12:59:08.172068] iteration      170/   18750 | elapsed time per iteration (ms): 3634.5 | learning rate: 1.328E-04 | lm loss: 1.062323E+01 | loss scale: 16384.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.95
START iteration 170, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 12:59:11.775185] Finished iteration 171, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 3603.535
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     171 to s3://spot-checkpoints/gpt/iter_0000171/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000171/mp_rank_00/model_optim_rng.pt
Opt ckpt time 18.90280508995056
Process done with return code 0
Parent process ID: 37419 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 12 0 2007202.880859375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4913759
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 58711; min bwd 92748, max bwd 103910
Min long fwd: 56704, max long fwd 63285; min long bwd 98353, max long bwd 106648
Time taken by simulation: 378 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 18 0 1481951.2939453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4877293
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69316, max bwd 77937
Min long fwd: 43561, max long fwd 51496; min long bwd 71918, max long bwd 80151
Time taken by simulation: 843 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 25 0 976051.4526367188 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4466988
Min send: 10000000, max send 0
Min long send: 38237, max long send 65217
Min fwd: 19236, max fwd 35580; min bwd 42158, max bwd 57089
Min long fwd: 24076, max long fwd 35713; min long bwd 50505, max long bwd 59414
Time taken by simulation: 2103 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4222 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10634 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30128 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 45896 microseconds

{1: inf, 2: inf, 3: 4.913759, 4: 4.877293, 6: 4.466988, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 6 1
expected time is 4.466988
5 per stage
30 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 6
chunk_size: 1
data depth: 5
stage to rank map: 0,6,12,18,24;1,7,13,19,25;2,8,14,20,26;3,9,15,21,27;4,10,16,22,28;5,11,17,23,29;
World size is 30
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,6,12,18,24;1,7,13,19,25;2,8,14,20,26;3,9,15,21,27;4,10,16,22,28;5,11,17,23,29; --batch-size=25 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 171
using world size: 30 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 25
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 171
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,6,12,18,24;1,7,13,19,25;2,8,14,20,26;3,9,15,21,27;4,10,16,22,28;5,11,17,23,29;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 30
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2343750
    validation: 2500
    test:       1250
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.747460126876831
SHARED WEIGHTS ARE
[(0, 5)]
this rank  0 is part of pipeline replica  0
25 chunks
 > number of parameters on model parallel rank 0: 328051200
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000171/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000171/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 56.234 seconds
setting training data start iteration to 171
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 73768.13 | train/valid/test data iterators: 270.51
training ...
START iteration 171, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 13:01:41.910214] Finished iteration 172, CKPT_AND_STOP: True, flag: tensor([2], dtype=torch.int32), speed: 9930.970
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     172 to s3://spot-checkpoints/gpt/iter_0000172/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000172/mp_rank_00/model_optim_rng.pt
Opt ckpt time 17.911690950393677
Process done with return code 0
Parent process ID: 38650 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 12 0 2007202.880859375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4913759
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 58711; min bwd 92748, max bwd 103910
Min long fwd: 56704, max long fwd 63285; min long bwd 98353, max long bwd 106648
Time taken by simulation: 372 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 16 0 1727535.5224609375 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4820532
Min send: 10000000, max send 0
Min long send: 38109, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69446, max bwd 80083
Min long fwd: 43192, max long fwd 50561; min long bwd 74672, max long bwd 80151
Time taken by simulation: 819 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 25 0 976051.4526367188 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4466988
Min send: 10000000, max send 0
Min long send: 38237, max long send 65217
Min fwd: 19236, max fwd 35580; min bwd 42158, max bwd 57089
Min long fwd: 24076, max long fwd 35713; min long bwd 50505, max long bwd 59414
Time taken by simulation: 1848 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 32 0 700971.3134765625 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4198444
Min send: 10000000, max send 0
Min long send: 38109, max long send 68059
Min fwd: 11544, max fwd 27276; min bwd 28686, max bwd 44788
Min long fwd: 22228, max long fwd 29781; min long bwd 41714, max long bwd 48296
Time taken by simulation: 3766 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 11136 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 64 0 294405.0598144531 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5043807
Min send: 10000000, max send 0
Min long send: 38109, max long send 77793
Min fwd: 803, max fwd 17174; min bwd 10907, max bwd 28118
Min long fwd: 12084, max long fwd 20913; min long bwd 17914, max long bwd 26363
Time taken by simulation: 14924 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46927 microseconds

{1: inf, 2: inf, 3: 4.913759, 4: 4.820532, 6: 4.466988, 8: 4.198444, 12: 5.360725, 16: 5.043807, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.198444
4 per stage
32 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 4
stage to rank map: 0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31;
World size is 32
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31; --batch-size=32 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 172
using world size: 32 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 32
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 172
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16,24;1,9,17,25;2,10,18,26;3,11,19,27;4,12,20,28;5,13,21,29;6,14,22,30;7,15,23,31;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 32
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2400000
    validation: 2560
    test:       1280
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.4949460029602051
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
32 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000172/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000172/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 45.602 seconds
setting training data start iteration to 172
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 62960.51 | train/valid/test data iterators: 248.84
training ...
START iteration 172, CKPT_AND_STOP: False
[2022-12-05 13:03:32.127976] Finished iteration 173, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 11698.732
[2022-12-05 13:03:32.128835] iteration      173/   18750 | elapsed time per iteration (ms): 11699.6 | learning rate: 1.352E-04 | lm loss: 1.062721E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 173 iterations memory (MB) | allocated: 3614.15087890625 | max allocated: 6242.98388671875 | reserved: 6842.0 | max reserved: 6842.0
time (ms) | optimizer: 14.42 | batch generator: 7.50
START iteration 173, CKPT_AND_STOP: False
[2022-12-05 13:03:37.639640] Finished iteration 174, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5511.689
[2022-12-05 13:03:37.640089] iteration      174/   18750 | elapsed time per iteration (ms): 5511.2 | learning rate: 1.360E-04 | lm loss: 1.062476E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.27
START iteration 174, CKPT_AND_STOP: False
[2022-12-05 13:03:42.358662] Finished iteration 175, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4719.023
[2022-12-05 13:03:42.359159] iteration      175/   18750 | elapsed time per iteration (ms): 4719.1 | learning rate: 1.368E-04 | lm loss: 1.062650E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.01 | batch generator: 4.05
START iteration 175, CKPT_AND_STOP: False
[2022-12-05 13:03:47.048578] Finished iteration 176, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4689.913
[2022-12-05 13:03:47.049266] iteration      176/   18750 | elapsed time per iteration (ms): 4690.1 | learning rate: 1.376E-04 | lm loss: 1.062773E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.03 | batch generator: 2.28
START iteration 176, CKPT_AND_STOP: False
[2022-12-05 13:03:51.656493] Finished iteration 177, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4607.917
[2022-12-05 13:03:51.657106] iteration      177/   18750 | elapsed time per iteration (ms): 4607.8 | learning rate: 1.384E-04 | lm loss: 1.062803E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.18 | batch generator: 2.01
START iteration 177, CKPT_AND_STOP: False
[2022-12-05 13:03:55.263628] Finished iteration 178, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3607.136
[2022-12-05 13:03:55.264066] iteration      178/   18750 | elapsed time per iteration (ms): 3607.0 | learning rate: 1.392E-04 | lm loss: 1.062932E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.09
START iteration 178, CKPT_AND_STOP: False
[2022-12-05 13:03:58.823507] Finished iteration 179, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3559.879
[2022-12-05 13:03:58.823982] iteration      179/   18750 | elapsed time per iteration (ms): 3559.9 | learning rate: 1.400E-04 | lm loss: 1.062691E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.11
START iteration 179, CKPT_AND_STOP: False
[2022-12-05 13:04:02.883159] Finished iteration 180, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4059.644
[2022-12-05 13:04:02.883548] iteration      180/   18750 | elapsed time per iteration (ms): 4059.5 | learning rate: 1.408E-04 | lm loss: 1.062500E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.92
START iteration 180, CKPT_AND_STOP: False
[2022-12-05 13:04:07.065879] Finished iteration 181, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4182.727
[2022-12-05 13:04:07.066337] iteration      181/   18750 | elapsed time per iteration (ms): 4182.8 | learning rate: 1.416E-04 | lm loss: 1.062598E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.87
START iteration 181, CKPT_AND_STOP: False
[2022-12-05 13:04:10.645630] Finished iteration 182, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3579.752
[2022-12-05 13:04:10.646061] iteration      182/   18750 | elapsed time per iteration (ms): 3579.7 | learning rate: 1.424E-04 | lm loss: 1.062626E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.08
START iteration 182, CKPT_AND_STOP: False
[2022-12-05 13:04:14.187421] Finished iteration 183, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3541.790
[2022-12-05 13:04:14.187820] iteration      183/   18750 | elapsed time per iteration (ms): 3541.7 | learning rate: 1.432E-04 | lm loss: 1.062486E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.01
START iteration 183, CKPT_AND_STOP: False
[2022-12-05 13:04:17.743019] Finished iteration 184, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3555.599
[2022-12-05 13:04:17.743445] iteration      184/   18750 | elapsed time per iteration (ms): 3555.6 | learning rate: 1.440E-04 | lm loss: 1.060168E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.37
START iteration 184, CKPT_AND_STOP: False
[2022-12-05 13:04:21.363089] Finished iteration 185, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3620.068
[2022-12-05 13:04:21.363651] iteration      185/   18750 | elapsed time per iteration (ms): 3620.2 | learning rate: 1.448E-04 | lm loss: 1.062488E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.07
START iteration 185, CKPT_AND_STOP: False
[2022-12-05 13:04:24.923476] Finished iteration 186, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3560.387
[2022-12-05 13:04:24.923986] iteration      186/   18750 | elapsed time per iteration (ms): 3560.3 | learning rate: 1.456E-04 | lm loss: 1.062478E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.06 | batch generator: 2.12
START iteration 186, CKPT_AND_STOP: False
[2022-12-05 13:04:28.499529] Finished iteration 187, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3576.054
[2022-12-05 13:04:28.499948] iteration      187/   18750 | elapsed time per iteration (ms): 3575.9 | learning rate: 1.464E-04 | lm loss: 1.062445E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 4.85
START iteration 187, CKPT_AND_STOP: False
[2022-12-05 13:04:32.186096] Finished iteration 188, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3686.567
[2022-12-05 13:04:32.186491] iteration      188/   18750 | elapsed time per iteration (ms): 3686.5 | learning rate: 1.472E-04 | lm loss: 1.063335E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.53
START iteration 188, CKPT_AND_STOP: False
[2022-12-05 13:04:35.941931] Finished iteration 189, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3755.835
[2022-12-05 13:04:35.942332] iteration      189/   18750 | elapsed time per iteration (ms): 3755.8 | learning rate: 1.480E-04 | lm loss: 1.062416E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.14
START iteration 189, CKPT_AND_STOP: False
[2022-12-05 13:04:39.576246] Finished iteration 190, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3634.316
[2022-12-05 13:04:39.576646] iteration      190/   18750 | elapsed time per iteration (ms): 3634.3 | learning rate: 1.488E-04 | lm loss: 1.062443E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.04 | batch generator: 1.80
START iteration 190, CKPT_AND_STOP: False
[2022-12-05 13:04:43.303630] Finished iteration 191, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3727.383
[2022-12-05 13:04:43.304066] iteration      191/   18750 | elapsed time per iteration (ms): 3727.4 | learning rate: 1.496E-04 | lm loss: 1.062511E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.11
START iteration 191, CKPT_AND_STOP: False
[2022-12-05 13:04:47.010614] Finished iteration 192, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3706.984
[2022-12-05 13:04:47.011073] iteration      192/   18750 | elapsed time per iteration (ms): 3707.0 | learning rate: 1.500E-04 | lm loss: 1.062653E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.89
START iteration 192, CKPT_AND_STOP: False
[2022-12-05 13:04:50.734920] Finished iteration 193, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3724.306
[2022-12-05 13:04:50.735349] iteration      193/   18750 | elapsed time per iteration (ms): 3724.3 | learning rate: 1.500E-04 | lm loss: 1.062440E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.20
START iteration 193, CKPT_AND_STOP: False
[2022-12-05 13:04:54.357211] Finished iteration 194, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3622.293
[2022-12-05 13:04:54.357647] iteration      194/   18750 | elapsed time per iteration (ms): 3622.3 | learning rate: 1.500E-04 | lm loss: 1.062451E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.07 | batch generator: 2.35
START iteration 194, CKPT_AND_STOP: False
[2022-12-05 13:04:58.020882] Finished iteration 195, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3663.670
[2022-12-05 13:04:58.021385] iteration      195/   18750 | elapsed time per iteration (ms): 3663.7 | learning rate: 1.500E-04 | lm loss: 1.062343E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.04 | batch generator: 2.13
START iteration 195, CKPT_AND_STOP: False
[2022-12-05 13:05:01.609507] Finished iteration 196, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3588.626
[2022-12-05 13:05:01.609905] iteration      196/   18750 | elapsed time per iteration (ms): 3588.5 | learning rate: 1.500E-04 | lm loss: 1.062753E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.15
START iteration 196, CKPT_AND_STOP: False
[2022-12-05 13:05:05.160951] Finished iteration 197, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3551.443
[2022-12-05 13:05:05.161519] iteration      197/   18750 | elapsed time per iteration (ms): 3551.6 | learning rate: 1.500E-04 | lm loss: 1.062588E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.48
START iteration 197, CKPT_AND_STOP: False
[2022-12-05 13:05:08.687015] Finished iteration 198, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3526.066
[2022-12-05 13:05:08.687416] iteration      198/   18750 | elapsed time per iteration (ms): 3525.9 | learning rate: 1.500E-04 | lm loss: 1.062597E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.01 | batch generator: 2.20
START iteration 198, CKPT_AND_STOP: False
[2022-12-05 13:05:12.266120] Finished iteration 199, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3579.104
[2022-12-05 13:05:12.266594] iteration      199/   18750 | elapsed time per iteration (ms): 3579.2 | learning rate: 1.500E-04 | lm loss: 1.062631E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.12
START iteration 199, CKPT_AND_STOP: False
[2022-12-05 13:05:16.025472] Finished iteration 200, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3759.352
[2022-12-05 13:05:16.025928] iteration      200/   18750 | elapsed time per iteration (ms): 3759.3 | learning rate: 1.500E-04 | lm loss: 1.063725E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.04 | batch generator: 2.43
START iteration 200, CKPT_AND_STOP: False
[2022-12-05 13:05:19.579646] Finished iteration 201, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3554.173
[2022-12-05 13:05:19.580050] iteration      201/   18750 | elapsed time per iteration (ms): 3554.1 | learning rate: 1.500E-04 | lm loss: 1.062456E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 3.87
START iteration 201, CKPT_AND_STOP: False
[2022-12-05 13:05:23.277558] Finished iteration 202, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3697.912
[2022-12-05 13:05:23.278005] iteration      202/   18750 | elapsed time per iteration (ms): 3697.9 | learning rate: 1.500E-04 | lm loss: 1.062334E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.52
START iteration 202, CKPT_AND_STOP: False
[2022-12-05 13:05:26.870777] Finished iteration 203, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3593.219
[2022-12-05 13:05:26.871165] iteration      203/   18750 | elapsed time per iteration (ms): 3593.1 | learning rate: 1.500E-04 | lm loss: 1.062251E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.26
START iteration 203, CKPT_AND_STOP: False
[2022-12-05 13:05:30.535995] Finished iteration 204, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3665.219
[2022-12-05 13:05:30.536405] iteration      204/   18750 | elapsed time per iteration (ms): 3665.2 | learning rate: 1.500E-04 | lm loss: 1.062017E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.42
START iteration 204, CKPT_AND_STOP: False
[2022-12-05 13:05:34.146369] Finished iteration 205, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3610.370
[2022-12-05 13:05:34.146957] iteration      205/   18750 | elapsed time per iteration (ms): 3610.5 | learning rate: 1.500E-04 | lm loss: 1.062512E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.22
START iteration 205, CKPT_AND_STOP: False
[2022-12-05 13:05:37.836344] Finished iteration 206, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3689.978
[2022-12-05 13:05:37.836734] iteration      206/   18750 | elapsed time per iteration (ms): 3689.8 | learning rate: 1.500E-04 | lm loss: 1.062378E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.30
START iteration 206, CKPT_AND_STOP: False
[2022-12-05 13:05:41.503573] Finished iteration 207, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3667.229
[2022-12-05 13:05:41.503992] iteration      207/   18750 | elapsed time per iteration (ms): 3667.2 | learning rate: 1.500E-04 | lm loss: 1.062494E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.07 | batch generator: 2.22
START iteration 207, CKPT_AND_STOP: False
[2022-12-05 13:05:45.071878] Finished iteration 208, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3568.305
[2022-12-05 13:05:45.072444] iteration      208/   18750 | elapsed time per iteration (ms): 3568.4 | learning rate: 1.500E-04 | lm loss: 1.062381E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.23
START iteration 208, CKPT_AND_STOP: False
[2022-12-05 13:05:48.643329] Finished iteration 209, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3571.451
[2022-12-05 13:05:48.643722] iteration      209/   18750 | elapsed time per iteration (ms): 3571.3 | learning rate: 1.500E-04 | lm loss: 1.062405E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.36
START iteration 209, CKPT_AND_STOP: False
[2022-12-05 13:05:52.152236] Finished iteration 210, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3508.907
[2022-12-05 13:05:52.152956] iteration      210/   18750 | elapsed time per iteration (ms): 3509.2 | learning rate: 1.500E-04 | lm loss: 1.062462E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.84
START iteration 210, CKPT_AND_STOP: False
[2022-12-05 13:05:55.727674] Finished iteration 211, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3575.438
[2022-12-05 13:05:55.728168] iteration      211/   18750 | elapsed time per iteration (ms): 3575.2 | learning rate: 1.500E-04 | lm loss: 1.062442E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.30
START iteration 211, CKPT_AND_STOP: False
[2022-12-05 13:05:59.402948] Finished iteration 212, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3675.275
[2022-12-05 13:05:59.403409] iteration      212/   18750 | elapsed time per iteration (ms): 3675.2 | learning rate: 1.500E-04 | lm loss: 1.062086E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.35
START iteration 212, CKPT_AND_STOP: False
[2022-12-05 13:06:02.998966] Finished iteration 213, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.017
[2022-12-05 13:06:02.999358] iteration      213/   18750 | elapsed time per iteration (ms): 3595.9 | learning rate: 1.500E-04 | lm loss: 1.062346E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.07
START iteration 213, CKPT_AND_STOP: False
[2022-12-05 13:06:06.513693] Finished iteration 214, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3514.727
[2022-12-05 13:06:06.514236] iteration      214/   18750 | elapsed time per iteration (ms): 3514.9 | learning rate: 1.500E-04 | lm loss: 1.062408E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.97
START iteration 214, CKPT_AND_STOP: False
[2022-12-05 13:06:10.121792] Finished iteration 215, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3608.099
[2022-12-05 13:06:10.122220] iteration      215/   18750 | elapsed time per iteration (ms): 3608.0 | learning rate: 1.500E-04 | lm loss: 1.061785E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.25
START iteration 215, CKPT_AND_STOP: False
[2022-12-05 13:06:13.705492] Finished iteration 216, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3583.700
[2022-12-05 13:06:13.705909] iteration      216/   18750 | elapsed time per iteration (ms): 3583.7 | learning rate: 1.500E-04 | lm loss: 1.062306E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.04
START iteration 216, CKPT_AND_STOP: False
[2022-12-05 13:06:17.302876] Finished iteration 217, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3597.383
[2022-12-05 13:06:17.303272] iteration      217/   18750 | elapsed time per iteration (ms): 3597.3 | learning rate: 1.500E-04 | lm loss: 1.062361E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.14
START iteration 217, CKPT_AND_STOP: False
[2022-12-05 13:06:20.868528] Finished iteration 218, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3565.653
[2022-12-05 13:06:20.869048] iteration      218/   18750 | elapsed time per iteration (ms): 3565.7 | learning rate: 1.500E-04 | lm loss: 1.062453E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.83
START iteration 218, CKPT_AND_STOP: False
[2022-12-05 13:06:24.470405] Finished iteration 219, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3601.877
[2022-12-05 13:06:24.470950] iteration      219/   18750 | elapsed time per iteration (ms): 3601.9 | learning rate: 1.500E-04 | lm loss: 1.062376E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.03
START iteration 219, CKPT_AND_STOP: False
[2022-12-05 13:06:28.034630] Finished iteration 220, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3564.225
[2022-12-05 13:06:28.035068] iteration      220/   18750 | elapsed time per iteration (ms): 3564.1 | learning rate: 1.500E-04 | lm loss: 1.062487E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.96
START iteration 220, CKPT_AND_STOP: False
[2022-12-05 13:06:31.680773] Finished iteration 221, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3646.143
[2022-12-05 13:06:31.681158] iteration      221/   18750 | elapsed time per iteration (ms): 3646.1 | learning rate: 1.500E-04 | lm loss: 1.062155E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.90
START iteration 221, CKPT_AND_STOP: False
[2022-12-05 13:06:35.281904] Finished iteration 222, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3601.130
[2022-12-05 13:06:35.282296] iteration      222/   18750 | elapsed time per iteration (ms): 3601.1 | learning rate: 1.500E-04 | lm loss: 1.062195E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.14
START iteration 222, CKPT_AND_STOP: False
[2022-12-05 13:06:38.899936] Finished iteration 223, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3618.032
[2022-12-05 13:06:38.900483] iteration      223/   18750 | elapsed time per iteration (ms): 3618.2 | learning rate: 1.500E-04 | lm loss: 1.062246E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.01
START iteration 223, CKPT_AND_STOP: False
[2022-12-05 13:06:42.496042] Finished iteration 224, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3596.107
[2022-12-05 13:06:42.496450] iteration      224/   18750 | elapsed time per iteration (ms): 3595.9 | learning rate: 1.500E-04 | lm loss: 1.062245E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.25
START iteration 224, CKPT_AND_STOP: False
[2022-12-05 13:06:46.142750] Finished iteration 225, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3646.706
[2022-12-05 13:06:46.143376] iteration      225/   18750 | elapsed time per iteration (ms): 3646.9 | learning rate: 1.500E-04 | lm loss: 1.062350E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.05 | batch generator: 1.89
START iteration 225, CKPT_AND_STOP: False
[2022-12-05 13:06:49.886856] Finished iteration 226, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3744.107
[2022-12-05 13:06:49.887263] iteration      226/   18750 | elapsed time per iteration (ms): 3743.9 | learning rate: 1.500E-04 | lm loss: 1.062249E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 9.97
START iteration 226, CKPT_AND_STOP: False
[2022-12-05 13:06:53.506384] Finished iteration 227, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3619.528
[2022-12-05 13:06:53.506906] iteration      227/   18750 | elapsed time per iteration (ms): 3619.6 | learning rate: 1.500E-04 | lm loss: 1.062088E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.42
START iteration 227, CKPT_AND_STOP: False
[2022-12-05 13:06:57.039080] Finished iteration 228, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3532.695
[2022-12-05 13:06:57.039477] iteration      228/   18750 | elapsed time per iteration (ms): 3532.6 | learning rate: 1.500E-04 | lm loss: 1.062200E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.11
START iteration 228, CKPT_AND_STOP: False
[2022-12-05 13:07:00.611510] Finished iteration 229, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3572.432
[2022-12-05 13:07:00.611957] iteration      229/   18750 | elapsed time per iteration (ms): 3572.5 | learning rate: 1.500E-04 | lm loss: 1.062250E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.00
START iteration 229, CKPT_AND_STOP: False
[2022-12-05 13:07:04.306897] Finished iteration 230, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3695.387
[2022-12-05 13:07:04.307272] iteration      230/   18750 | elapsed time per iteration (ms): 3695.3 | learning rate: 1.500E-04 | lm loss: 1.062276E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.04
START iteration 230, CKPT_AND_STOP: False
[2022-12-05 13:07:07.947296] Finished iteration 231, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3640.398
[2022-12-05 13:07:07.947852] iteration      231/   18750 | elapsed time per iteration (ms): 3640.6 | learning rate: 1.500E-04 | lm loss: 1.062269E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.10
START iteration 231, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
[2022-12-05 13:07:11.524649] Finished iteration 232, CKPT_AND_STOP: True, flag: tensor([1], dtype=torch.int32), speed: 3577.352
Begin to save checkpont and exit
global rank 0 is saving checkpoint at iteration     232 to s3://spot-checkpoints/gpt/iter_0000232/mp_rank_00/model_optim_rng.pt
  successfully saved s3://spot-checkpoints/gpt/iter_0000232/mp_rank_00/model_optim_rng.pt
Opt ckpt time 20.422662258148193
Process done with return code 0
Parent process ID: 40341 node: 172.31.28.108
48 cutpoints
Stages 1
Micro-bs 1 Max mem: 35008318771.20003
Predicted microbatch size for 1: -1
Stages 2
Micro-bs 1 Max mem: 17531106099.199993
Predicted microbatch size for 2: -1
Stages 3
Micro-bs 1 Max mem: 12228800511.999996
Predicted microbatch size for 3: 1
comm size 1638400
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 14 0 2006653.3203125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5367030
Min send: 10000000, max send 0
Min long send: 38387, max long send 60521
Min fwd: 45773, max fwd 60689; min bwd 93028, max bwd 103910
Min long fwd: 57033, max long fwd 64913; min long bwd 98397, max long bwd 102857
Time taken by simulation: 430 microseconds

Stages 4
Micro-bs 1 Max mem: 9577647718.399998
Predicted microbatch size for 4: 1
comm size 1638400
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 18 0 1481951.2939453125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4877293
Min send: 10000000, max send 0
Min long send: 38055, max long send 64155
Min fwd: 32292, max fwd 48132; min bwd 69316, max bwd 77937
Min long fwd: 43561, max long fwd 51496; min long bwd 71918, max long bwd 80151
Time taken by simulation: 773 microseconds

Stages 6
Micro-bs 1 Max mem: 6926494924.799999
Predicted microbatch size for 6: 1
comm size 1638400
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 32 0 876895.5078125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5017062
Min send: 10000000, max send 0
Min long send: 38071, max long send 66276
Min fwd: 18837, max fwd 35859; min bwd 42514, max bwd 56347
Min long fwd: 27753, max long fwd 35222; min long bwd 51765, max long bwd 59571
Time taken by simulation: 2249 microseconds

Stages 8
Micro-bs 1 Max mem: 5600918528.0
Predicted microbatch size for 8: 1
comm size 1638400
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 42 0 610855.46875 38055.03383759529
End of simulation:  Mini-batch time (usec) = 4799842
Min send: 10000000, max send 0
Min long send: 38071, max long send 68059
Min fwd: 10665, max fwd 27276; min bwd 28791, max bwd 43945
Min long fwd: 21459, max long fwd 30507; min long bwd 38642, max long bwd 48142
Time taken by simulation: 4233 microseconds

Stages 12
Micro-bs 1 Max mem: 4275342131.2000003
Predicted microbatch size for 12: 1
comm size 1638400
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 64 0 343797.36328125 38055.03383759529
End of simulation:  Mini-batch time (usec) = 5360725
Min send: 10000000, max send 0
Min long send: 38056, max long send 68144
Min fwd: 5149, max fwd 20736; min bwd 17314, max bwd 33457
Min long fwd: 11639, max long fwd 22378; min long bwd 26559, max long bwd 34539
Time taken by simulation: 10422 microseconds

Stages 16
Micro-bs 1 Max mem: 3612553932.8
Predicted microbatch size for 16: 1
comm size 1638400
WARNING: no send time found, 16 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 16 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 7580423
Min send: 10000000, max send 0
Min long send: 38056, max long send 77793
Min fwd: 803, max fwd 17420; min bwd 10356, max bwd 28687
Min long fwd: 10499, max long fwd 20148; min long bwd 17021, max long bwd 27905
Time taken by simulation: 30356 microseconds

Stages 24
Micro-bs 1 Max mem: 2949765734.4
Predicted microbatch size for 24: 1
comm size 1638400
WARNING: no send time found, 24 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 24 128 0 0 38055.03383759529
End of simulation:  Mini-batch time (usec) = 6789982
Min send: 10000000, max send 0
Min long send: 38055, max long send 74455
Min fwd: 26, max fwd 15549; min bwd 3842, max bwd 20573
Min long fwd: 7764, max long fwd 18097; min long bwd 11316, max long bwd 21840
Time taken by simulation: 46642 microseconds

{1: inf, 2: inf, 3: 5.36703, 4: 4.877293, 6: 5.017062, 8: 4.799842, 12: 5.360725, 16: 7.580423, 24: 6.789982}
{1: -1, 2: -1, 3: 1, 4: 1, 6: 1, 8: 1, 12: 1, 16: 1, 24: 1}
best config is: 8 1
expected time is 4.799842
3 per stage
24 servers!
Config:
ranks: range(0, 1)
train batch size: 128
partitions: 8
chunk_size: 1
data depth: 3
stage to rank map: 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
World size is 24
/opt/conda/envs/varuna/bin/python -u pretrain_gpt2.py --rank=0 --chunk_size=1 --local_rank=0 --stage_to_rank_map=0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23; --batch-size=42 --num-layers 48 --hidden-size 1600 --num-attention-heads 25 --seq-length 1024 --max-position-embeddings 1024 --train-iters 18750 --lr-decay-iters 18750 --save s3://spot-checkpoints/gpt --load s3://spot-checkpoints/gpt --vocab-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json --merge-file /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt --data-impl mmap --split 949,50,1 --distributed-backend gloo --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --warmup .01 --log-interval 1 --exit-interval 10000 --save-interval 500 --eval-interval 10000 --use-cpu-initialization --synthetic --eval-iters 10 --varuna --fp16 --resume_step 232
using world size: 24 and model-parallel size: 1 
using torch.float32 for parameters ...
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 42
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... False
  checkpoint_num_layers ........... 1
  chunk_size ...................... 1
  clip_grad ....................... 1.0
  data_impl ....................... mmap
  data_path ....................... None
  DDP_impl ........................ local
  distribute_checkpointed_activations  False
  distributed_backend ............. gloo
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 10000
  eval_iters ...................... 10
  exit_interval ................... 10000
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 1600
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ s3://spot-checkpoints/gpt
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 18750
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 1024
  merge_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/merges.txt
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 1
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 25
  num_layers ...................... 48
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float32
  profiling ....................... False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  resume_step ..................... 232
  save ............................ s3://spot-checkpoints/gpt
  save_interval ................... 500
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  seed ............................ 1234
  seq_length ...................... 1024
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  stage_to_rank_map ............... 0,8,16;1,9,17;2,10,18;3,11,19;4,12,20;5,13,21;6,14,22;7,15,23;
  synthetic ....................... True
  tensorboard_dir ................. None
  titles_data_path ................ None
  tokenizer_type .................. GPT2BPETokenizer
  train_iters ..................... 18750
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... True
  use_one_sent_docs ............... False
  varuna .......................... True
  vocab_file ...................... /home/ubuntu/SpotDL-DeesSpeed/megatron/Megatron-LM-v1.1.5-3D_parallelism/data/gpt2/vocab.json
  vocab_size ...................... 40478
  warmup .......................... 0.01
  weight_decay .................... 0.01
  world_size ...................... 24
---------------- end of arguments ----------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2362500
    validation: 2520
    test:       1260
> building train, validation, and test datasets for GPT2 ...
> finished creating GPT2 datasets ...
building GPT2 model ...
dry run time 0.5500636100769043
SHARED WEIGHTS ARE
[(0, 7)]
this rank  0 is part of pipeline replica  0
42 chunks
 > number of parameters on model parallel rank 0: 266569600
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
> learning rate decay style: cosine
Begin to load checkpoint from global store s3://spot-checkpoints/gpt
global rank 0 is loading checkpoint s3://spot-checkpoints/gpt/iter_0000232/mp_rank_00/model_optim_rng.pt
 > using checkpoint value 0.00015 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 187.5 for warmup iterations
 > using checkpoint value 18750 for total number of iterations
 > using checkpoint value cosine for decay style
  successfully loaded s3://spot-checkpoints/gpt/iter_0000232/mp_rank_00/model_optim_rng.pt
 > finished loading checkpoint in 40.972 seconds
setting training data start iteration to 232
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 58453.11 | train/valid/test data iterators: 253.21
training ...
START iteration 232, CKPT_AND_STOP: False
[2022-12-05 13:09:29.746062] Finished iteration 233, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 12353.176
[2022-12-05 13:09:29.746859] iteration      233/   18750 | elapsed time per iteration (ms): 12354.0 | learning rate: 1.500E-04 | lm loss: 1.060012E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 233 iterations memory (MB) | allocated: 3614.34619140625 | max allocated: 6243.17919921875 | reserved: 6842.0 | max reserved: 6842.0
time (ms) | optimizer: 14.17 | batch generator: 7.61
START iteration 233, CKPT_AND_STOP: False
[2022-12-05 13:09:34.881300] Finished iteration 234, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5135.271
[2022-12-05 13:09:34.881921] iteration      234/   18750 | elapsed time per iteration (ms): 5135.0 | learning rate: 1.500E-04 | lm loss: 1.060163E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.07 | batch generator: 2.33
START iteration 234, CKPT_AND_STOP: False
[2022-12-05 13:09:39.943624] Finished iteration 235, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5062.324
[2022-12-05 13:09:39.944228] iteration      235/   18750 | elapsed time per iteration (ms): 5062.3 | learning rate: 1.500E-04 | lm loss: 1.060430E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 4.48
START iteration 235, CKPT_AND_STOP: False
[2022-12-05 13:09:45.050477] Finished iteration 236, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5106.854
[2022-12-05 13:09:45.051088] iteration      236/   18750 | elapsed time per iteration (ms): 5106.8 | learning rate: 1.500E-04 | lm loss: 1.060590E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 4.47
START iteration 236, CKPT_AND_STOP: False
[2022-12-05 13:09:50.196818] Finished iteration 237, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5146.340
[2022-12-05 13:09:50.197468] iteration      237/   18750 | elapsed time per iteration (ms): 5146.4 | learning rate: 1.500E-04 | lm loss: 1.064190E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.08 | batch generator: 2.78
START iteration 237, CKPT_AND_STOP: False
[2022-12-05 13:09:54.345014] Finished iteration 238, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4148.198
[2022-12-05 13:09:54.345435] iteration      238/   18750 | elapsed time per iteration (ms): 4148.0 | learning rate: 1.500E-04 | lm loss: 1.064466E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 3.51
START iteration 238, CKPT_AND_STOP: False
[2022-12-05 13:09:58.456994] Finished iteration 239, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4111.980
[2022-12-05 13:09:58.457510] iteration      239/   18750 | elapsed time per iteration (ms): 4112.1 | learning rate: 1.500E-04 | lm loss: 1.064587E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 1.86
START iteration 239, CKPT_AND_STOP: False
[2022-12-05 13:10:02.549097] Finished iteration 240, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4092.102
[2022-12-05 13:10:02.549706] iteration      240/   18750 | elapsed time per iteration (ms): 4092.2 | learning rate: 1.500E-04 | lm loss: 1.064621E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.82
START iteration 240, CKPT_AND_STOP: False
[2022-12-05 13:10:06.608582] Finished iteration 241, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4059.486
[2022-12-05 13:10:06.608987] iteration      241/   18750 | elapsed time per iteration (ms): 4059.3 | learning rate: 1.500E-04 | lm loss: 1.064420E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.56
START iteration 241, CKPT_AND_STOP: False
[2022-12-05 13:10:10.664427] Finished iteration 242, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4055.846
[2022-12-05 13:10:10.664857] iteration      242/   18750 | elapsed time per iteration (ms): 4055.8 | learning rate: 1.500E-04 | lm loss: 1.064337E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.19
START iteration 242, CKPT_AND_STOP: False
[2022-12-05 13:10:14.760856] Finished iteration 243, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4096.428
[2022-12-05 13:10:14.761434] iteration      243/   18750 | elapsed time per iteration (ms): 4096.6 | learning rate: 1.500E-04 | lm loss: 1.064221E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.25
START iteration 243, CKPT_AND_STOP: False
[2022-12-05 13:10:18.874897] Finished iteration 244, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4114.040
[2022-12-05 13:10:18.875510] iteration      244/   18750 | elapsed time per iteration (ms): 4114.0 | learning rate: 1.500E-04 | lm loss: 1.063931E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.20
START iteration 244, CKPT_AND_STOP: False
[2022-12-05 13:10:22.959713] Finished iteration 245, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4084.816
[2022-12-05 13:10:22.960358] iteration      245/   18750 | elapsed time per iteration (ms): 4084.8 | learning rate: 1.500E-04 | lm loss: 1.063778E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.30
START iteration 245, CKPT_AND_STOP: False
[2022-12-05 13:10:27.103485] Finished iteration 246, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4143.772
[2022-12-05 13:10:27.103977] iteration      246/   18750 | elapsed time per iteration (ms): 4143.6 | learning rate: 1.500E-04 | lm loss: 1.063644E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.00 | batch generator: 2.25
START iteration 246, CKPT_AND_STOP: False
[2022-12-05 13:10:31.200685] Finished iteration 247, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4097.201
[2022-12-05 13:10:31.201073] iteration      247/   18750 | elapsed time per iteration (ms): 4097.1 | learning rate: 1.500E-04 | lm loss: 1.063300E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.94
START iteration 247, CKPT_AND_STOP: False
[2022-12-05 13:10:35.314639] Finished iteration 248, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4113.954
[2022-12-05 13:10:35.315034] iteration      248/   18750 | elapsed time per iteration (ms): 4113.9 | learning rate: 1.500E-04 | lm loss: 1.063208E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.09 | batch generator: 2.00
START iteration 248, CKPT_AND_STOP: False
[2022-12-05 13:10:39.419402] Finished iteration 249, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4104.763
[2022-12-05 13:10:39.419816] iteration      249/   18750 | elapsed time per iteration (ms): 4104.8 | learning rate: 1.500E-04 | lm loss: 1.062769E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.15
START iteration 249, CKPT_AND_STOP: False
[2022-12-05 13:10:43.548774] Finished iteration 250, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4129.372
[2022-12-05 13:10:43.549145] iteration      250/   18750 | elapsed time per iteration (ms): 4129.3 | learning rate: 1.500E-04 | lm loss: 1.062383E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 4.47
START iteration 250, CKPT_AND_STOP: False
[2022-12-05 13:10:47.649428] Finished iteration 251, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4100.654
[2022-12-05 13:10:47.649922] iteration      251/   18750 | elapsed time per iteration (ms): 4100.7 | learning rate: 1.500E-04 | lm loss: 1.062649E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.97
START iteration 251, CKPT_AND_STOP: False
[2022-12-05 13:10:51.701788] Finished iteration 252, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4052.360
[2022-12-05 13:10:51.702445] iteration      252/   18750 | elapsed time per iteration (ms): 4052.5 | learning rate: 1.500E-04 | lm loss: 1.062572E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.35
START iteration 252, CKPT_AND_STOP: False
[2022-12-05 13:10:55.767043] Finished iteration 253, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4065.255
[2022-12-05 13:10:55.767449] iteration      253/   18750 | elapsed time per iteration (ms): 4065.0 | learning rate: 1.500E-04 | lm loss: 1.061711E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.15
START iteration 253, CKPT_AND_STOP: False
[2022-12-05 13:10:59.799295] Finished iteration 254, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4032.251
[2022-12-05 13:10:59.799963] iteration      254/   18750 | elapsed time per iteration (ms): 4032.5 | learning rate: 1.500E-04 | lm loss: 1.061673E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.89
START iteration 254, CKPT_AND_STOP: False
[2022-12-05 13:11:03.845550] Finished iteration 255, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4046.257
[2022-12-05 13:11:03.845987] iteration      255/   18750 | elapsed time per iteration (ms): 4046.0 | learning rate: 1.500E-04 | lm loss: 1.061654E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.27
START iteration 255, CKPT_AND_STOP: False
[2022-12-05 13:11:07.932071] Finished iteration 256, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4086.521
[2022-12-05 13:11:07.932469] iteration      256/   18750 | elapsed time per iteration (ms): 4086.5 | learning rate: 1.500E-04 | lm loss: 1.061533E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.86
START iteration 256, CKPT_AND_STOP: False
[2022-12-05 13:11:11.974981] Finished iteration 257, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4042.908
[2022-12-05 13:11:11.975512] iteration      257/   18750 | elapsed time per iteration (ms): 4043.0 | learning rate: 1.500E-04 | lm loss: 1.061559E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.49
START iteration 257, CKPT_AND_STOP: False
[2022-12-05 13:11:16.108616] Finished iteration 258, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4133.635
[2022-12-05 13:11:16.109189] iteration      258/   18750 | elapsed time per iteration (ms): 4133.6 | learning rate: 1.500E-04 | lm loss: 1.061712E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.09
START iteration 258, CKPT_AND_STOP: False
[2022-12-05 13:11:20.315133] Finished iteration 259, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4206.517
[2022-12-05 13:11:20.315522] iteration      259/   18750 | elapsed time per iteration (ms): 4206.3 | learning rate: 1.500E-04 | lm loss: 1.061650E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.32
START iteration 259, CKPT_AND_STOP: False
[2022-12-05 13:11:24.445168] Finished iteration 260, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4130.036
[2022-12-05 13:11:24.445578] iteration      260/   18750 | elapsed time per iteration (ms): 4130.0 | learning rate: 1.500E-04 | lm loss: 1.061802E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.26
START iteration 260, CKPT_AND_STOP: False
[2022-12-05 13:11:28.506943] Finished iteration 261, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4061.772
[2022-12-05 13:11:28.507674] iteration      261/   18750 | elapsed time per iteration (ms): 4062.1 | learning rate: 1.500E-04 | lm loss: 1.061658E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.22
START iteration 261, CKPT_AND_STOP: False
[2022-12-05 13:11:32.588229] Finished iteration 262, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4081.288
[2022-12-05 13:11:32.588841] iteration      262/   18750 | elapsed time per iteration (ms): 4081.1 | learning rate: 1.500E-04 | lm loss: 1.061694E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.59
START iteration 262, CKPT_AND_STOP: False
[2022-12-05 13:11:36.727538] Finished iteration 263, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4139.308
[2022-12-05 13:11:36.728185] iteration      263/   18750 | elapsed time per iteration (ms): 4139.3 | learning rate: 1.500E-04 | lm loss: 1.061452E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.18
START iteration 263, CKPT_AND_STOP: False
[2022-12-05 13:11:40.783354] Finished iteration 264, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4055.817
[2022-12-05 13:11:40.783917] iteration      264/   18750 | elapsed time per iteration (ms): 4055.7 | learning rate: 1.500E-04 | lm loss: 1.061561E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.17
START iteration 264, CKPT_AND_STOP: False
[2022-12-05 13:11:44.914390] Finished iteration 265, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4131.035
[2022-12-05 13:11:44.914942] iteration      265/   18750 | elapsed time per iteration (ms): 4131.0 | learning rate: 1.500E-04 | lm loss: 1.062245E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.02 | batch generator: 2.92
START iteration 265, CKPT_AND_STOP: False
[2022-12-05 13:11:48.992870] Finished iteration 266, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4078.480
[2022-12-05 13:11:48.993557] iteration      266/   18750 | elapsed time per iteration (ms): 4078.6 | learning rate: 1.500E-04 | lm loss: 1.062313E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.10 | batch generator: 2.23
START iteration 266, CKPT_AND_STOP: False
[2022-12-05 13:11:53.059891] Finished iteration 267, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4067.022
[2022-12-05 13:11:53.060419] iteration      267/   18750 | elapsed time per iteration (ms): 4066.8 | learning rate: 1.500E-04 | lm loss: 1.062215E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.18
START iteration 267, CKPT_AND_STOP: False
[2022-12-05 13:11:57.145020] Finished iteration 268, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4085.129
[2022-12-05 13:11:57.145594] iteration      268/   18750 | elapsed time per iteration (ms): 4085.2 | learning rate: 1.500E-04 | lm loss: 1.062168E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.29
START iteration 268, CKPT_AND_STOP: False
[2022-12-05 13:12:01.223107] Finished iteration 269, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4078.087
[2022-12-05 13:12:01.223498] iteration      269/   18750 | elapsed time per iteration (ms): 4077.9 | learning rate: 1.500E-04 | lm loss: 1.062487E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.25
START iteration 269, CKPT_AND_STOP: False
[2022-12-05 13:12:05.333059] Finished iteration 270, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4109.952
[2022-12-05 13:12:05.333488] iteration      270/   18750 | elapsed time per iteration (ms): 4110.0 | learning rate: 1.500E-04 | lm loss: 1.062288E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.84
START iteration 270, CKPT_AND_STOP: False
[2022-12-05 13:12:09.396766] Finished iteration 271, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4063.706
[2022-12-05 13:12:09.397356] iteration      271/   18750 | elapsed time per iteration (ms): 4063.8 | learning rate: 1.500E-04 | lm loss: 1.062475E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.00 | batch generator: 2.01
START iteration 271, CKPT_AND_STOP: False
[2022-12-05 13:12:13.486653] Finished iteration 272, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4089.887
[2022-12-05 13:12:13.487216] iteration      272/   18750 | elapsed time per iteration (ms): 4089.8 | learning rate: 1.500E-04 | lm loss: 1.062236E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.41
START iteration 272, CKPT_AND_STOP: False
[2022-12-05 13:12:17.558493] Finished iteration 273, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4071.840
[2022-12-05 13:12:17.558879] iteration      273/   18750 | elapsed time per iteration (ms): 4071.6 | learning rate: 1.500E-04 | lm loss: 1.062212E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.65
START iteration 273, CKPT_AND_STOP: False
[2022-12-05 13:12:21.610266] Finished iteration 274, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4051.774
[2022-12-05 13:12:21.610654] iteration      274/   18750 | elapsed time per iteration (ms): 4051.8 | learning rate: 1.500E-04 | lm loss: 1.062253E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 1.90
START iteration 274, CKPT_AND_STOP: False
[2022-12-05 13:12:25.659038] Finished iteration 275, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4048.772
[2022-12-05 13:12:25.659433] iteration      275/   18750 | elapsed time per iteration (ms): 4048.8 | learning rate: 1.500E-04 | lm loss: 1.062228E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.04
START iteration 275, CKPT_AND_STOP: False
[2022-12-05 13:12:29.757207] Finished iteration 276, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4098.169
[2022-12-05 13:12:29.757655] iteration      276/   18750 | elapsed time per iteration (ms): 4098.2 | learning rate: 1.500E-04 | lm loss: 1.062312E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.21
START iteration 276, CKPT_AND_STOP: False
[2022-12-05 13:12:33.841567] Finished iteration 277, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4084.360
[2022-12-05 13:12:33.841955] iteration      277/   18750 | elapsed time per iteration (ms): 4084.3 | learning rate: 1.500E-04 | lm loss: 1.062234E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.26
START iteration 277, CKPT_AND_STOP: False
[2022-12-05 13:12:37.954072] Finished iteration 278, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4112.503
[2022-12-05 13:12:37.954804] iteration      278/   18750 | elapsed time per iteration (ms): 4112.8 | learning rate: 1.500E-04 | lm loss: 1.062217E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.01 | batch generator: 1.97
START iteration 278, CKPT_AND_STOP: False
[2022-12-05 13:12:42.086013] Finished iteration 279, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4131.942
[2022-12-05 13:12:42.086428] iteration      279/   18750 | elapsed time per iteration (ms): 4131.6 | learning rate: 1.500E-04 | lm loss: 1.062175E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 7.03
START iteration 279, CKPT_AND_STOP: False
[2022-12-05 13:12:46.156565] Finished iteration 280, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4070.552
[2022-12-05 13:12:46.157249] iteration      280/   18750 | elapsed time per iteration (ms): 4070.8 | learning rate: 1.500E-04 | lm loss: 1.062168E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.95
START iteration 280, CKPT_AND_STOP: False
[2022-12-05 13:12:50.240027] Finished iteration 281, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4083.462
[2022-12-05 13:12:50.240502] iteration      281/   18750 | elapsed time per iteration (ms): 4083.2 | learning rate: 1.500E-04 | lm loss: 1.061965E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.08
START iteration 281, CKPT_AND_STOP: False
[2022-12-05 13:12:54.436456] Finished iteration 282, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4196.430
[2022-12-05 13:12:54.436908] iteration      282/   18750 | elapsed time per iteration (ms): 4196.4 | learning rate: 1.500E-04 | lm loss: 1.061970E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.13
START iteration 282, CKPT_AND_STOP: False
[2022-12-05 13:12:58.511846] Finished iteration 283, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4075.390
[2022-12-05 13:12:58.512231] iteration      283/   18750 | elapsed time per iteration (ms): 4075.3 | learning rate: 1.500E-04 | lm loss: 1.061776E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.34
START iteration 283, CKPT_AND_STOP: False
[2022-12-05 13:13:02.627988] Finished iteration 284, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4116.142
[2022-12-05 13:13:02.628392] iteration      284/   18750 | elapsed time per iteration (ms): 4116.1 | learning rate: 1.500E-04 | lm loss: 1.061835E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.05
START iteration 284, CKPT_AND_STOP: False
[2022-12-05 13:13:06.686254] Finished iteration 285, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4058.265
[2022-12-05 13:13:06.686647] iteration      285/   18750 | elapsed time per iteration (ms): 4058.2 | learning rate: 1.500E-04 | lm loss: 1.061903E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.14
START iteration 285, CKPT_AND_STOP: False
[2022-12-05 13:13:10.751012] Finished iteration 286, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4064.759
[2022-12-05 13:13:10.751413] iteration      286/   18750 | elapsed time per iteration (ms): 4064.8 | learning rate: 1.500E-04 | lm loss: 1.061778E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.09
START iteration 286, CKPT_AND_STOP: False
[2022-12-05 13:13:15.302747] Finished iteration 287, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4551.735
[2022-12-05 13:13:15.303246] iteration      287/   18750 | elapsed time per iteration (ms): 4551.8 | learning rate: 1.500E-04 | lm loss: 1.061977E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.29
START iteration 287, CKPT_AND_STOP: False
[2022-12-05 13:13:19.374889] Finished iteration 288, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4072.142
[2022-12-05 13:13:19.375358] iteration      288/   18750 | elapsed time per iteration (ms): 4072.1 | learning rate: 1.500E-04 | lm loss: 1.061843E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.99
START iteration 288, CKPT_AND_STOP: False
[2022-12-05 13:13:23.397911] Finished iteration 289, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4023.021
[2022-12-05 13:13:23.398379] iteration      289/   18750 | elapsed time per iteration (ms): 4023.0 | learning rate: 1.500E-04 | lm loss: 1.061859E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.48
START iteration 289, CKPT_AND_STOP: False
[2022-12-05 13:13:27.553820] Finished iteration 290, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4155.908
[2022-12-05 13:13:27.554363] iteration      290/   18750 | elapsed time per iteration (ms): 4156.0 | learning rate: 1.500E-04 | lm loss: 1.061866E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.00 | batch generator: 1.96
START iteration 290, CKPT_AND_STOP: False
[2022-12-05 13:13:31.718553] Finished iteration 291, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4164.734
[2022-12-05 13:13:31.718941] iteration      291/   18750 | elapsed time per iteration (ms): 4164.6 | learning rate: 1.500E-04 | lm loss: 1.061748E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.23
START iteration 291, CKPT_AND_STOP: False
[2022-12-05 13:13:35.864906] Finished iteration 292, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4146.353
[2022-12-05 13:13:35.865346] iteration      292/   18750 | elapsed time per iteration (ms): 4146.4 | learning rate: 1.500E-04 | lm loss: 1.061989E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 4.37
START iteration 292, CKPT_AND_STOP: False
[2022-12-05 13:13:39.943476] Finished iteration 293, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4078.570
[2022-12-05 13:13:39.943936] iteration      293/   18750 | elapsed time per iteration (ms): 4078.6 | learning rate: 1.500E-04 | lm loss: 1.061833E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.00 | batch generator: 2.19
START iteration 293, CKPT_AND_STOP: False
[2022-12-05 13:13:43.938565] Finished iteration 294, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3995.088
[2022-12-05 13:13:43.938955] iteration      294/   18750 | elapsed time per iteration (ms): 3995.0 | learning rate: 1.500E-04 | lm loss: 1.061714E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.01
START iteration 294, CKPT_AND_STOP: False
[2022-12-05 13:13:47.995585] Finished iteration 295, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4057.020
[2022-12-05 13:13:47.995979] iteration      295/   18750 | elapsed time per iteration (ms): 4057.0 | learning rate: 1.500E-04 | lm loss: 1.061797E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.93
START iteration 295, CKPT_AND_STOP: False
[2022-12-05 13:13:52.130445] Finished iteration 296, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4134.861
[2022-12-05 13:13:52.130895] iteration      296/   18750 | elapsed time per iteration (ms): 4134.9 | learning rate: 1.500E-04 | lm loss: 1.061880E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 3.98
START iteration 296, CKPT_AND_STOP: False
[2022-12-05 13:13:56.192776] Finished iteration 297, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4062.330
[2022-12-05 13:13:56.193322] iteration      297/   18750 | elapsed time per iteration (ms): 4062.4 | learning rate: 1.500E-04 | lm loss: 1.061994E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 1.95
START iteration 297, CKPT_AND_STOP: False
[2022-12-05 13:14:01.067881] Finished iteration 298, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4875.105
[2022-12-05 13:14:01.068402] iteration      298/   18750 | elapsed time per iteration (ms): 4875.1 | learning rate: 1.500E-04 | lm loss: 1.061977E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 1.91
START iteration 298, CKPT_AND_STOP: False
[2022-12-05 13:14:05.128373] Finished iteration 299, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4060.493
[2022-12-05 13:14:05.128788] iteration      299/   18750 | elapsed time per iteration (ms): 4060.4 | learning rate: 1.500E-04 | lm loss: 1.061954E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.45
START iteration 299, CKPT_AND_STOP: False
[2022-12-05 13:14:09.245519] Finished iteration 300, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4117.147
[2022-12-05 13:14:09.245931] iteration      300/   18750 | elapsed time per iteration (ms): 4117.1 | learning rate: 1.500E-04 | lm loss: 1.061798E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.04
START iteration 300, CKPT_AND_STOP: False
[2022-12-05 13:14:13.318549] Finished iteration 301, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4073.029
[2022-12-05 13:14:13.318941] iteration      301/   18750 | elapsed time per iteration (ms): 4073.0 | learning rate: 1.500E-04 | lm loss: 1.061858E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.26
START iteration 301, CKPT_AND_STOP: False
[2022-12-05 13:14:17.514119] Finished iteration 302, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4195.569
[2022-12-05 13:14:17.514581] iteration      302/   18750 | elapsed time per iteration (ms): 4195.6 | learning rate: 1.500E-04 | lm loss: 1.061837E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.11 | batch generator: 1.76
START iteration 302, CKPT_AND_STOP: False
[2022-12-05 13:14:21.604548] Finished iteration 303, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4090.430
[2022-12-05 13:14:21.604959] iteration      303/   18750 | elapsed time per iteration (ms): 4090.4 | learning rate: 1.500E-04 | lm loss: 1.061840E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.28
START iteration 303, CKPT_AND_STOP: False
[2022-12-05 13:14:25.701901] Finished iteration 304, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4097.352
[2022-12-05 13:14:25.702314] iteration      304/   18750 | elapsed time per iteration (ms): 4097.3 | learning rate: 1.500E-04 | lm loss: 1.061788E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.00
START iteration 304, CKPT_AND_STOP: False
[2022-12-05 13:14:29.751478] Finished iteration 305, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4049.578
[2022-12-05 13:14:29.751885] iteration      305/   18750 | elapsed time per iteration (ms): 4049.6 | learning rate: 1.500E-04 | lm loss: 1.061891E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.86
START iteration 305, CKPT_AND_STOP: False
[2022-12-05 13:14:33.829632] Finished iteration 306, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4078.154
[2022-12-05 13:14:33.830035] iteration      306/   18750 | elapsed time per iteration (ms): 4078.1 | learning rate: 1.500E-04 | lm loss: 1.061870E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.94
START iteration 306, CKPT_AND_STOP: False
[2022-12-05 13:14:37.856142] Finished iteration 307, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4026.509
[2022-12-05 13:14:37.856679] iteration      307/   18750 | elapsed time per iteration (ms): 4026.6 | learning rate: 1.500E-04 | lm loss: 1.061794E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 2.09
START iteration 307, CKPT_AND_STOP: False
[2022-12-05 13:14:41.953542] Finished iteration 308, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4097.401
[2022-12-05 13:14:41.953931] iteration      308/   18750 | elapsed time per iteration (ms): 4097.2 | learning rate: 1.500E-04 | lm loss: 1.061895E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.06
START iteration 308, CKPT_AND_STOP: False
[2022-12-05 13:14:46.141293] Finished iteration 309, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4187.749
[2022-12-05 13:14:46.141916] iteration      309/   18750 | elapsed time per iteration (ms): 4188.0 | learning rate: 1.500E-04 | lm loss: 1.061758E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 1.82
START iteration 309, CKPT_AND_STOP: False
[2022-12-05 13:14:50.212749] Finished iteration 310, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4071.457
[2022-12-05 13:14:50.213139] iteration      310/   18750 | elapsed time per iteration (ms): 4071.2 | learning rate: 1.500E-04 | lm loss: 1.061730E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.96 | batch generator: 2.16
START iteration 310, CKPT_AND_STOP: False
[2022-12-05 13:14:54.260672] Finished iteration 311, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4047.923
[2022-12-05 13:14:54.261213] iteration      311/   18750 | elapsed time per iteration (ms): 4048.1 | learning rate: 1.500E-04 | lm loss: 1.061826E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 1.97
START iteration 311, CKPT_AND_STOP: False
[2022-12-05 13:14:58.420565] Finished iteration 312, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4159.893
[2022-12-05 13:14:58.420956] iteration      312/   18750 | elapsed time per iteration (ms): 4159.7 | learning rate: 1.500E-04 | lm loss: 1.061893E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 2.04
START iteration 312, CKPT_AND_STOP: False
[2022-12-05 13:15:02.520312] Finished iteration 313, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4099.747
[2022-12-05 13:15:02.520802] iteration      313/   18750 | elapsed time per iteration (ms): 4099.8 | learning rate: 1.500E-04 | lm loss: 1.061882E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.83
START iteration 313, CKPT_AND_STOP: False
[2022-12-05 13:15:06.610259] Finished iteration 314, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4089.947
[2022-12-05 13:15:06.610665] iteration      314/   18750 | elapsed time per iteration (ms): 4089.8 | learning rate: 1.500E-04 | lm loss: 1.061773E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.97 | batch generator: 2.13
START iteration 314, CKPT_AND_STOP: False
[2022-12-05 13:15:10.716487] Finished iteration 315, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4106.228
[2022-12-05 13:15:10.717009] iteration      315/   18750 | elapsed time per iteration (ms): 4106.3 | learning rate: 1.500E-04 | lm loss: 1.061713E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.05 | batch generator: 2.19
START iteration 315, CKPT_AND_STOP: False
[2022-12-05 13:15:14.845986] Finished iteration 316, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4129.499
[2022-12-05 13:15:14.846515] iteration      316/   18750 | elapsed time per iteration (ms): 4129.5 | learning rate: 1.500E-04 | lm loss: 1.061880E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.98 | batch generator: 2.31
START iteration 316, CKPT_AND_STOP: False
[2022-12-05 13:15:18.948111] Finished iteration 317, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4102.126
[2022-12-05 13:15:18.948540] iteration      317/   18750 | elapsed time per iteration (ms): 4102.0 | learning rate: 1.500E-04 | lm loss: 1.061772E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.92 | batch generator: 2.27
START iteration 317, CKPT_AND_STOP: False
[2022-12-05 13:15:23.013396] Finished iteration 318, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4065.284
[2022-12-05 13:15:23.013817] iteration      318/   18750 | elapsed time per iteration (ms): 4065.3 | learning rate: 1.500E-04 | lm loss: 1.061840E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 14.08 | batch generator: 2.33
START iteration 318, CKPT_AND_STOP: False
[2022-12-05 13:15:27.068770] Finished iteration 319, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4055.374
[2022-12-05 13:15:27.069192] iteration      319/   18750 | elapsed time per iteration (ms): 4055.4 | learning rate: 1.500E-04 | lm loss: 1.061785E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.95 | batch generator: 2.36
START iteration 319, CKPT_AND_STOP: False
[2022-12-05 13:15:31.149951] Finished iteration 320, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4081.181
[2022-12-05 13:15:31.150340] iteration      320/   18750 | elapsed time per iteration (ms): 4081.1 | learning rate: 1.500E-04 | lm loss: 1.061820E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.99 | batch generator: 2.35
START iteration 320, CKPT_AND_STOP: False
[2022-12-05 13:15:36.105028] Finished iteration 321, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4955.078
[2022-12-05 13:15:36.105445] iteration      321/   18750 | elapsed time per iteration (ms): 4955.1 | learning rate: 1.500E-04 | lm loss: 1.061761E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.93 | batch generator: 1.99
START iteration 321, CKPT_AND_STOP: False
[2022-12-05 13:15:40.109013] Finished iteration 322, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4003.985
[2022-12-05 13:15:40.109508] iteration      322/   18750 | elapsed time per iteration (ms): 4004.0 | learning rate: 1.500E-04 | lm loss: 1.061836E+01 | loss scale: 131072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | optimizer: 13.94 | batch generator: 1.88
START iteration 322, CKPT_AND_STOP: False
Signal handler called with signal 10


 STOPPING VARUNA !!



