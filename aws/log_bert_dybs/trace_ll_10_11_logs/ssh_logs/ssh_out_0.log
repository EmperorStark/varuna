Parent process ID: 76835 node: 172.31.28.108
24 cutpoints
Stages 1
13 20868437708.800007
7 13915698585.599995
10 17153729331.199993
8 14622140211.200005
9 15846820659.199995
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 7 0 1318861.9384765625 0
End of simulation:  Mini-batch time (usec) = 3575127
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 125868, max long fwd 129698; min long bwd 191031, max long bwd 199310
Time taken by simulation: 54 microseconds

Stages 2
13 10987980083.2
19 14996390399.999996
22 16963644723.199997
20 15631126630.400002
Predicted microbatch size for 2: 19
comm size 9961472
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 6 0 608928.955078125 542254.6839368516
End of simulation:  Mini-batch time (usec) = 3931026
Min send: 10000000, max send 0
Min long send: 542586, max long send 555338
Min fwd: 106816, max fwd 113936; min bwd 153953, max bwd 157645
Min long fwd: 134083, max long fwd 138708; min long bwd 168934, max long bwd 174223
Time taken by simulation: 133 microseconds

Stages 3
13 7832942284.800001
19 10697074380.8
22 12097432473.599998
23 12575272243.2
24 13035207577.6
Predicted microbatch size for 3: 24
comm size 12582912
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 8 0 399702.3010253906 663682.2411979454
End of simulation:  Mini-batch time (usec) = 5733616
Min send: 10000000, max send 0
Min long send: 663764, max long send 684071
Min fwd: 70710, max fwd 87166; min bwd 113708, max bwd 132902
Min long fwd: 107949, max long fwd 112670; min long bwd 147972, max long bwd 155784
Time taken by simulation: 390 microseconds

Stages 4
13 6255423385.6
19 8547416371.2
22 9664326348.8
23 10045446451.2
24 10412856422.4
Predicted microbatch size for 4: 24
comm size 12582912
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 11 0 298590.2404785156 663682.2411979454
End of simulation:  Mini-batch time (usec) = 7279061
Min send: 10000000, max send 0
Min long send: 663870, max long send 686148
Min fwd: 51765, max fwd 67812; min bwd 85632, max bwd 99460
Min long fwd: 85707, max long fwd 91318; min long bwd 118788, max long bwd 124254
Time taken by simulation: 788 microseconds

Stages 6
13 4698582732.8
19 6397758361.599999
22 7231220224.0
23 7515620659.2
24 7790505267.2
Predicted microbatch size for 6: 24
comm size 12582912
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 15 0 205807.43408203125 663682.2411979454
End of simulation:  Mini-batch time (usec) = 11152793
Min send: 10000000, max send 0
Min long send: 663736, max long send 688387
Min fwd: 32603, max fwd 47172; min bwd 54003, max bwd 69793
Min long fwd: 66617, max long fwd 70492; min long bwd 81618, max long bwd 89601
Time taken by simulation: 1062 microseconds

Stages 8
13 3930501529.6000004
19 5322929356.799999
22 6014667161.6
23 6250707763.2
24 6479329689.6
Predicted microbatch size for 8: 24
comm size 12582912
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 120660.06469726562 663682.2411979454
End of simulation:  Mini-batch time (usec) = 15792286
Min send: 10000000, max send 0
Min long send: 663682, max long send 690844
Min fwd: 23237, max fwd 38139; min bwd 35712, max bwd 56890
Min long fwd: 51613, max long fwd 57677; min long bwd 69478, max long bwd 76496
Time taken by simulation: 2529 microseconds

Stages 12
13 3141742080.0
19 4248100352.0
22 4798114099.200001
23 4985794867.2
24 5168154112.0
Predicted microbatch size for 12: 24
comm size 12582912
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 43 0 0 663682.2411979454
End of simulation:  Mini-batch time (usec) = 28245316
Min send: 10000000, max send 0
Min long send: 663682, max long send 693511
Min fwd: 10606, max fwd 31119; min bwd 22396, max bwd 41317
Min long fwd: 40838, max long fwd 49874; min long bwd 47936, max long bwd 55473
Time taken by simulation: 7152 microseconds

{1: 3.575127, 2: 3.931026, 3: 5.733616, 4: 7.279061, 6: 11.152793, 8: 15.792286, 12: 28.245316}
{1: 8, 2: 19, 3: 24, 4: 24, 6: 24, 8: 24, 12: 24}
best config is: 2 19
expected time is 3.931026
9 per stage
18 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 19
data depth: 9
stage to rank map: 0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;
World size is 18
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=19 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17; --batch-size=113 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-24 14:43:11.451794 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=113, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=False, resume_step=-1, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14,16;1,3,5,7,9,11,13,15,17;', chunk_size=19, batch_size=113, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.10882282257080078
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
6 chunks
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
DLL 2022-11-24 14:43:22.045807 - PARAMETER SEED : 12439 
DLL 2022-11-24 14:43:22.045959 - PARAMETER train_start : True 
DLL 2022-11-24 14:43:22.046036 - PARAMETER batch_size_per_gpu : 113 
DLL 2022-11-24 14:43:22.046124 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-24 14:43:27.785093] Finished iteration 0, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 5694.349
DLL 2022-11-24 14:43:27.796507 - Training Epoch: 0 Training Iteration: 1  average_loss : 11.231771469116211  step_loss : 11.231771469116211  learning_rate : 2.9986455118223097e-06 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-24 14:43:30.654151] Finished iteration 1, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2868.735
DLL 2022-11-24 14:43:30.661965 - Training Epoch: 0 Training Iteration: 2  average_loss : nan  step_loss : nan  learning_rate : 5.9972910236446195e-06 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-24 14:43:33.491155] Finished iteration 2, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2837.042
DLL 2022-11-24 14:43:33.496239 - Training Epoch: 0 Training Iteration: 3  average_loss : nan  step_loss : nan  learning_rate : 8.995936535466931e-06 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-24 14:43:37.334831] Finished iteration 3, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3843.624
DLL 2022-11-24 14:43:37.339992 - Training Epoch: 0 Training Iteration: 4  average_loss : nan  step_loss : nan  learning_rate : 1.1994582047289239e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-24 14:43:41.097194] Finished iteration 4, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3762.300
DLL 2022-11-24 14:43:41.102121 - Training Epoch: 0 Training Iteration: 5  average_loss : nan  step_loss : nan  learning_rate : 1.499322755911155e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-24 14:43:43.839794] Finished iteration 5, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2742.570
DLL 2022-11-24 14:43:43.844743 - Training Epoch: 0 Training Iteration: 6  average_loss : nan  step_loss : nan  learning_rate : 1.7991873070933862e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-24 14:43:46.658991] Finished iteration 6, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2819.126
DLL 2022-11-24 14:43:46.664706 - Training Epoch: 0 Training Iteration: 7  average_loss : nan  step_loss : nan  learning_rate : 2.099051858275617e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-24 14:43:49.465420] Finished iteration 7, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2806.389
DLL 2022-11-24 14:43:49.470810 - Training Epoch: 0 Training Iteration: 8  average_loss : nan  step_loss : nan  learning_rate : 2.3989164094578478e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-24 14:43:52.249741] Finished iteration 8, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2784.293
DLL 2022-11-24 14:43:52.254576 - Training Epoch: 0 Training Iteration: 9  average_loss : nan  step_loss : nan  learning_rate : 2.698780960640079e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-24 14:43:55.237518] Finished iteration 9, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2987.754
DLL 2022-11-24 14:43:55.242175 - Training Epoch: 0 Training Iteration: 10  average_loss : nan  step_loss : nan  learning_rate : 2.99864551182231e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-24 14:43:58.025873] Finished iteration 10, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2788.345
DLL 2022-11-24 14:43:58.030601 - Training Epoch: 0 Training Iteration: 11  average_loss : nan  step_loss : nan  learning_rate : 3.298510063004541e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-24 14:44:00.815020] Finished iteration 11, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2789.087
DLL 2022-11-24 14:44:00.819861 - Training Epoch: 0 Training Iteration: 12  average_loss : nan  step_loss : nan  learning_rate : 3.5983746141867724e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-24 14:44:03.614904] Finished iteration 12, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2799.886
DLL 2022-11-24 14:44:03.619807 - Training Epoch: 0 Training Iteration: 13  average_loss : nan  step_loss : nan  learning_rate : 3.898239165369003e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-24 14:44:06.456224] Finished iteration 13, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2841.299
DLL 2022-11-24 14:44:06.461478 - Training Epoch: 0 Training Iteration: 14  average_loss : nan  step_loss : nan  learning_rate : 4.198103716551234e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-24 14:44:09.275227] Finished iteration 14, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2818.928
DLL 2022-11-24 14:44:09.280144 - Training Epoch: 0 Training Iteration: 15  average_loss : nan  step_loss : nan  learning_rate : 4.497968267733465e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-24 14:44:12.098599] Finished iteration 15, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2823.368
DLL 2022-11-24 14:44:12.103655 - Training Epoch: 0 Training Iteration: 16  average_loss : nan  step_loss : nan  learning_rate : 4.7978328189156956e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-24 14:44:14.902281] Finished iteration 16, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2803.695
DLL 2022-11-24 14:44:14.907084 - Training Epoch: 0 Training Iteration: 17  average_loss : nan  step_loss : nan  learning_rate : 5.097697370097927e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4.0
[2022-11-24 14:44:17.741138] Finished iteration 17, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2838.770
DLL 2022-11-24 14:44:17.746092 - Training Epoch: 0 Training Iteration: 18  average_loss : nan  step_loss : nan  learning_rate : 5.397561921280158e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2.0
[2022-11-24 14:44:20.581284] Finished iteration 18, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2840.135
DLL 2022-11-24 14:44:20.586135 - Training Epoch: 0 Training Iteration: 19  average_loss : nan  step_loss : nan  learning_rate : 5.697426472462389e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:23.371590] Finished iteration 19, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2790.282
DLL 2022-11-24 14:44:23.376702 - Training Epoch: 0 Training Iteration: 20  average_loss : nan  step_loss : nan  learning_rate : 5.99729102364462e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:26.140799] Finished iteration 20, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2769.176
DLL 2022-11-24 14:44:26.182906 - Training Epoch: 0 Training Iteration: 21  average_loss : nan  step_loss : nan  learning_rate : 6.297155574826851e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:28.969320] Finished iteration 21, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2828.493
DLL 2022-11-24 14:44:28.974003 - Training Epoch: 0 Training Iteration: 22  average_loss : nan  step_loss : nan  learning_rate : 6.597020126009082e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:31.796919] Finished iteration 22, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2827.543
DLL 2022-11-24 14:44:31.801782 - Training Epoch: 0 Training Iteration: 23  average_loss : nan  step_loss : nan  learning_rate : 6.896884677191313e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:34.644959] Finished iteration 23, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2848.014
DLL 2022-11-24 14:44:34.650013 - Training Epoch: 0 Training Iteration: 24  average_loss : nan  step_loss : nan  learning_rate : 7.196749228373545e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:37.810870] Finished iteration 24, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3165.864
DLL 2022-11-24 14:44:37.815768 - Training Epoch: 0 Training Iteration: 25  average_loss : nan  step_loss : nan  learning_rate : 7.496613779555775e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:40.611072] Finished iteration 25, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2800.180
DLL 2022-11-24 14:44:40.615864 - Training Epoch: 0 Training Iteration: 26  average_loss : nan  step_loss : nan  learning_rate : 7.796478330738006e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:43.390146] Finished iteration 26, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2779.054
DLL 2022-11-24 14:44:43.394917 - Training Epoch: 0 Training Iteration: 27  average_loss : nan  step_loss : nan  learning_rate : 8.096342881920237e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:46.230042] Finished iteration 27, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2839.947
DLL 2022-11-24 14:44:46.235374 - Training Epoch: 0 Training Iteration: 28  average_loss : nan  step_loss : nan  learning_rate : 8.396207433102469e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:49.032816] Finished iteration 28, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2802.652
DLL 2022-11-24 14:44:49.037820 - Training Epoch: 0 Training Iteration: 29  average_loss : nan  step_loss : nan  learning_rate : 8.696071984284699e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:51.829923] Finished iteration 29, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2797.067
DLL 2022-11-24 14:44:51.834701 - Training Epoch: 0 Training Iteration: 30  average_loss : nan  step_loss : nan  learning_rate : 8.99593653546693e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:54.632195] Finished iteration 30, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2802.254
DLL 2022-11-24 14:44:54.637260 - Training Epoch: 0 Training Iteration: 31  average_loss : nan  step_loss : nan  learning_rate : 9.29580108664916e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:44:57.469282] Finished iteration 31, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2837.048
DLL 2022-11-24 14:44:57.474183 - Training Epoch: 0 Training Iteration: 32  average_loss : nan  step_loss : nan  learning_rate : 9.595665637831391e-05 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:45:00.235109] Finished iteration 32, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2765.788
DLL 2022-11-24 14:45:00.239889 - Training Epoch: 0 Training Iteration: 33  average_loss : nan  step_loss : nan  learning_rate : 9.895530189013622e-05 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1.0
[2022-11-24 14:45:03.049533] Finished iteration 33, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 2814.381
DLL 2022-11-24 14:45:03.054584 - Training Epoch: 0 Training Iteration: 34  average_loss : nan  step_loss : nan  learning_rate : 0.00010195394740195854 
2022-11-24 14:45:03.054706 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-24 14:45:03.054726 - PARAMETER checkpoint_step : 34 
Opt ckpt time 8.43617582321167
Process done with return code 0
Parent process ID: 77815 node: 172.31.28.108
24 cutpoints
Stages 1
13 20868437708.800007
7 13915698585.599995
10 17153729331.199993
8 14622140211.200005
9 15846820659.199995
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 8 0 1286522.0947265625 0
End of simulation:  Mini-batch time (usec) = 3866112
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 125868, max long fwd 129698; min long bwd 191031, max long bwd 199310
Time taken by simulation: 61 microseconds

Stages 2
13 10987980083.2
19 14996390399.999996
22 16963644723.199997
20 15631126630.400002
Predicted microbatch size for 2: 19
comm size 9961472
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 7 0 609002.8076171875 542254.6839368516
End of simulation:  Mini-batch time (usec) = 4220388
Min send: 10000000, max send 0
Min long send: 542586, max long send 556277
Min fwd: 106816, max fwd 113936; min bwd 154276, max bwd 159445
Min long fwd: 133244, max long fwd 138708; min long bwd 168934, max long bwd 174223
Time taken by simulation: 145 microseconds

Stages 3
13 7832942284.800001
19 10697074380.8
22 12097432473.599998
23 12575272243.2
24 13035207577.6
Predicted microbatch size for 3: 24
comm size 12582912
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 9 0 391075.1953125 663682.2411979454
End of simulation:  Mini-batch time (usec) = 5984079
Min send: 10000000, max send 0
Min long send: 664014, max long send 688387
Min fwd: 70710, max fwd 87989; min bwd 116815, max bwd 130195
Min long fwd: 106418, max long fwd 113434; min long bwd 146938, max long bwd 153983
Time taken by simulation: 323 microseconds

Stages 4
13 6255423385.6
19 8547416371.2
22 9664326348.8
23 10045446451.2
24 10412856422.4
Predicted microbatch size for 4: 24
comm size 12582912
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 11 0 298590.2404785156 663682.2411979454
End of simulation:  Mini-batch time (usec) = 7279061
Min send: 10000000, max send 0
Min long send: 663870, max long send 686148
Min fwd: 51765, max fwd 67812; min bwd 85632, max bwd 99460
Min long fwd: 85707, max long fwd 91318; min long bwd 118788, max long bwd 124254
Time taken by simulation: 498 microseconds

Stages 6
13 4698582732.8
19 6397758361.599999
22 7231220224.0
23 7515620659.2
24 7790505267.2
Predicted microbatch size for 6: 24
comm size 12582912
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 22 0 139487.73193359375 663682.2411979454
End of simulation:  Mini-batch time (usec) = 13247012
Min send: 10000000, max send 0
Min long send: 663764, max long send 688387
Min fwd: 32840, max fwd 46918; min bwd 55338, max bwd 72497
Min long fwd: 65566, max long fwd 72714; min long bwd 84609, max long bwd 89154
Time taken by simulation: 1599 microseconds

Stages 8
13 3930501529.6000004
19 5322929356.799999
22 6014667161.6
23 6250707763.2
24 6479329689.6
Predicted microbatch size for 8: 24
comm size 12582912
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 22 0 120660.06469726562 663682.2411979454
End of simulation:  Mini-batch time (usec) = 15792286
Min send: 10000000, max send 0
Min long send: 663682, max long send 690844
Min fwd: 23237, max fwd 38139; min bwd 35712, max bwd 56890
Min long fwd: 51613, max long fwd 57677; min long bwd 69478, max long bwd 76496
Time taken by simulation: 2240 microseconds

Stages 12
13 3141742080.0
19 4248100352.0
22 4798114099.200001
23 4985794867.2
24 5168154112.0
Predicted microbatch size for 12: 24
comm size 12582912
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 43 0 0 663682.2411979454
End of simulation:  Mini-batch time (usec) = 28245316
Min send: 10000000, max send 0
Min long send: 663682, max long send 693511
Min fwd: 10606, max fwd 31119; min bwd 22396, max bwd 41317
Min long fwd: 40838, max long fwd 49874; min long bwd 47936, max long bwd 55473
Time taken by simulation: 6949 microseconds

{1: 3.866112, 2: 4.220388, 3: 5.984079, 4: 7.279061, 6: 13.247012, 8: 15.792286, 12: 28.245316}
{1: 8, 2: 19, 3: 24, 4: 24, 6: 24, 8: 24, 12: 24}
best config is: 2 19
expected time is 4.220388
8 per stage
16 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 19
data depth: 8
stage to rank map: 0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;
World size is 16
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=19 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15; --batch-size=128 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 34
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-24 14:45:53.061583 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=128, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=34, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12,14;1,3,5,7,9,11,13,15;', chunk_size=19, batch_size=128, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.1253979206085205
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
7 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_34.pt
2022-11-24 14:46:03.345571 resume step from  34
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-24 14:46:50.819422 - Finished loading checkpoint, takes 47.345 secs
DLL 2022-11-24 14:46:50.820438 - PARAMETER SEED : 12439 
DLL 2022-11-24 14:46:50.820587 - PARAMETER train_start : True 
DLL 2022-11-24 14:46:50.820664 - PARAMETER batch_size_per_gpu : 128 
DLL 2022-11-24 14:46:50.820748 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-24 14:47:00.841214] Finished iteration 34, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6983.965
DLL 2022-11-24 14:47:00.847581 - Training Epoch: 0 Training Iteration: 35  average_loss : nan  step_loss : nan  learning_rate : 0.00010495259291378085 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-24 14:47:03.780603] Finished iteration 35, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 2939.083
DLL 2022-11-24 14:47:03.785787 - Training Epoch: 0 Training Iteration: 36  average_loss : nan  step_loss : nan  learning_rate : 0.00010795123842560316 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-24 14:47:07.929266] Finished iteration 36, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4148.649
DLL 2022-11-24 14:47:07.938387 - Training Epoch: 0 Training Iteration: 37  average_loss : nan  step_loss : nan  learning_rate : 0.00011094988393742548 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-24 14:47:11.046951] Finished iteration 37, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3117.629
DLL 2022-11-24 14:47:11.139359 - Training Epoch: 0 Training Iteration: 38  average_loss : nan  step_loss : nan  learning_rate : 0.00011394852944924778 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-24 14:47:14.263678] Finished iteration 38, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3216.718
DLL 2022-11-24 14:47:14.268638 - Training Epoch: 0 Training Iteration: 39  average_loss : nan  step_loss : nan  learning_rate : 0.00011694717496107008 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-24 14:47:17.384019] Finished iteration 39, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3120.259
DLL 2022-11-24 14:47:17.389047 - Training Epoch: 0 Training Iteration: 40  average_loss : nan  step_loss : nan  learning_rate : 0.0001199458204728924 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-24 14:47:20.479866] Finished iteration 40, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3095.840
DLL 2022-11-24 14:47:20.485101 - Training Epoch: 0 Training Iteration: 41  average_loss : nan  step_loss : nan  learning_rate : 0.0001229444659847147 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-24 14:47:23.579858] Finished iteration 41, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3099.994
DLL 2022-11-24 14:47:23.585022 - Training Epoch: 0 Training Iteration: 42  average_loss : nan  step_loss : nan  learning_rate : 0.00012594311149653702 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-24 14:47:26.658117] Finished iteration 42, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3078.187
DLL 2022-11-24 14:47:26.663314 - Training Epoch: 0 Training Iteration: 43  average_loss : nan  step_loss : nan  learning_rate : 0.00012894175700835933 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-24 14:47:29.776021] Finished iteration 43, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3117.919
DLL 2022-11-24 14:47:29.781213 - Training Epoch: 0 Training Iteration: 44  average_loss : nan  step_loss : nan  learning_rate : 0.00013194040252018164 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-24 14:47:32.892630] Finished iteration 44, CKPT_AND_STOP: True, flag: tensor([3], dtype=torch.int32), speed: 3116.559
DLL 2022-11-24 14:47:32.897640 - Training Epoch: 0 Training Iteration: 45  average_loss : nan  step_loss : nan  learning_rate : 0.00013493904803200396 
2022-11-24 14:47:32.897764 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-24 14:47:32.897785 - PARAMETER checkpoint_step : 45 
Opt ckpt time 9.262847185134888
Process done with return code 0
Parent process ID: 79277 node: 172.31.28.108
24 cutpoints
Stages 1
13 20868437708.800007
7 13915698585.599995
10 17153729331.199993
8 14622140211.200005
9 15846820659.199995
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 9 0 1278524.7802734375 0
End of simulation:  Mini-batch time (usec) = 4182966
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 125868, max long fwd 130461; min long bwd 191031, max long bwd 199310
Time taken by simulation: 61 microseconds

Stages 2
13 10987980083.2
19 14996390399.999996
22 16963644723.199997
20 15631126630.400002
Predicted microbatch size for 2: 19
comm size 9961472
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 8 0 600510.986328125 542254.6839368516
End of simulation:  Mini-batch time (usec) = 4525989
Min send: 10000000, max send 0
Min long send: 542442, max long send 556435
Min fwd: 107744, max fwd 113936; min bwd 154028, max bwd 157296
Min long fwd: 133001, max long fwd 138295; min long bwd 169394, max long bwd 174223
Time taken by simulation: 165 microseconds

Stages 3
13 7832942284.800001
19 10697074380.8
22 12097432473.599998
23 12575272243.2
24 13035207577.6
Predicted microbatch size for 3: 24
comm size 12582912
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 9 0 391075.1953125 663682.2411979454
End of simulation:  Mini-batch time (usec) = 5984079
Min send: 10000000, max send 0
Min long send: 664014, max long send 688387
Min fwd: 70710, max fwd 87989; min bwd 116815, max bwd 130195
Min long fwd: 106418, max long fwd 113434; min long bwd 146938, max long bwd 153983
Time taken by simulation: 294 microseconds

Stages 4
13 6255423385.6
19 8547416371.2
22 9664326348.8
23 10045446451.2
24 10412856422.4
Predicted microbatch size for 4: 24
comm size 12582912
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 15 0 264588.56201171875 663682.2411979454
End of simulation:  Mini-batch time (usec) = 8956234
Min send: 10000000, max send 0
Min long send: 663870, max long send 688176
Min fwd: 51018, max fwd 67812; min bwd 86129, max bwd 98869
Min long fwd: 83628, max long fwd 91318; min long bwd 119532, max long bwd 124677
Time taken by simulation: 680 microseconds

Stages 6
13 4698582732.8
19 6397758361.599999
22 7231220224.0
23 7515620659.2
24 7790505267.2
Predicted microbatch size for 6: 24
comm size 12582912
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 22 0 139487.73193359375 663682.2411979454
End of simulation:  Mini-batch time (usec) = 13247012
Min send: 10000000, max send 0
Min long send: 663764, max long send 688387
Min fwd: 32840, max fwd 46918; min bwd 55338, max bwd 72497
Min long fwd: 65566, max long fwd 72714; min long bwd 84609, max long bwd 89154
Time taken by simulation: 1619 microseconds

Stages 8
13 3930501529.6000004
19 5322929356.799999
22 6014667161.6
23 6250707763.2
24 6479329689.6
Predicted microbatch size for 8: 24
comm size 12582912
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 0 663682.2411979454
End of simulation:  Mini-batch time (usec) = 22906615
Min send: 10000000, max send 0
Min long send: 663682, max long send 693511
Min fwd: 20179, max fwd 38698; min bwd 38108, max bwd 56452
Min long fwd: 50555, max long fwd 58723; min long bwd 68720, max long bwd 75958
Time taken by simulation: 4569 microseconds

Stages 12
13 3141742080.0
19 4248100352.0
22 4798114099.200001
23 4985794867.2
24 5168154112.0
Predicted microbatch size for 12: 24
comm size 12582912
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 43 0 0 663682.2411979454
End of simulation:  Mini-batch time (usec) = 28245316
Min send: 10000000, max send 0
Min long send: 663682, max long send 693511
Min fwd: 10606, max fwd 31119; min bwd 22396, max bwd 41317
Min long fwd: 40838, max long fwd 49874; min long bwd 47936, max long bwd 55473
Time taken by simulation: 7144 microseconds

{1: 4.182966, 2: 4.525989, 3: 5.984079, 4: 8.956234, 6: 13.247012, 8: 22.906615, 12: 28.245316}
{1: 8, 2: 19, 3: 24, 4: 24, 6: 24, 8: 24, 12: 24}
best config is: 2 19
expected time is 4.525989
7 per stage
14 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 19
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=19 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 45
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-24 14:48:23.556679 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=146, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=45, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12;1,3,5,7,9,11,13;', chunk_size=19, batch_size=146, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.12984848022460938
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
8 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_45.pt
2022-11-24 14:48:33.703808 resume step from  45
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-24 14:49:17.719328 - Finished loading checkpoint, takes 43.871 secs
DLL 2022-11-24 14:49:17.720203 - PARAMETER SEED : 12439 
DLL 2022-11-24 14:49:17.720326 - PARAMETER train_start : True 
DLL 2022-11-24 14:49:17.720391 - PARAMETER batch_size_per_gpu : 146 
DLL 2022-11-24 14:49:17.720439 - PARAMETER learning_rate : 0.006 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-24 14:49:33.708838] Finished iteration 45, CKPT_AND_STOP: True, flag: tensor([4], dtype=torch.int32), speed: 6824.279
DLL 2022-11-24 14:49:33.715170 - Training Epoch: 0 Training Iteration: 46  average_loss : nan  step_loss : nan  learning_rate : 0.00013793769354382627 
2022-11-24 14:49:33.715427 Begin to save checkpont to s3://spot-checkpoints/bert and exit
DLL 2022-11-24 14:49:33.715460 - PARAMETER checkpoint_step : 46 
Opt ckpt time 10.327970027923584
Process done with return code 0
Parent process ID: 80658 node: 172.31.28.108
24 cutpoints
Stages 1
13 20868437708.800007
7 13915698585.599995
10 17153729331.199993
8 14622140211.200005
9 15846820659.199995
Predicted microbatch size for 1: 8
comm size 0
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 1 10 0 1298431.5185546875 0
End of simulation:  Mini-batch time (usec) = 4527444
Min send: 10000000, max send 0
Min long send: 10000000, max long send 0
Min fwd: 10000000, max fwd 0; min bwd 10000000, max bwd 0
Min long fwd: 125868, max long fwd 130461; min long bwd 191031, max long bwd 199310
Time taken by simulation: 70 microseconds

Stages 2
13 10987980083.2
19 14996390399.999996
22 16963644723.199997
20 15631126630.400002
Predicted microbatch size for 2: 19
comm size 9961472
WARNING: no send time found, 2 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 2 8 0 600510.986328125 542254.6839368516
End of simulation:  Mini-batch time (usec) = 4525989
Min send: 10000000, max send 0
Min long send: 542442, max long send 556435
Min fwd: 107744, max fwd 113936; min bwd 154028, max bwd 157296
Min long fwd: 133001, max long fwd 138295; min long bwd 169394, max long bwd 174223
Time taken by simulation: 164 microseconds

Stages 3
13 7832942284.800001
19 10697074380.8
22 12097432473.599998
23 12575272243.2
24 13035207577.6
Predicted microbatch size for 3: 24
comm size 12582912
WARNING: no send time found, 3 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 3 11 0 367036.0107421875 663682.2411979454
End of simulation:  Mini-batch time (usec) = 6485322
Min send: 10000000, max send 0
Min long send: 663870, max long send 686148
Min fwd: 70710, max fwd 87372; min bwd 114242, max bwd 132372
Min long fwd: 107507, max long fwd 114298; min long bwd 148697, max long bwd 154156
Time taken by simulation: 362 microseconds

Stages 4
13 6255423385.6
19 8547416371.2
22 9664326348.8
23 10045446451.2
24 10412856422.4
Predicted microbatch size for 4: 24
comm size 12582912
WARNING: no send time found, 4 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 4 15 0 264588.56201171875 663682.2411979454
End of simulation:  Mini-batch time (usec) = 8956234
Min send: 10000000, max send 0
Min long send: 663870, max long send 688176
Min fwd: 51018, max fwd 67812; min bwd 86129, max bwd 98869
Min long fwd: 83628, max long fwd 91318; min long bwd 119532, max long bwd 124677
Time taken by simulation: 680 microseconds

Stages 6
13 4698582732.8
19 6397758361.599999
22 7231220224.0
23 7515620659.2
24 7790505267.2
Predicted microbatch size for 6: 24
comm size 12582912
WARNING: no send time found, 6 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 6 22 0 139487.73193359375 663682.2411979454
End of simulation:  Mini-batch time (usec) = 13247012
Min send: 10000000, max send 0
Min long send: 663764, max long send 688387
Min fwd: 32840, max fwd 46918; min bwd 55338, max bwd 72497
Min long fwd: 65566, max long fwd 72714; min long bwd 84609, max long bwd 89154
Time taken by simulation: 1619 microseconds

Stages 8
13 3930501529.6000004
19 5322929356.799999
22 6014667161.6
23 6250707763.2
24 6479329689.6
Predicted microbatch size for 8: 24
comm size 12582912
WARNING: no send time found, 8 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 8 43 0 0 663682.2411979454
End of simulation:  Mini-batch time (usec) = 22906615
Min send: 10000000, max send 0
Min long send: 663682, max long send 693511
Min fwd: 20179, max fwd 38698; min bwd 38108, max bwd 56452
Min long fwd: 50555, max long fwd 58723; min long bwd 68720, max long bwd 75958
Time taken by simulation: 4531 microseconds

Stages 12
13 3141742080.0
19 4248100352.0
22 4798114099.200001
23 4985794867.2
24 5168154112.0
Predicted microbatch size for 12: 24
comm size 12582912
WARNING: no send time found, 12 partitions
GPUS_PER_VM=1 /home/ubuntu/varuna/tools/simulator/simulate-varuna 12 43 0 0 663682.2411979454
End of simulation:  Mini-batch time (usec) = 28245316
Min send: 10000000, max send 0
Min long send: 663682, max long send 693511
Min fwd: 10606, max fwd 31119; min bwd 22396, max bwd 41317
Min long fwd: 40838, max long fwd 49874; min long bwd 47936, max long bwd 55473
Time taken by simulation: 7174 microseconds

{1: 4.527444, 2: 4.525989, 3: 6.485322, 4: 8.956234, 6: 13.247012, 8: 22.906615, 12: 28.245316}
{1: 8, 2: 19, 3: 24, 4: 24, 6: 24, 8: 24, 12: 24}
best config is: 2 19
expected time is 4.525989
7 per stage
14 servers!
Config:
ranks: range(0, 1)
train batch size: 1024
partitions: 2
chunk_size: 19
data depth: 7
stage to rank map: 0,2,4,6,8,10,12;1,3,5,7,9,11,13;
World size is 14
/opt/conda/envs/varuna/bin/python -u /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/run_pretraining.py --rank=0 --chunk_size=19 --local_rank=0 --stage_to_rank_map=0,2,4,6,8,10,12;1,3,5,7,9,11,13; --batch-size=146 --input_dir=/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/ --output_dir=s3://spot-checkpoints/bert --config_file=bert_config.json --bert_model=bert-large-uncased --train_batch_size=1024 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=200 --learning_rate=6e-3 --seed=12439 --fp16 --gradient_accumulation_steps=128 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --json-summary /home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json --varuna --resume_step 46
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2022-11-24 14:50:25.527769 - PARAMETER Config : ["Namespace(input_dir='/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en/', config_file='bert_config.json', bert_model='bert-large-uncased', output_dir='s3://spot-checkpoints/bert', init_checkpoint=None, max_seq_length=128, max_predictions_per_seq=20, train_batch_size=146, learning_rate=0.006, num_train_epochs=3.0, max_steps=7038.0, warmup_proportion=0.2843, local_rank=0, seed=12439, gradient_accumulation_steps=1, fp16=True, amp=False, loss_scale=0.0, log_freq=1.0, checkpoint_activations=False, resume_from_checkpoint=True, resume_step=46, num_steps_per_checkpoint=200, skip_checkpoint=False, phase2=False, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, phase1_end_step=7038, init_loss_scale=1048576, do_train=True, json_summary='/home/ubuntu/varuna_examples/DeepLearningExamples/PyTorch/LanguageModeling/BERT/results/dllogger.json', use_env=False, disable_progress_bar=False, steps_this_run=7038.0, varuna=True, stage_to_rank_map='0,2,4,6,8,10,12;1,3,5,7,9,11,13;', chunk_size=19, batch_size=146, rank=0, profiling=False, n_gpu=1)"] 
dry run time 0.2538447380065918
SHARED WEIGHTS ARE
[(0, 1)]
this rank  0 is part of pipeline replica  0
8 chunks
Begin to load checkpint from s3://spot-checkpoints/bert/ckpt_46.pt
2022-11-24 14:50:35.875321 resume step from  46
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
2022-11-24 14:51:16.577115 - Finished loading checkpoint, takes 40.616 secs
DLL 2022-11-24 14:51:16.578038 - PARAMETER SEED : 12439 
DLL 2022-11-24 14:51:16.578169 - PARAMETER train_start : True 
DLL 2022-11-24 14:51:16.578249 - PARAMETER batch_size_per_gpu : 146 
DLL 2022-11-24 14:51:16.578333 - PARAMETER learning_rate : 0.006 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  524288.0
[2022-11-24 14:51:32.636582] Finished iteration 46, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 6810.687
DLL 2022-11-24 14:51:32.642913 - Training Epoch: 0 Training Iteration: 47  average_loss : nan  step_loss : nan  learning_rate : 0.00014093633905564855 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  262144.0
[2022-11-24 14:51:37.020018] Finished iteration 47, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4383.089
DLL 2022-11-24 14:51:37.025034 - Training Epoch: 0 Training Iteration: 48  average_loss : nan  step_loss : nan  learning_rate : 0.0001439349845674709 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  131072.0
[2022-11-24 14:51:40.391504] Finished iteration 48, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3371.459
DLL 2022-11-24 14:51:40.396373 - Training Epoch: 0 Training Iteration: 49  average_loss : nan  step_loss : nan  learning_rate : 0.00014693363007929318 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  65536.0
[2022-11-24 14:51:44.771271] Finished iteration 49, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4379.759
DLL 2022-11-24 14:51:44.779991 - Training Epoch: 0 Training Iteration: 50  average_loss : nan  step_loss : nan  learning_rate : 0.0001499322755911155 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32768.0
[2022-11-24 14:51:49.155361] Finished iteration 50, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 4384.051
DLL 2022-11-24 14:51:49.160527 - Training Epoch: 0 Training Iteration: 51  average_loss : nan  step_loss : nan  learning_rate : 0.0001529309211029378 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16384.0
[2022-11-24 14:51:52.503626] Finished iteration 51, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3348.185
DLL 2022-11-24 14:51:52.508630 - Training Epoch: 0 Training Iteration: 52  average_loss : nan  step_loss : nan  learning_rate : 0.00015592956661476012 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8192.0
[2022-11-24 14:51:55.875481] Finished iteration 52, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3371.833
DLL 2022-11-24 14:51:55.880877 - Training Epoch: 0 Training Iteration: 53  average_loss : nan  step_loss : nan  learning_rate : 0.0001589282121265824 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  4096.0
[2022-11-24 14:51:59.207609] Finished iteration 53, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3332.085
DLL 2022-11-24 14:51:59.212544 - Training Epoch: 0 Training Iteration: 54  average_loss : nan  step_loss : nan  learning_rate : 0.00016192685763840475 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  2048.0
[2022-11-24 14:52:02.596665] Finished iteration 54, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3389.017
DLL 2022-11-24 14:52:02.601892 - Training Epoch: 0 Training Iteration: 55  average_loss : nan  step_loss : nan  learning_rate : 0.00016492550315022706 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  1024.0
[2022-11-24 14:52:05.971241] Finished iteration 55, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3374.552
DLL 2022-11-24 14:52:05.976152 - Training Epoch: 0 Training Iteration: 56  average_loss : nan  step_loss : nan  learning_rate : 0.00016792414866204937 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  512.0
[2022-11-24 14:52:09.394912] Finished iteration 56, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3423.668
DLL 2022-11-24 14:52:09.400078 - Training Epoch: 0 Training Iteration: 57  average_loss : nan  step_loss : nan  learning_rate : 0.00017092279417387166 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  256.0
[2022-11-24 14:52:12.763906] Finished iteration 57, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3368.970
DLL 2022-11-24 14:52:12.769097 - Training Epoch: 0 Training Iteration: 58  average_loss : nan  step_loss : nan  learning_rate : 0.00017392143968569397 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  128.0
[2022-11-24 14:52:16.120135] Finished iteration 58, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3356.189
DLL 2022-11-24 14:52:16.125050 - Training Epoch: 0 Training Iteration: 59  average_loss : nan  step_loss : nan  learning_rate : 0.0001769200851975163 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  64.0
[2022-11-24 14:52:19.499420] Finished iteration 59, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3379.226
DLL 2022-11-24 14:52:19.504422 - Training Epoch: 0 Training Iteration: 60  average_loss : nan  step_loss : nan  learning_rate : 0.0001799187307093386 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  32.0
[2022-11-24 14:52:22.861273] Finished iteration 60, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3361.871
DLL 2022-11-24 14:52:22.866358 - Training Epoch: 0 Training Iteration: 61  average_loss : nan  step_loss : nan  learning_rate : 0.00018291737622116094 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  16.0
[2022-11-24 14:52:26.206962] Finished iteration 61, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3345.616
DLL 2022-11-24 14:52:26.212054 - Training Epoch: 0 Training Iteration: 62  average_loss : nan  step_loss : nan  learning_rate : 0.0001859160217329832 
0 Overflow !!
0 : update_scale(): _has_overflow, dynamic. _loss_scale =  8.0
[2022-11-24 14:52:29.546494] Finished iteration 62, CKPT_AND_STOP: False, flag: tensor([0], dtype=torch.int32), speed: 3339.493
DLL 2022-11-24 14:52:29.553067 - Training Epoch: 0 Training Iteration: 63  average_loss : nan  step_loss : nan  learning_rate : 0.00018891466724480554 
Signal handler called with signal 10


 STOPPING VARUNA !!



Rank 0 signal handler called with signal 10
