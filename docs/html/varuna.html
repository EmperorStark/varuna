
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Varuna class &#8212; Varuna  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Varuna documentation" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="the-varuna-class">
<h1>The Varuna class<a class="headerlink" href="#the-varuna-class" title="Permalink to this headline">¶</a></h1>
<p>The torch.nn.Module object for your DNN model should be wrapped in a <a class="reference internal" href="#varuna.Varuna" title="varuna.Varuna"><code class="xref py py-class docutils literal notranslate"><span class="pre">Varuna</span></code></a> instance for training.
This class extends torch.nn.Module and handles distributed pipeline &amp; data parallelism, mixed precision and
shared parameter weights internally.</p>
<p>Wrapping in <a class="reference internal" href="#varuna.Varuna" title="varuna.Varuna"><code class="xref py py-class docutils literal notranslate"><span class="pre">Varuna</span></code></a> partitions the model into pipeline stages across the distributed job.
For this, it uses stage allocation information that is passed by <code class="docutils literal notranslate"><span class="pre">varuna.launcher</span></code> to all
worker processes. The launcher uses a string argument <code class="docutils literal notranslate"><span class="pre">stage_to_rank_map</span></code> which must be parsed
and used for <a class="reference internal" href="#varuna.Varuna" title="varuna.Varuna"><code class="xref py py-class docutils literal notranslate"><span class="pre">Varuna</span></code></a> initialisation.</p>
<p>Sample inputs (with any batch size) also need to be passed for this automatic partitioning. These inputs
are used to profile the model’s computation graph and should be passed a a dictionary of keywords to args.</p>
<p>The model passed to <a class="reference internal" href="#varuna.Varuna" title="varuna.Varuna"><code class="xref py py-class docutils literal notranslate"><span class="pre">Varuna</span></code></a> should be on CPU. Once the profiling and partitioning are done,
the model is moved to the assigned GPU. So the user need not do <code class="docutils literal notranslate"><span class="pre">model.cuda()</span></code> anywhere.</p>
<p>Optimizer creation should be after wrapping in <a class="reference internal" href="#varuna.Varuna" title="varuna.Varuna"><code class="xref py py-class docutils literal notranslate"><span class="pre">Varuna</span></code></a>, since it requires model parameters as input.
The optimizer needs to be registered with Varuna using a setter.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>             <span class="c1"># full model on CPU</span>
<span class="n">dry_run_input</span> <span class="o">=</span> <span class="p">{</span>             <span class="c1"># sample inputs for MyModel</span>
   <span class="s1">&#39;inputs&#39;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span>          <span class="c1"># keyword: arg</span>
   <span class="s1">&#39;mask&#39;</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>
   <span class="s1">&#39;extra_norm&#39;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>
<span class="c1"># parameter sharing across the model, marked as pairs of param_names</span>
<span class="n">shared_weights</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;language_model.embedding.word_embeddings.weight&quot;</span><span class="p">,</span><span class="s2">&quot;lm_head_weight&quot;</span><span class="p">)]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Varuna</span><span class="p">(</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">stage_to_rank_map</span><span class="p">,</span> <span class="n">dry_run_input</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="p">,</span>
                  <span class="n">args</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16</span><span class="p">,</span> <span class="n">local_rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span>
                  <span class="n">device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">shared_weights</span><span class="o">=</span><span class="n">shared_weights</span><span class="p">)</span>

<span class="c1"># now model is a subset of the original model, moved to the GPU on each process</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py class">
<dt id="varuna.Varuna">
<em class="property">class </em><code class="sig-prename descclassname">varuna.</code><code class="sig-name descname">Varuna</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">stage_to_rank_map</span></em>, <em class="sig-param"><span class="n">dummy_inputs</span></em>, <em class="sig-param"><span class="n">batch_size</span></em>, <em class="sig-param"><span class="n">chunk_size</span></em>, <em class="sig-param"><span class="n">fp16</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">local_rank</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">device</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">shared_weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">from_cache</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#varuna.Varuna" title="Permalink to this definition">¶</a></dt>
<dd><p>Module to implement varuna training. The model must be wrapped in an instance 
of <code class="docutils literal notranslate"><span class="pre">Varuna</span></code> before training. This should be done before optimizer creation and the 
<code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code> passed should be on CPU.</p>
<p>Creating a <code class="docutils literal notranslate"><span class="pre">Varuna</span></code> instance profiles the model briefly using <code class="xref py py-attr docutils literal notranslate"><span class="pre">dummy_inputs</span></code>
and partitions it according to the distributed rank and launcher arguments.
The partitioned model is then moved to the allocated cuda device. The profiling
information is cached and can be re-used on resuming, unless <code class="xref py py-attr docutils literal notranslate"><span class="pre">from_cache</span></code> is False.
The <code class="docutils literal notranslate"><span class="pre">Varuna</span></code> module performs mixed precision training internally if enabled through the 
<code class="xref py py-attr docutils literal notranslate"><span class="pre">fp16</span></code> arg, no external handling is required.</p>
<dl>
<dt>Args:</dt><dd><p>model (nn.Module): The model to initialize for training.</p>
<p>stage_to_rank_map (str): Placement of pipeline stages in the distribued job, encoded as a string. 
Passed by <code class="docutils literal notranslate"><span class="pre">varuna.launcher</span></code> to each worker as an argument.</p>
<p>dummy_inputs (dict): Sample inputs to the model as a dictionary. These are used to profile the             model as <code class="docutils literal notranslate"><span class="pre">model(**dummy_inputs)</span></code>. The batch size dimention in these inputs can be any <code class="docutils literal notranslate"><span class="pre">n&gt;=1</span></code>,
but is recommended to be small for speed.</p>
<p>batch_size (int): Global batch size for the distributed training job.</p>
<p>chunk_size (int): The micro-batch size to be used for pipeline parallelism.</p>
<p>fp16 (bool): whether to enable mixed precision training.</p>
<p>local_rank (int): The local rank as passed by <code class="docutils literal notranslate"><span class="pre">varuna.launcher</span></code>. If not given, 
defaults to the global rank.</p>
<p>device (int): index of the cuda device to use. Recommended to be the same as local_rank,
which is the default if not specified.</p>
<p>shared_weights (list or None): A list of tuples, where each each tuple is a pair of weight names (strings),
such that the two weights are shared in the model (see weight sharing)</p>
<p>from_cache (bool): Whether to use cached profiling information if available.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Optimizer initiliastion should be done after  <code class="docutils literal notranslate"><span class="pre">Varuna</span></code> initialisation, so that the <code class="docutils literal notranslate"><span class="pre">param_group</span></code> s
for the optimizer only contain parameters from the partitioned model. This is important both for memory 
usage and correctness of fp16 training. Once <code class="docutils literal notranslate"><span class="pre">Varuna</span></code> and the optimizer are initialised, <a class="reference internal" href="#varuna.Varuna.set_optimizer" title="varuna.Varuna.set_optimizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_optimizer()</span></code></a>
should be called to connect the two.</p>
</div>
<dl class="py method">
<dt id="varuna.Varuna.set_optimizer">
<code class="sig-name descname">set_optimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span></em>, <em class="sig-param"><span class="n">loss_scale</span><span class="o">=</span><span class="default_value">'dynamic'</span></em>, <em class="sig-param"><span class="n">init_loss_scale</span><span class="o">=</span><span class="default_value">1048576</span></em>, <em class="sig-param"><span class="n">min_loss_scale</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#varuna.Varuna.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure optimizer for training. if <code class="docutils literal notranslate"><span class="pre">fp16</span></code> is enabled, this function
initializes the mixed precision state in apex.</p>
<dl>
<dt>Args:</dt><dd><p>optimizer (nn.Optimizer): the optimizer for training.</p>
<p>loss_scale (float or “dynamic”, optional): A floating point number for a static loss scale 
or the string “dynamic” for dynamic loss scaling.</p>
<p>init_loss_scale (float, optional): Initial loss scale (for dynamic scaling)</p>
<p>min_loss_scale (float, optional): minimum loss scale (for dynamic scaling)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="varuna.Varuna.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">clip_grad_max_norm</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#varuna.Varuna.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a single training step. Executes forward and backward passes for 
the global batch. This function must be called by all distributed workers in the training loop.
After this function, the optimizer gradients are reduced accross data parallel replicas and
overflow is checked for mixed precision training.</p>
<p>Returns average loss and a boolean for overflow.</p>
<dl>
<dt>Args:</dt><dd><p>inputs (dict): The inputs to the model as a dictionary. These should be coordinated amongst workers -
the global batch is sharded across data parallel replicas, so each worker should have 
<code class="docutils literal notranslate"><span class="pre">global_batch_size</span> <span class="pre">/</span> <span class="pre">data_parallel_depth</span></code> number of examples. And all pipeline stages of the same
data parallel replica should recieve the same inputs.</p>
<p>clip_grad_max_norm (float or None, optional): If given, the L2 gradient norm of the entire model
is clipped to this upper bound.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="varuna.Varuna.checkpoint">
<code class="sig-name descname">checkpoint</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">global_store</span></em>, <em class="sig-param"><span class="n">step</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tempdir</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">shard</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">on_demand</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#varuna.Varuna.checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Writes a varuna checkpoint with model parameters, optimizer state etc. 
Each checkpoint is a directory, written under the given path.</p>
<dl>
<dt>Args:</dt><dd><p>global_store (str): path to a folder accessible by all nodes/ranks in the training job. 
For example, path to a mounted blob storage. This is where the varuna checkpoint folder is written.</p>
<p>step (int or None): iteration number for checkpoint. If None, it’ll be taken from varuna’s tracked progress.</p>
<p>tempdir (str): path to a local directory to which to write checkpoints temporarily, and sync
with the global store in the background. Lowers checkpoint write time in the critical path.</p>
<p>shard (bool): whether to shard checkpoint writes over data parallel workers as well. Speeds up checkpoint</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="varuna.Varuna.load_checkpoint">
<code class="sig-name descname">load_checkpoint</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">global_store</span></em>, <em class="sig-param"><span class="n">iteration</span></em>, <em class="sig-param"><span class="n">check_complete</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#varuna.Varuna.load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a varuna checkpoint from a shared directory. Each varuna checkpoint is a directory
named as “varuna_ckpt_&lt;iteration&gt;”. So the path under which all such checkpoints were written
should be specified.</p>
<blockquote>
<div><dl>
<dt>Args:</dt><dd><p>global_store (str): path under which varuna checkpoints were written. 
Should be accessible by all workers.</p>
<p>iteration (int): Which iteration checkpoint to load.</p>
<p>check_complete (bool, optional): Check that the checkpoint is complete before loading it.
A checkpoint can be incomplete if the write was interrupted.</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt id="varuna.Varuna.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#varuna.Varuna.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the model on the given inputs. Returns loss.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs (dict): Model inputs as dictionary. The number of examples
for these inputs should be the same as the batch_size defined for training.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Varuna</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">The Varuna class</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Varuna documentation</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Nitika Saran.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/varuna.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>