diff --git a/examples/pretrain_gpt2.sh b/examples/pretrain_gpt2.sh
index 66232bf..1c5b807 100755
--- a/examples/pretrain_gpt2.sh
+++ b/examples/pretrain_gpt2.sh
@@ -5,19 +5,25 @@
 RANK=0
 WORLD_SIZE=1
 
-DATA_PATH=<Specify path and file prefix>_text_document
-CHECKPOINT_PATH=<Specify path>
+NNODES=$1
+NODE_RANK=$2
+MASTER_ADDR=$3
+ckpt=$4
 
+DATA_PATH=/home/varuna/gpt2-blob/turing/megatron
+CHECKPOINT_PATH=/home/varuna/gpt2-blob/dummy
 
-python pretrain_gpt2.py \
+NCCL_DEBUG=INFO NCCL_SOCKET_IFNAME=eth0 NCCL_SOCKET_NTHREADS=4 NCCL_NSOCKS_PERTHREAD=4 \
+python3 run_varuna.py --nstages 4 --batch_size 64 --chunk_size 1 --total_num_stages 48 \
+       --ngpus_per_server 4 --nservers $NNODES --node_rank $NODE_RANK \
+       --master_addr $MASTER_ADDR pretrain_gpt2.py \
        --num-layers 24 \
        --hidden-size 1024 \
        --num-attention-heads 16 \
-       --batch-size 8 \
        --seq-length 1024 \
        --max-position-embeddings 1024 \
-       --train-iters 500000 \
-       --lr-decay-iters 320000 \
+       --train-iters 18750 \
+       --lr-decay-iters 18750 \
        --save $CHECKPOINT_PATH \
        --load $CHECKPOINT_PATH \
        --data-path $DATA_PATH \
@@ -25,19 +31,19 @@ python pretrain_gpt2.py \
        --merge-file gpt2-merges.txt \
        --data-impl mmap \
        --split 949,50,1 \
-       --distributed-backend nccl \
+       --distributed-backend gloo \
        --lr 0.00015 \
        --min-lr 1.0e-5 \
        --lr-decay-style cosine \
        --weight-decay 1e-2 \
        --clip-grad 1.0 \
        --warmup .01 \
-       --checkpoint-activations \
-       --log-interval 100 \
-       --save-interval 10000 \
+       --log-interval 1 \
+       --save-interval 10 \
        --eval-interval 1000 \
+       --use-cpu-initialization \
        --eval-iters 10 \
-       --fp16
+       --fp16 --varuna 
 
 
 set +x
diff --git a/megatron/arguments.py b/megatron/arguments.py
index c4555af..f69091f 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -40,7 +40,7 @@ def parse_args(extra_args_provider=None, defaults={},
     parser = _add_data_args(parser)
     parser = _add_autoresume_args(parser)
     parser = _add_realm_args(parser)
-
+    parser = _add_varuna_args(parser)
     # Custom arguments.
     if extra_args_provider is not None:
         parser = extra_args_provider(parser)
@@ -66,7 +66,7 @@ def parse_args(extra_args_provider=None, defaults={},
 
     # Parameters dtype.
     args.params_dtype = torch.float
-    if args.fp16:
+    if args.fp16 and not args.varuna:
         args.params_dtype = torch.half
     if args.rank == 0:
         print('using {} for parameters ...'.format(args.params_dtype),
@@ -90,6 +90,8 @@ def parse_args(extra_args_provider=None, defaults={},
     # Check required arguments.
     required_args = ['num_layers', 'hidden_size', 'num_attention_heads',
                      'max_position_embeddings']
+    if args.varuna:
+        required_args += ['stage_to_rank_map', 'chunk_size']
     for req_arg in required_args: 
         _check_arg_is_not_none(args, req_arg)
 
@@ -300,6 +302,17 @@ def _add_learning_rate_args(parser):
 
     return parser
 
+def _add_varuna_args(parser):
+    group = parser.add_argument_group(title='varuna')
+
+    group.add_argument("--varuna", action='store_true', default=False,
+                        help = "Enable varuna pipeline training")
+    group.add_argument("--stage_to_rank_map", type=str, default=None,
+                        help = "stage to rank map of Varuna model")
+    group.add_argument("--chunk_size", type=int,default=None,
+                        help = "number of microbatches for pipeline")
+    group.add_argument("--rank", type=int, default=-1)
+    return parser
 
 def _add_checkpointing_args(parser):
     group = parser.add_argument_group(title='checkpointing')
diff --git a/megatron/checkpointing.py b/megatron/checkpointing.py
index 1a8bd40..7213040 100644
--- a/megatron/checkpointing.py
+++ b/megatron/checkpointing.py
@@ -103,11 +103,12 @@ def save_checkpoint(iteration, model, optimizer, lr_scheduler):
         state_dict['args'] = args
         state_dict['checkpoint_version'] = 2.0
         state_dict['iteration'] = iteration
-        state_dict['model'] = model.state_dict_for_save_checkpoint()
+        if not args.varuna:
+            state_dict['model'] = model.state_dict_for_save_checkpoint()
 
         # Optimizer stuff.
         if not args.no_save_optim:
-            if optimizer is not None:
+            if optimizer is not None and not args.varuna:
                 state_dict['optimizer'] = optimizer.state_dict()
             if lr_scheduler is not None:
                 state_dict['lr_scheduler'] = lr_scheduler.state_dict()
@@ -129,10 +130,14 @@ def save_checkpoint(iteration, model, optimizer, lr_scheduler):
         torch.save(state_dict, checkpoint_name)
         print('  successfully saved {}'.format(checkpoint_name))
 
+    ckpt_future = None
+    if args.varuna:
+        ckpt_future = model.checkpoint(args.save, tempdir=None, step=iteration)
+
     # Wait so everyone is done (necessary)
     torch.distributed.barrier()
     # And update the latest iteration
-    if torch.distributed.get_rank() == 0:
+    if ckpt_future is not None and torch.distributed.get_rank() == 0:
         tracker_filename = get_checkpoint_tracker_filename(args.save)
         with open(tracker_filename, 'w') as f:
             f.write(str(iteration))
@@ -223,12 +228,15 @@ def load_checkpoint(model, optimizer, lr_scheduler, load_arg='load'):
         print_rank_0('could not find arguments in the checkpoint ...')
 
     # Model.
-    model.load_state_dict(state_dict['model'])
+    if not args.varuna:
+        model.load_state_dict(state_dict['model'])
+    else:
+        model.load_checkpoint(args.load, iteration)
 
     # Optimizer.
     if not release and not args.finetune and not args.no_load_optim:
         try:
-            if optimizer is not None:
+            if optimizer is not None and not args.varuna:
                 optimizer.load_state_dict(state_dict['optimizer'])
             if lr_scheduler is not None:
                 lr_scheduler.load_state_dict(state_dict['lr_scheduler'])
diff --git a/megatron/data/gpt2_dataset.py b/megatron/data/gpt2_dataset.py
index f630a3c..0e1463b 100644
--- a/megatron/data/gpt2_dataset.py
+++ b/megatron/data/gpt2_dataset.py
@@ -69,6 +69,16 @@ def build_train_valid_test_datasets(data_prefix, data_impl, splits_string,
     return (train_dataset, valid_dataset, test_dataset)
 
 
+def build_dry_run_dataset(data_prefix, data_impl, seq_length, seed, skip_warmup):
+    indexed_dataset = get_indexed_dataset_(data_prefix,
+                                           data_impl,
+                                           skip_warmup)
+    documents = np.arange(start=0, stop=1, step=1, dtype=np.int32)
+    dataset = GPT2Dataset("dryrun", data_prefix,
+                        documents, indexed_dataset,
+                        1, seq_length, seed)
+    return dataset
+
 def get_indexed_dataset_(data_prefix, data_impl, skip_warmup):
     """Build indexed dataset."""
     print_rank_0(' > building dataset index ...')
diff --git a/megatron/model/gpt2_model.py b/megatron/model/gpt2_model.py
index b0d275f..e641b8f 100644
--- a/megatron/model/gpt2_model.py
+++ b/megatron/model/gpt2_model.py
@@ -50,9 +50,12 @@ class GPT2Model(MegatronModule):
             scaled_init_method=scaled_init_method_normal(args.init_method_std,
                                                          args.num_layers))
 
+        self.lm_head_weight = torch.nn.Parameter(self.language_model.embedding.word_embeddings.weight)
+
+
     def forward(self, input_ids, position_ids, attention_mask, labels=None,
                 tokentype_ids=None, layer_past=None, get_key_value=False,
-                forward_method_parallel_output=None):
+                forward_method_parallel_output=None, loss_mask=None):
 
         # Language model.
         lm_output = self.language_model(input_ids,
@@ -71,7 +74,7 @@ class GPT2Model(MegatronModule):
             parallel_output = forward_method_parallel_output
         output = parallel_lm_logits(
             lm_output,
-            self.language_model.embedding.word_embeddings.weight,
+            self.lm_head_weight,
             parallel_output)
 
         if get_key_value:
@@ -85,6 +88,11 @@ class GPT2Model(MegatronModule):
                 loss = mpu.vocab_parallel_cross_entropy(output, labels)
             else:
                 loss = mpu.vocab_parallel_cross_entropy(output.float(), labels)
+            
+            if loss_mask is not None:
+                loss_mask = loss_mask.view(-1)
+                loss = torch.sum(loss.view(-1) * loss_mask) / loss_mask.sum()
+            
             return loss
 
 
diff --git a/megatron/model/transformer.py b/megatron/model/transformer.py
index f2be536..1ac58fb 100644
--- a/megatron/model/transformer.py
+++ b/megatron/model/transformer.py
@@ -28,6 +28,8 @@ from megatron.model.fused_softmax import FusedScaleMaskSoftmax
 from megatron.model.fused_bias_gelu import bias_gelu_impl
 from megatron.model.utils import openai_gelu, erf_gelu
 
+from varuna import CutPoint, is_varuna_dummy_val
+
 # flags required to enable jit fusion kernels
 torch._C._jit_set_profiling_mode(False)
 torch._C._jit_set_profiling_executor(False)
@@ -269,7 +271,7 @@ class ParallelSelfAttention(MegatronModule):
             output_size[2], 
             output_size[3],
             dtype=query_layer.dtype, 
-            device=torch.cuda.current_device())
+            device=query_layer.device)
 
         # Raw attention scores. [b * np, sq, sk]
         matmul_result = torch.baddbmm(matmul_result, 
@@ -520,6 +522,8 @@ class ParallelTransformer(MegatronModule):
                 output_layer_init_method, layer_number)
         self.layers = torch.nn.ModuleList(
             [build_layer(i + 1) for i in range(self.num_unique_layers)])
+        self.cutpoints = torch.nn.ModuleList([CutPoint() for i in range(self.num_layers - 1)])
+
 
         # Print layer ordering.
         if self.num_layers != self.num_unique_layers:
@@ -581,7 +585,7 @@ class ParallelTransformer(MegatronModule):
                 'activation checkpointing'
 
         # data format change to avoid explicit tranposes : [b s h] --> [s b h]
-        hidden_states = hidden_states.transpose(0, 1).contiguous()
+        hidden_states = hidden_states.transpose(0, 1).contiguous() if hidden_states is not None else hidden_states
 
         if self.checkpoint_activations:
             hidden_states = self._checkpointed_forward(hidden_states,
@@ -598,6 +602,14 @@ class ParallelTransformer(MegatronModule):
                                       attention_mask,
                                       layer_past=past,
                                       get_key_value=get_key_value)
+                
+                if index < (self.num_layers - 1):
+                    # transpose to get [b,s,h] for send/recv and [s,b,h] again for operations
+                    if not hidden_states is None and hidden_states.dim() >= 2:
+                        hidden_states = hidden_states.transpose(0, 1).contiguous()
+                    hidden_states = self.cutpoints[index](hidden_states)
+                    if not hidden_states is None and hidden_states.dim() >= 2:
+                        hidden_states = hidden_states.transpose(0, 1).contiguous()
                 if get_key_value:
                     hidden_states, present = hidden_states
                     presents.append(present)
diff --git a/megatron/training.py b/megatron/training.py
index ca1bd26..6e49692 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -40,9 +40,17 @@ from megatron.utils import check_adlr_autoresume_termination
 from megatron.utils import make_data_loader
 from megatron.utils import report_memory
 
+from varuna import Varuna, get_varuna_config
+
+CKPT_AND_STOP = False
+
+def on_demand_checkpoint():
+    global CKPT_AND_STOP
+    CKPT_AND_STOP = True
 
 def pretrain(train_valid_test_dataset_provider, model_provider,
-             forward_step_func, extra_args_provider=None, args_defaults={}):
+             forward_step_func, dry_run_input_provider=None, varuna_step_func=None, 
+             extra_args_provider=None, args_defaults={}):
     """Main training program.
 
     This function will run the followings in the order provided:
@@ -76,7 +84,8 @@ def pretrain(train_valid_test_dataset_provider, model_provider,
 
     # Model, optimizer, and learning rate.
     timers('model and optimizer').start()
-    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)
+    dry_run_input = dry_run_input_provider()
+    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider, dry_run_input)
     timers('model and optimizer').stop()
 
     # Data stuff.
@@ -93,7 +102,7 @@ def pretrain(train_valid_test_dataset_provider, model_provider,
 
     iteration = 0
     if args.do_train and args.train_iters > 0:
-        iteration = train(forward_step_func,
+        iteration = train(forward_step_func if not args.varuna else varuna_step_func,
                           model, optimizer, lr_scheduler,
                           train_data_iterator, valid_data_iterator)
 
@@ -114,12 +123,20 @@ def pretrain(train_valid_test_dataset_provider, model_provider,
                                    0, True)
 
 
-def get_model(model_provider_func):
+def get_model(model_provider_func, dry_run_input=None):
     """Build the model."""
     args = get_args()
 
     # Build model on cpu.
     model = model_provider_func()
+    if args.varuna:
+        assert dry_run_input is not None, "Must provide dummy input to varuna for dry runs"
+        shared_weights = [("language_model.embedding.word_embeddings.weight","lm_head_weight")]
+        pipeline_parallel_size, data_parallel_size = get_varuna_config(args.stage_to_rank_map)
+        global_batch_size = args.batch_size * data_parallel_size
+        model = Varuna( model, args.stage_to_rank_map, dry_run_input, global_batch_size, 
+                        args.chunk_size, args.fp16, local_rank=args.local_rank, 
+                        device=args.local_rank, shared_weights=shared_weights)
 
     # Print number of parameters.
     if mpu.get_data_parallel_rank() == 0:
@@ -127,26 +144,29 @@ def get_model(model_provider_func):
             mpu.get_model_parallel_rank(),
             sum([p.nelement() for p in model.parameters()])), flush=True)
 
-    # GPU allocation.
-    model.cuda(torch.cuda.current_device())
+    # Varuna handles fp16, parallelisation, moving to devices internally
+    if not args.varuna:
+        # GPU allocation.
+        model.cuda(torch.cuda.current_device())
 
-    # Fp16 conversion.
-    if args.fp16:
-        model = FP16_Module(model)
-
-    # Wrap model for distributed training."""
-    if args.DDP_impl == 'torch':
-        i = torch.cuda.current_device()
-        model = torchDDP(model, device_ids=[i], output_device=i,
-                         process_group=mpu.get_data_parallel_group())
-        return model
-    if args.DDP_impl == 'local':
-        model = LocalDDP(model)
-        return model
+        # Fp16 conversion.
+        if args.fp16:
+            model = FP16_Module(model)
+
+        # Wrap model for distributed training."""
+        if args.DDP_impl == 'torch':
+            i = torch.cuda.current_device()
+            model = torchDDP(model, device_ids=[i], output_device=i,
+                            process_group=mpu.get_data_parallel_group())
+            return model
+        if args.DDP_impl == 'local':
+            model = LocalDDP(model)
+            return model
 
-    raise NotImplementedError('Unknown DDP implementation specified: {}. '
-                              'Exiting.'.format(args.DDP_impl))
+        raise NotImplementedError('Unknown DDP implementation specified: {}. '
+                                'Exiting.'.format(args.DDP_impl))
 
+    return model
 
 def get_optimizer(model):
     """Set up the optimizer."""
@@ -167,8 +187,8 @@ def get_optimizer(model):
     optimizer = Adam(param_groups, lr=args.lr, weight_decay=args.weight_decay,
         betas=(args.adam_beta1, args.adam_beta2), eps=args.adam_eps)
 
-    # Wrap into fp16 optimizer.
-    if args.fp16:
+    # Wrap into fp16 optimizer (if not handled by varuna).
+    if not args.varuna and args.fp16:
         optimizer = FP16_Optimizer(optimizer,
                                    static_loss_scale=args.loss_scale,
                                    dynamic_loss_scale=args.dynamic_loss_scale,
@@ -206,12 +226,13 @@ def get_learning_rate_scheduler(optimizer):
     return lr_scheduler
 
 
-def setup_model_and_optimizer(model_provider_func):
+def setup_model_and_optimizer(model_provider_func, dry_run_input):
     """Setup model and optimizer."""
     args = get_args()
 
-    model = get_model(model_provider_func)
+    model = get_model(model_provider_func, dry_run_input)
     optimizer = get_optimizer(model)
+    model.set_optimizer(optimizer)
     lr_scheduler = get_learning_rate_scheduler(optimizer)
 
     if args.load is not None:
@@ -269,30 +290,41 @@ def backward_step(optimizer, model, loss):
     timers('backward-clip-grad').stop()
 
 
-def train_step(forward_step_func, data_iterator,
+def train_step(forward_or_varuna_step_func, data_iterator,
                model, optimizer, lr_scheduler):
     """Single training step."""
     args = get_args()
     timers = get_timers()
 
-    # Forward model for one step.
-    timers('forward').start()
-    loss, loss_reduced = forward_step_func(data_iterator, model)
-    timers('forward').stop()
-
-    # Calculate gradients, reduce across processes, and clip.
-    timers('backward').start()
-    backward_step(optimizer, model, loss)
-    timers('backward').stop()
+    if not args.varuna:
+        forward_step_func = forward_or_varuna_step_func
+        # Forward model for one step.
+        timers('forward').start()
+        loss, loss_reduced = forward_step_func(data_iterator, model)
+        timers('forward').stop()
+
+        # Calculate gradients, reduce across processes, and clip.
+        timers('backward').start()
+        backward_step(optimizer, model, loss)
+        timers('backward').stop()
+        overflow = False
+    else:
+        # timers('varuna')
+        varuna_step_func = forward_or_varuna_step_func
+        loss, loss_reduced, overflow = varuna_step_func(data_iterator, model)
 
     # Update parameters.
     timers('optimizer').start()
-    optimizer.step()
+    if not overflow:
+        optimizer.step()
+    else:
+        model.zero_grad()
+    overflow = optimizer.overflow if (args.fp16 and not args.varuna) else overflow
     timers('optimizer').stop()
 
     # Update learning rate.
     skipped_iter = 0
-    if not (args.fp16 and optimizer.overflow):
+    if not overflow:
         lr_scheduler.step()
     else:
         skipped_iter = 1
@@ -391,7 +423,7 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
     return report_memory_flag
 
 
-def train(forward_step_func, model, optimizer, lr_scheduler,
+def train(forward_or_varuna_step_func, model, optimizer, lr_scheduler,
           train_data_iterator, valid_data_iterator):
     """Train the model function."""
     args = get_args()
@@ -409,17 +441,21 @@ def train(forward_step_func, model, optimizer, lr_scheduler,
     timers('interval time').start()
     report_memory_flag = True
     while iteration < args.train_iters:
-        loss_dict, skipped_iter = train_step(forward_step_func,
+        loss_dict, skipped_iter = train_step(forward_or_varuna_step_func,
                                              train_data_iterator,
                                              model,
                                              optimizer,
                                              lr_scheduler)
         iteration += 1
 
+        if CKPT_AND_STOP:
+            save_checkpoint(iteration, model, optimizer, lr_scheduler)
+            exit()
+
         # Logging.
         loss_scale = None
         if args.fp16:
-            loss_scale = optimizer.loss_scale
+            loss_scale = optimizer.loss_scale if not args.varuna else model.get_loss_scale()
         report_memory_flag = training_log(loss_dict, total_loss_dict,
                                           optimizer.param_groups[0]['lr'],
                                           iteration, loss_scale,
@@ -522,6 +558,8 @@ def build_train_valid_test_data_iterators(
     if mpu.get_model_parallel_rank() == 0:
         # Rank, size, and global batch size.
         data_parallel_size = mpu.get_data_parallel_world_size()
+        if args.varuna:
+            pipeline_parallel_size, data_parallel_size = get_varuna_config(args.stage_to_rank_map)
         global_batch_size = args.batch_size * data_parallel_size
 
         # Number of train/valid/test samples.
diff --git a/megatron/utils.py b/megatron/utils.py
index 24d832d..556770b 100644
--- a/megatron/utils.py
+++ b/megatron/utils.py
@@ -27,6 +27,7 @@ from megatron.checkpointing import save_checkpoint
 from megatron.data.samplers import DistributedBatchSampler
 from megatron.fp16 import FP16_Optimizer
 
+from varuna import get_varuna_config, get_this_rank_config_varuna
 
 def reduce_losses(losses):
     """Reduce a tensor of losses across all GPUs."""
@@ -96,8 +97,13 @@ def make_data_loader(dataset):
     args = get_args()
 
     # Data parallel arguments.
-    world_size = mpu.get_data_parallel_world_size()
-    rank = mpu.get_data_parallel_rank()
+    if args.varuna:
+        pipeline_parallel_size, data_parallel_size = get_varuna_config(args.stage_to_rank_map)
+        pipeline_stage, data_parallel_rank = get_this_rank_config_varuna(args.stage_to_rank_map, args.rank)
+        world_size = data_parallel_size; rank = data_parallel_rank
+    else:
+        world_size = mpu.get_data_parallel_world_size()
+        rank = mpu.get_data_parallel_rank()
     global_batch_size = args.batch_size * world_size
     num_workers = args.num_workers
 
diff --git a/pretrain_gpt2.py b/pretrain_gpt2.py
index 372258f..b9d25f6 100644
--- a/pretrain_gpt2.py
+++ b/pretrain_gpt2.py
@@ -22,12 +22,14 @@ from megatron import print_rank_0
 from megatron import get_timers
 from megatron import get_tokenizer
 from megatron import mpu
-from megatron.data.gpt2_dataset import build_train_valid_test_datasets
+from megatron.data.gpt2_dataset import build_train_valid_test_datasets, build_dry_run_dataset
 from megatron.model import GPT2Model
-from megatron.training import pretrain
+from megatron.training import pretrain, on_demand_checkpoint
 from megatron.utils import get_ltor_masks_and_position_ids
 from megatron.utils import reduce_losses
 
+import signal
+
 def model_provider():
     """Build the model."""
 
@@ -89,6 +91,31 @@ def forward_step(data_iterator, model):
 
     return loss, {'lm loss': reduced_loss[0]}
 
+def varuna_step(data_iterator, model):
+
+    args = get_args()
+    timers = get_timers()
+
+    # Get the batch.
+    timers('batch generator').start()
+    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(
+        data_iterator)
+    timers('batch generator').stop()
+
+    inputs = dict({
+        "input_ids": tokens,
+        "position_ids": position_ids,
+        "attention_mask": attention_mask,
+        "loss_mask": loss_mask,
+        "labels": labels
+    })
+
+    loss, overflow = model.step(inputs)
+    loss = torch.Tensor([loss])
+    # Reduce loss for logging.
+    reduced_loss = reduce_losses([loss])
+
+    return loss, {'lm loss': reduced_loss[0]}, overflow
 
 def train_valid_test_datasets_provider(train_val_test_num_samples):
     """Build train, valid, and test datasets."""
@@ -108,8 +135,34 @@ def train_valid_test_datasets_provider(train_val_test_num_samples):
 
     return train_ds, valid_ds, test_ds
 
+def get_dry_run_input():
+    args = get_args()
+    dataset =  build_dry_run_dataset(data_prefix=args.data_path,
+                                    data_impl=args.data_impl,
+                                    seq_length=args.seq_length,
+                                    seed=args.seed,
+                                    skip_warmup=(not args.mmap_warmup) )
+    iterator = iter(torch.utils.data.DataLoader(dataset, batch_size=1))
+    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(iterator)
+    dry_run_input = {
+        "input_ids": tokens.cpu(),
+        "position_ids": position_ids.cpu(),
+        "attention_mask": attention_mask.cpu(),
+        "loss_mask": loss_mask.cpu(),
+        "labels": labels.cpu()
+    }
+    return dry_run_input
 
 if __name__ == "__main__":
 
-    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
-             args_defaults={'tokenizer_type': 'GPT2BPETokenizer'})
+    
+    def handler(signum,_):
+        print(torch.distributed.get_rank(), 'signal handler called with signal', signum)
+        on_demand_checkpoint()
+        exit()
+
+    signal.signal(signal.SIGUSR1, handler)
+
+    pretrain(train_valid_test_datasets_provider, model_provider, forward_step, 
+            dry_run_input_provider=get_dry_run_input, varuna_step_func=varuna_step, 
+            args_defaults={'tokenizer_type': 'GPT2BPETokenizer'})
